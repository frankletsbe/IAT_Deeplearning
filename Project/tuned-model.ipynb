{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0697d144",
   "metadata": {},
   "source": [
    "Tuning opportunities\n",
    "1. Configuration-Based Hyperparameter Tuning\n",
    "2. Enhanced Data Augmentation\n",
    "3. Improved Model Architecture\n",
    "4. Advanced Callbacks\n",
    "5. Transfer Learning (Often Best Results)\n",
    "6. Class Imbalance Handling\n",
    "7. Ensemble Methods\n",
    "8. Systematic Tuning Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9fe6a",
   "metadata": {},
   "source": [
    "Quick Start Guide:\n",
    "For best results immediately: Set \"backbone\": \"mobilenet\" and \"fine_tune\": True\n",
    "For faster training: Set \"backbone\": \"custom_cnn\" and \"aug_level\": \"light\"\n",
    "For maximum accuracy: Enable ensemble with \"use_ensemble\": True\n",
    "For experimentation: Uncomment Cell 17 to run systematic tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00b4b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-12 02:23:12.180237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1762914192.406982      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1762914192.477898      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Python executable: /usr/bin/python3\n",
      "======================================================================\n",
      "= is device GPU device\n",
      "======================================================================\n",
      "Mixed precision enabled: mixed_float16\n",
      "Global policy: <DTypePolicy \"mixed_float16\">\n",
      "======================================================================\n",
      "âœ… All libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Facial Emotion Recognition - Enhanced with 8 Tuning Improvements\n",
    "# =======================================================================\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning libraries\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Flatten, Dropout, Dense, Input, \n",
    "    GlobalAveragePooling2D, Conv2D, \n",
    "    BatchNormalization, Activation, MaxPooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping, \n",
    "    ReduceLROnPlateau, LearningRateScheduler)\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"= is device GPU device\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable mixed precision only if a GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled: mixed_float16\")\n",
    "else:\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    print(\"No GPU detected â€” using float32\")\n",
    "    \n",
    "print(\"Global policy:\", mixed_precision.global_policy())\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c880f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŒ Running on Kaggle\n",
      "Data folder path: /kaggle/input/face-expression-recognition-dataset/images\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Environment Detection\n",
    "# =======================================================================\n",
    "\n",
    "def is_on_kaggle():\n",
    "    \"\"\"Detect if running on Kaggle.\"\"\"\n",
    "    return os.path.exists('/kaggle/input')\n",
    "\n",
    "def get_data_path():\n",
    "    \"\"\"Detect environment and return appropriate data path.\"\"\"\n",
    "    if is_on_kaggle():\n",
    "        print(\"ğŸŒ Running on Kaggle\")\n",
    "        import kagglehub\n",
    "        image_path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
    "        folder_path = os.path.join(image_path, \"images\")\n",
    "    else:\n",
    "        print(\"ğŸ’» Running on local machine\")\n",
    "        folder_path = \"data/images/\"\n",
    "    \n",
    "    return folder_path\n",
    "\n",
    "folder_path = get_data_path()\n",
    "print(f\"Data folder path: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1475dab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  backbone: mobilenet\n",
      "  picture_size: 48\n",
      "  color_mode: grayscale\n",
      "  batch_size: 64\n",
      "  epochs: 5\n",
      "  learning_rate: 0.0001\n",
      "  dropout_rate: 0.25\n",
      "  dense_units: [512]\n",
      "  aug_level: light\n",
      "  precision: mixed\n",
      "  fine_tune: True\n",
      "  fine_tune_epochs: 0\n",
      "  optimizer: adam\n",
      "  weight_decay: 0.0001\n",
      "  use_lr_schedule: True\n",
      "  use_class_weights: True\n",
      "  use_ensemble: False\n",
      "  n_ensemble_models: 3\n",
      "  no_of_classes: 7\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Configuration with Tuning Options\n",
    "# =======================================================================\n",
    "\n",
    "cfg = {\n",
    "    # Model Architecture\n",
    "    \"backbone\": \"mobilenet\",  # Options: \"custom_cnn\", \"mobilenet\", \"efficientnet\"\n",
    "    \n",
    "    # Image Parameters\n",
    "    \"picture_size\": 48,  # Try: 64, 96 for more detail\n",
    "    \"color_mode\": \"grayscale\",  # \"grayscale\" or \"rgb\" (rgb required for transfer learning)\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch_size\": 64,  # Reduced from 128 for better gradient estimates\n",
    "    \"epochs\": 5,  # Increased from 30\n",
    "    \"learning_rate\": 0.0001,  # Adjusted learning rate\n",
    "    \"dropout_rate\": 0.25,  # Increased from 0.25\n",
    "     \n",
    "    # Dense layers\n",
    "    \"dense_units\": [512],  # Added an extra dense layer (Improvement #6)\n",
    "    \n",
    "    # Data Augmentation (Improvement #2)\n",
    "    \"aug_level\": \"light\",  # Options: \"none\", \"light\", \"strong\", \"aggressive\"\n",
    "    \n",
    "    # Advanced Training\n",
    "    \"precision\": \"mixed\",  # \"float32\" or \"mixed\"\n",
    "    \"fine_tune\": True,  # Enable fine-tuning for transfer learning\n",
    "    \"fine_tune_epochs\": 0,\n",
    "    \n",
    "    # Optimizer\n",
    "    \"optimizer\": \"adam\",  # Options: \"adam\", \"adamw\", \"sgd\"\n",
    "    \"weight_decay\": 1e-4,  # For AdamW\n",
    "    \n",
    "    # Learning Rate Schedule\n",
    "    \"use_lr_schedule\": True,  # Cosine annealing (Improvement #5)\n",
    "    \n",
    "    # Class Weights (Improvement #3)\n",
    "    \"use_class_weights\": True,\n",
    "    \n",
    "    # Ensemble (Improvement #7)\n",
    "    \"use_ensemble\": False,  # Set True to train multiple models\n",
    "    \"n_ensemble_models\": 3,\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"no_of_classes\": 7,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract commonly used values\n",
    "picture_size = cfg[\"picture_size\"]\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "epochs = cfg[\"epochs\"]\n",
    "learning_rate = cfg[\"learning_rate\"]\n",
    "no_of_classes = cfg[\"no_of_classes\"]\n",
    "dropout_rate = cfg[\"dropout_rate\"]\n",
    "dense_units = cfg[\"dense_units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6e6b725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  backbone: mobilenet\n",
      "  picture_size: 96\n",
      "  color_mode: rgb\n",
      "  batch_size: 64\n",
      "  epochs: 5\n",
      "  learning_rate: 0.0001\n",
      "  dropout_rate: 0.25\n",
      "  dense_units: [512]\n",
      "  aug_level: light\n",
      "  precision: mixed\n",
      "  fine_tune: True\n",
      "  fine_tune_epochs: 20\n",
      "  optimizer: adam\n",
      "  weight_decay: 0.0001\n",
      "  use_lr_schedule: True\n",
      "  use_class_weights: True\n",
      "  use_ensemble: False\n",
      "  n_ensemble_models: 3\n",
      "  no_of_classes: 7\n",
      "  head_epochs: 10\n",
      "  fine_tune_unfreeze_layers: 30\n"
     ]
    }
   ],
   "source": [
    "# near your cfg definition (apply before building generators)\n",
    "cfg.update({\n",
    "    \"picture_size\": 96,\n",
    "    \"color_mode\": \"rgb\",\n",
    "    \"head_epochs\": 10,\n",
    "    \"fine_tune\": True,\n",
    "    \"fine_tune_epochs\": 20,\n",
    "    \"fine_tune_unfreeze_layers\": 30,\n",
    "})\n",
    "# rebind frequently used variables\n",
    "picture_size = cfg[\"picture_size\"]\n",
    "color_mode = cfg[\"color_mode\"]\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365719a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using 'light' augmentation level for training data\n",
      "âœ… Using 'light' augmentation level for validation data\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Data Augmentation\n",
    "# =======================================================================\n",
    "\n",
    "aug_map = {\n",
    "    \"none\": dict(rescale=1./255),\n",
    "    \n",
    "    \"light\": dict(\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    ),\n",
    "    \n",
    "    \"strong\": dict(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        zoom_range=0.2,\n",
    "        brightness_range=[0.7, 1.3],\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ),\n",
    "    \n",
    "    \"aggressive\": dict(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.25,\n",
    "        height_shift_range=0.25,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.25,\n",
    "        brightness_range=[0.6, 1.4],\n",
    "        horizontal_flip=True,\n",
    "        channel_shift_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create data generators\n",
    "datagen_train = ImageDataGenerator(**aug_map[cfg[\"aug_level\"]])\n",
    "datagen_validation = ImageDataGenerator(**aug_map[cfg[\"aug_level\"]])\n",
    "\n",
    "print(f\"âœ… Using '{cfg['aug_level']}' augmentation level for training data\")\n",
    "print(f\"âœ… Using '{cfg['aug_level']}' augmentation level for validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db6c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n",
      "Train image shape: (96, 96, 3)\n",
      "Val image shape:   (96, 96, 3)\n",
      "\n",
      "âœ… Data loaded successfully\n",
      "Colour Mode: rgb\n",
      "Training samples: 28821\n",
      "Validation samples: 7066\n",
      "Class indices: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Load Training and Validation Data\n",
    "# =======================================================================\n",
    "\n",
    "# Determine color mode\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Prepare augmentation for train (remove rescale if present)\n",
    "train_aug = aug_map[cfg[\"aug_level\"]].copy()\n",
    "train_aug.pop(\"rescale\", None)\n",
    "train_aug.update({\"preprocessing_function\": preprocess_input})\n",
    "datagen_train = ImageDataGenerator(**train_aug)\n",
    "\n",
    "# Validation: only preprocessing (no augmentation)\n",
    "datagen_validation = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Create flows (must use the same picture_size and color_mode)\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    os.path.join(folder_path, \"train\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_set = datagen_validation.flow_from_directory(\n",
    "    os.path.join(folder_path, \"validation\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "print(\"Train image shape:\", train_set.image_shape)\n",
    "print(\"Val image shape:  \", validation_set.image_shape)\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully\")\n",
    "print (f\"Colour Mode: {color_mode}\")\n",
    "print(f\"Training samples: {train_set.n}\")\n",
    "print(f\"Validation samples: {validation_set.n}\")\n",
    "print(f\"Class indices: {train_set.class_indices}\")\n",
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a196b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Class weights calculated:\n",
      "       Angry: 1.031\n",
      "     Disgust: 9.443\n",
      "        Fear: 1.003\n",
      "       Happy: 0.575\n",
      "     Neutral: 0.826\n",
      "         Sad: 0.834\n",
      "    Surprise: 1.285\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Calculate Class Weights for Imbalanced Data\n",
    "# =======================================================================\n",
    "\n",
    "class_weight_dict = None\n",
    "\n",
    "if cfg.get(\"use_class_weights\", False):\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_set.classes),\n",
    "        y=train_set.classes\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"\\nâœ… Class weights calculated:\")\n",
    "    for emotion, weight in zip(class_labels, class_weights):\n",
    "        print(f\"  {emotion:>10}: {weight:.3f}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Class weights disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a23d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model(cfg):\n",
    "    \"\"\"Build and compile a MobileNetV2 transfer-learning model.\n",
    "       Returns (model, base_model).\n",
    "    \"\"\"\n",
    "\n",
    "    picture_size = cfg[\"picture_size\"]\n",
    "    color_mode = cfg[\"color_mode\"]\n",
    "    batch_size = cfg[\"batch_size\"]\n",
    "\n",
    "    channels = 1 if cfg[\"color_mode\"] == \"grayscale\" else 3\n",
    "    input_shape = (picture_size, picture_size, channels)\n",
    "\n",
    "    # Input\n",
    "    inp = Input(shape=input_shape)\n",
    "\n",
    "    # If grayscale, convert to RGB for ImageNet backbone\n",
    "    if channels == 1:\n",
    "        x = tf.keras.layers.Lambda(lambda z: tf.image.grayscale_to_rgb(z))(inp)\n",
    "    else:\n",
    "        x = inp\n",
    "\n",
    "    # Pre-trained backbone\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(picture_size, picture_size, 3), pooling=None)\n",
    "    # Attach to our RGB input\n",
    "    x = base_model(x, training=False)\n",
    "\n",
    "    # Classification head\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(cfg[\"dropout_rate\"])(x)\n",
    "    for units in cfg[\"dense_units\"]:\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        x = Dropout(cfg[\"dropout_rate\"])(x)\n",
    "    out = Dense(no_of_classes, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    # Freeze base model initially\n",
    "    if not cfg.get(\"fine_tune\", False):\n",
    "        base_model.trainable = False\n",
    "    else:\n",
    "        # Start by freezing; we'll unfreeze later for fine-tuning if desired\n",
    "        base_model.trainable = False\n",
    "\n",
    "    # Optimizer selection (same logic as your custom CNN)\n",
    "    optimizer_name = cfg.get(\"optimizer\", \"adam\")\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = Adam(learning_rate=cfg[\"learning_rate\"])\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=cfg[\"learning_rate\"],\n",
    "            weight_decay=cfg.get(\"weight_decay\", 1e-4)\n",
    "        )\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=cfg[\"learning_rate\"],\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa95eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Building model with 'mobilenet' backbone...\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Model Building Functions\n",
    "# =======================================================================\n",
    "\n",
    "\n",
    "def build_improved_cnn(cfg):\n",
    "    \"\"\"Enhanced CNN with better architecture.\"\"\"\n",
    "    \n",
    "    \n",
    "    #color_mode = cfg.get(\"color_mode\", \"grayscale\")\n",
    "    channels = 1 if color_mode == \"grayscale\" else 3\n",
    "    input_shape = (picture_size, picture_size, channels)\n",
    "    \n",
    "    print(f\"Building Improved CNN with input shape: {input_shape}\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1 - Double Conv\n",
    "    model.add(Conv2D(filters=32, \n",
    "                        kernel_size=(3, 3), \n",
    "                        padding='same', \n",
    "                        input_shape=input_shape,\n",
    "                        name='conv2d_1'))\n",
    "\n",
    "    # Block 2 - Double Conv\n",
    "    model.add(Conv2D(filters=128, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_2'))\n",
    "    model.add(BatchNormalization(name='bn_2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_2'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_2'))\n",
    "    \n",
    "    # Block 3 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(5, 5), \n",
    "                      padding='same',\n",
    "                      name='conv2d_3'))\n",
    "    model.add(BatchNormalization(name='bn_3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_3'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_3'))\n",
    "    \n",
    "    # Block 4 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_4'))\n",
    "    model.add(BatchNormalization(name='bn_4'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_4'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_4'))\n",
    "\n",
    "    \n",
    "    # Block 5 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_5'))\n",
    "    model.add(BatchNormalization(name='bn_5'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_5'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_5'))\n",
    "    \n",
    "    \"\"\"\n",
    "        # Block 6 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_6'))\n",
    "    model.add(BatchNormalization(name='bn_6'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_6'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_6'))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Output layer\n",
    "    # Add dense layers before output\n",
    "    model.add(Flatten())\n",
    "    for idx, units in enumerate(dense_units):\n",
    "        print(f\"Adding dense layer with {units} units\")\n",
    "        model.add(Dense(units, name=f\"dense_{idx+1}\"))\n",
    "        model.add(BatchNormalization(name=f\"bn_dense_{idx+1}\"))\n",
    "        model.add(Activation('relu', name=f\"act_dense_{idx+1}\"))\n",
    "        model.add(Dropout(dropout_rate, name=f\"dropout_dense_{idx+1}\"))\n",
    "    model.add(Dense(no_of_classes, activation='softmax', dtype='float32', name='output_layer'))\n",
    "\n",
    "   \n",
    "    # Compile with optimizer choice\n",
    "    optimizer_name = cfg.get(\"optimizer\", \"adam\")\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=cfg.get(\"weight_decay\", 1e-4)\n",
    "        )\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    \n",
    "    return model, None\n",
    "\n",
    "# Build model based on configuration\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Building model with '{cfg['backbone']}' backbone...\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d513e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Prepare augmentation dict for train (remove rescale if present)\n",
    "train_aug = aug_map[cfg[\"aug_level\"]].copy()\n",
    "train_aug.pop(\"rescale\", None)          # MobileNet uses preprocess_input\n",
    "train_aug.update({\"preprocessing_function\": preprocess_input})\n",
    "datagen_train = ImageDataGenerator(**train_aug)\n",
    "\n",
    "# Validation: only preprocessing (no augmentation)\n",
    "datagen_validation = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Prepare augmentation dict for validation\n",
    "if cfg[\"backbone\"] == \"mobilenet\":\n",
    "    train_preproc = dict(preprocessing_function=preprocess_input)\n",
    "    val_preproc = dict(preprocessing_function=preprocess_input)\n",
    "else:\n",
    "    train_preproc = val_preproc = {}\n",
    "\n",
    "aug = aug_map[cfg[\"aug_level\"]].copy()\n",
    "aug.update(train_preproc)\n",
    "datagen_train = ImageDataGenerator(**aug)\n",
    "\n",
    "# For validation: do NOT use augmentation, only preprocessing/rescale\n",
    "val_aug = {\"preprocessing_function\": preprocess_input} if cfg[\"backbone\"] == \"mobilenet\" else {\"rescale\": 1./255}\n",
    "datagen_validation = ImageDataGenerator(**val_aug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2689827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762914234.121603      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1762914234.122328      48 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "\n",
      "âœ… Model built successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_96             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cast_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Cast</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,591</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m3\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_96             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚       \u001b[38;5;34m655,872\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ cast_1 (\u001b[38;5;33mCast\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              â”‚         \u001b[38;5;34m3,591\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,917,447</span> (11.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,917,447\u001b[0m (11.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">659,463</span> (2.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m659,463\u001b[0m (2.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Replace the model building section with:\n",
    "if cfg[\"backbone\"] == \"custom_cnn\":\n",
    "    model, base_model = build_improved_cnn(cfg)\n",
    "elif cfg[\"backbone\"] == \"mobilenet\":\n",
    "    model, base_model = build_transfer_learning_model(cfg)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported backbone: {cfg['backbone']}\")\n",
    "\n",
    "print(f\"\\nâœ… Model built successfully\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45e05a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using cosine annealing learning rate schedule\n",
      "âœ… Callbacks configured: 4 callbacks\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Advanced Training Callbacks\n",
    "# =======================================================================\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                                    monitor='val_accuracy',\n",
    "                                    save_best_only=True,\n",
    "                                    mode='max',\n",
    "                                    verbose=1\n",
    "                                )\n",
    "\n",
    "# Early stopping with more patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=5,  # Increased from 10\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "\n",
    "# Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2,  # Increased from 5\n",
    "                    min_delta=0.0001,\n",
    "                    verbose=1\n",
    "                )\n",
    "\n",
    "# Cosine annealing schedule (Improvement #5)\n",
    "def cosine_annealing(epoch, lr):\n",
    "    \"\"\"Cosine annealing learning rate schedule.\"\"\"\n",
    "    import math\n",
    "    # Fix:\n",
    "    total_epochs = cfg.get(\"epochs\", 50)\n",
    "    initial_lr = cfg.get(\"learning_rate\", 1e-4)\n",
    "    min_lr = 1e-7\n",
    "    \n",
    "    if epoch < 5:  # Warmup phase\n",
    "        return initial_lr * (epoch + 1) / 5\n",
    "    else:\n",
    "        progress = (epoch - 5) / (epochs - 5)\n",
    "        return min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "if cfg.get(\"use_lr_schedule\", False):\n",
    "    lr_scheduler = LearningRateScheduler(cosine_annealing, verbose=1)\n",
    "    callbacks_list.append(lr_scheduler)\n",
    "    print(\"âœ… Using cosine annealing learning rate schedule\")\n",
    "else:\n",
    "    callbacks_list.append(reduce_lr)\n",
    "    print(\"âœ… Using ReduceLROnPlateau\")\n",
    "\n",
    "print(f\"âœ… Callbacks configured: {len(callbacks_list)} callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "205a7c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg picture_size: 96\n",
      "cfg color_mode: rgb\n",
      "train_set.image_shape: (96, 96, 3)\n",
      "validation_set.image_shape: (96, 96, 3)\n",
      "model.input_shape: (None, 96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"cfg picture_size:\", cfg.get(\"picture_size\"))\n",
    "print(\"cfg color_mode:\", cfg.get(\"color_mode\"))\n",
    "print(\"train_set.image_shape:\", getattr(train_set, \"image_shape\", None))\n",
    "print(\"validation_set.image_shape:\", getattr(validation_set, \"image_shape\", None))\n",
    "print(\"model.input_shape:\", getattr(model, \"input_shape\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ecd8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING TRAINING\n",
      "======================================================================\n",
      "Target: 5 epochs with early stopping\n",
      "Batch size: 64\n",
      "Learning rate: 0.0001\n",
      "Augmentation: light\n",
      "Backbone: mobilenet\n",
      "Class weights: Enabled\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 2e-05.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762914244.761577     106 service.cc:148] XLA service 0x7d683c010020 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1762914244.762765     106 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762914244.762784     106 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1762914245.777780     106 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/451\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m1:36:40\u001b[0m 13s/step - accuracy: 0.0469 - loss: 3.1540"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1762914251.672108     106 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - accuracy: 0.1654 - loss: 2.3981\n",
      "Epoch 1: val_accuracy improved from -inf to 0.31588, saving model to best_model.keras\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 538ms/step - accuracy: 0.1655 - loss: 2.3978 - val_accuracy: 0.3159 - val_loss: 1.7819 - learning_rate: 2.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 4e-05.\n",
      "Epoch 2/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 208ms/step - accuracy: 0.2560 - loss: 1.9742\n",
      "Epoch 2: val_accuracy improved from 0.31588 to 0.34758, saving model to best_model.keras\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 227ms/step - accuracy: 0.2560 - loss: 1.9741 - val_accuracy: 0.3476 - val_loss: 1.7024 - learning_rate: 4.0000e-05\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 6.000000000000001e-05.\n",
      "Epoch 3/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.2977 - loss: 1.8138\n",
      "Epoch 3: val_accuracy improved from 0.34758 to 0.36824, saving model to best_model.keras\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 226ms/step - accuracy: 0.2977 - loss: 1.8138 - val_accuracy: 0.3682 - val_loss: 1.6578 - learning_rate: 6.0000e-05\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 8e-05.\n",
      "Epoch 4/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step - accuracy: 0.3216 - loss: 1.7341\n",
      "Epoch 4: val_accuracy improved from 0.36824 to 0.39103, saving model to best_model.keras\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 224ms/step - accuracy: 0.3216 - loss: 1.7340 - val_accuracy: 0.3910 - val_loss: 1.6050 - learning_rate: 8.0000e-05\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0001.\n",
      "Epoch 5/10\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.3429 - loss: 1.6736\n",
      "Epoch 5: val_accuracy did not improve from 0.39103\n",
      "\u001b[1m451/451\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 230ms/step - accuracy: 0.3429 - loss: 1.6736 - val_accuracy: 0.3881 - val_loss: 1.6112 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/1325493063.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Full head training (frozen backbone)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m history_head = model.fit(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"head_epochs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# e.g. 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/3643549818.py\u001b[0m in \u001b[0;36mcosine_annealing\u001b[0;34m(epoch, lr)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minitial_lr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin_lr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minitial_lr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_lr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Model Training\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: {epochs} epochs with early stopping\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Augmentation: {cfg['aug_level']}\")\n",
    "print(f\"Backbone: {cfg['backbone']}\")\n",
    "print(f\"Class weights: {'Enabled' if class_weight_dict else 'Disabled'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Full head training (frozen backbone)\n",
    "history_head = model.fit(\n",
    "    train_set,\n",
    "    epochs=cfg[\"head_epochs\"],  # e.g. 10\n",
    "    validation_data=validation_set,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "                        \n",
    "                    \n",
    "# Fine-tuning stage\n",
    "if cfg.get(\"fine_tune\", False) and cfg.get(\"fine_tune_epochs\", 0) > 0 and base_model is not None:\n",
    "    print(\"ğŸ”§ Starting fine-tuning...\")\n",
    "    \n",
    "    # Unfreeze top layers\n",
    "    N = cfg.get(\"fine_tune_unfreeze_layers\", 30)\n",
    "    for layer in base_model.layers[:-N]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[-N:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Recompile with lower LR\n",
    "    ft_lr = cfg.get(\"learning_rate\", 1e-4) * 0.1\n",
    "    model.compile(optimizer=Adam(learning_rate=ft_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Continue training\n",
    "    history_ft = model.fit(\n",
    "        train_set,\n",
    "        epochs=cfg[\"head_epochs\"] + cfg[\"fine_tune_epochs\"],\n",
    "        initial_epoch=cfg[\"head_epochs\"],\n",
    "        validation_data=validation_set,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Training History Visualization\n",
    "# =======================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "val_loss, val_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"Training Loss:       {train_loss:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692049e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Detailed Performance Analysis\n",
    "# =======================================================================\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load best model\n",
    "my_model = load_model('best_model.keras', compile=False)\n",
    "\n",
    "# Get predictions\n",
    "print(\"Generating predictions on validation set...\")\n",
    "validation_set.reset()\n",
    "predictions = my_model.predict(validation_set, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = validation_set.classes\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = np.mean(predicted_classes == true_classes)\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "print(f\"\\nâœ… Best Model Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be851daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Performance Visualizations\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            linewidths=0.5,\n",
    "            linecolor='gray')\n",
    "plt.title('Confusion Matrix - Emotion Recognition', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Normalized Confusion Matrix (%)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ DETAILED CLASSIFICATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(true_classes, predicted_classes, \n",
    "                           target_names=class_labels,\n",
    "                           digits=4))\n",
    "\n",
    "# 4. Per-Class Metrics Bar Chart\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    true_classes, predicted_classes, labels=range(7)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precision, width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, recall, width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Emotion', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Emotion Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Summary Table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Emotion': class_labels,\n",
    "    'Precision': [f'{p:.2%}' for p in precision],\n",
    "    'Recall': [f'{r:.2%}' for r in recall],\n",
    "    'F1-Score': [f'{f:.2%}' for f in f1],\n",
    "    'Support': support.astype(int)\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š PERFORMANCE SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 6. Misclassification Analysis\n",
    "misclassified = cm.copy()\n",
    "np.fill_diagonal(misclassified, 0)\n",
    "\n",
    "top_confusions = []\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i != j:\n",
    "            top_confusions.append((class_labels[i], class_labels[j], misclassified[i, j]))\n",
    "\n",
    "top_confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš ï¸  TOP 5 MISCLASSIFICATION PAIRS\")\n",
    "print(\"=\"*70)\n",
    "for true_label, pred_label, count in top_confusions[:5]:\n",
    "    print(f\"{true_label:>10} â†’ {pred_label:<10} : {count:>4} times\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8ce84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Save Model with Timestamp\n",
    "# =======================================================================\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"emotion_recognition_{timestamp}.keras\"\n",
    "\n",
    "if is_on_kaggle():\n",
    "    output_path = \"/kaggle/working/\"+timestamp+\"/\"\n",
    "else:\n",
    "    output_path = \"outputs/\"+timestamp+\"/\"\n",
    "    \n",
    "try:\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    full_path = Path(output_path) / model_name\n",
    "    model.save(str(full_path))\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Model saved: {full_path}\")\n",
    "print(f\"ğŸ“Š Final Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cf82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Ensemble Training (Optional)\n",
    "# =======================================================================\n",
    "\n",
    "if cfg.get(\"use_ensemble\", False):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ¯ TRAINING ENSEMBLE MODELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    n_models = cfg.get(\"n_ensemble_models\", 3)\n",
    "    ensemble_models = []\n",
    "    \n",
    "    for i in range(n_models):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Training Ensemble Model {i+1}/{n_models}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Vary hyperparameters slightly\n",
    "        cfg_copy = cfg.copy()\n",
    "        cfg_copy[\"learning_rate\"] = cfg[\"learning_rate\"] * (0.8 + 0.4 * np.random.random())\n",
    "        cfg_copy[\"dropout_rate\"] = 0.3 + 0.2 * np.random.random()\n",
    "        \n",
    "        # Build model\n",
    "        if cfg[\"backbone\"] in [\"mobilenet\", \"efficientnet\"]:\n",
    "            ensemble_model, _ = build_transfer_learning_model(cfg_copy)\n",
    "        else:\n",
    "            ensemble_model, _ = build_improved_cnn(cfg_copy)\n",
    "        \n",
    "        # Train\n",
    "        history_ens = ensemble_model.fit(\n",
    "            train_set,\n",
    "            validation_data=validation_set,\n",
    "            epochs=30,\n",
    "            callbacks=[early_stopping, reduce_lr],\n",
    "            class_weight=class_weight_dict,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        ensemble_models.append(ensemble_model)\n",
    "        \n",
    "        # Save\n",
    "        ensemble_model.save(f\"ensemble_model_{i+1}.keras\")\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ¯ ENSEMBLE PREDICTION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    validation_set.reset()\n",
    "    ensemble_predictions = []\n",
    "    \n",
    "    for i, ens_model in enumerate(ensemble_models):\n",
    "        print(f\"Getting predictions from model {i+1}...\")\n",
    "        pred = ens_model.predict(validation_set, verbose=0)\n",
    "        ensemble_predictions.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    avg_predictions = np.mean(ensemble_predictions, axis=0)\n",
    "    ensemble_predicted_classes = np.argmax(avg_predictions, axis=1)\n",
    "    \n",
    "    # Evaluate ensemble\n",
    "    ensemble_accuracy = np.mean(ensemble_predicted_classes == true_classes)\n",
    "    \n",
    "    print(f\"\\nâœ… Ensemble Accuracy: {ensemble_accuracy:.4f} ({ensemble_accuracy*100:.2f}%)\")\n",
    "    print(f\"ğŸ“ˆ Improvement over single model: {(ensemble_accuracy - accuracy)*100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Ensemble training disabled (set 'use_ensemble': True to enable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff50765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Sample Predictions Visualization\n",
    "# =======================================================================\n",
    "import random\n",
    "\n",
    "# Get a random batch from validation set\n",
    "random_batch_idx = random.randint(0, len(validation_set) - 1)\n",
    "sample_images, sample_labels = validation_set[random_batch_idx]\n",
    "\n",
    "# Predict\n",
    "sample_preds = my_model.predict(sample_images, verbose=0)\n",
    "sample_pred_classes = np.argmax(sample_preds, axis=1)\n",
    "sample_true_classes = np.argmax(sample_labels, axis=1)\n",
    "\n",
    "# Randomly select 16 indices from the batch\n",
    "num_samples = min(16, len(sample_images))\n",
    "random_indices = random.sample(range(len(sample_images)), num_samples)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axes[i].imshow(sample_images[idx].squeeze(), cmap='gray')\n",
    "    \n",
    "    true_label = class_labels[sample_true_classes[idx]]\n",
    "    pred_label = class_labels[sample_pred_classes[idx]]\n",
    "    confidence = sample_preds[idx][sample_pred_classes[idx]] * 100\n",
    "\n",
    "    # Color: green if correct, red if wrong\n",
    "    color = 'green' if sample_true_classes[idx] == sample_pred_classes[idx] else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                     color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2728baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global policy:\", mixed_precision.global_policy())\n",
    "print(\"Model dtype policy:\", model.dtype_policy)   # or model.input_dtype / model.output_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066da021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saved = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
