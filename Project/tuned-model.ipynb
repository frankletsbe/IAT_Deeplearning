{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0697d144",
   "metadata": {},
   "source": [
    "Tuning opportunities\n",
    "1. Configuration-Based Hyperparameter Tuning\n",
    "2. Enhanced Data Augmentation\n",
    "3. Improved Model Architecture\n",
    "4. Advanced Callbacks\n",
    "5. Transfer Learning (Often Best Results)\n",
    "6. Class Imbalance Handling\n",
    "7. Ensemble Methods\n",
    "8. Systematic Tuning Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9fe6a",
   "metadata": {},
   "source": [
    "Quick Start Guide:\n",
    "For best results immediately: Set \"backbone\": \"mobilenet\" and \"fine_tune\": True\n",
    "For faster training: Set \"backbone\": \"custom_cnn\" and \"aug_level\": \"light\"\n",
    "For maximum accuracy: Enable ensemble with \"use_ensemble\": True\n",
    "For experimentation: Uncomment Cell 17 to run systematic tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Facial Emotion Recognition - Enhanced with 8 Tuning Improvements\n",
    "# =======================================================================\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning libraries\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Flatten, Dropout, Dense, Input, \n",
    "    GlobalAveragePooling2D, Conv2D, \n",
    "    BatchNormalization, Activation, MaxPooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (ModelCheckpoint, EarlyStopping, \n",
    "    ReduceLROnPlateau, LearningRateScheduler)\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"= is device GPU device\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable mixed precision only if a GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision enabled: mixed_float16\")\n",
    "else:\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    print(\"No GPU detected â€” using float32\")\n",
    "    \n",
    "print(\"Global policy:\", mixed_precision.global_policy())\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c880f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Environment Detection\n",
    "# =======================================================================\n",
    "\n",
    "def is_on_kaggle():\n",
    "    \"\"\"Detect if running on Kaggle.\"\"\"\n",
    "    return os.path.exists('/kaggle/input')\n",
    "\n",
    "def get_data_path():\n",
    "    \"\"\"Detect environment and return appropriate data path.\"\"\"\n",
    "    if is_on_kaggle():\n",
    "        print(\"ðŸŒ Running on Kaggle\")\n",
    "        import kagglehub\n",
    "        image_path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
    "        folder_path = os.path.join(image_path, \"images\")\n",
    "    else:\n",
    "        print(\"ðŸ’» Running on local machine\")\n",
    "        folder_path = \"data/images/\"\n",
    "    \n",
    "    return folder_path\n",
    "\n",
    "folder_path = get_data_path()\n",
    "print(f\"Data folder path: {folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Configuration with Tuning Options\n",
    "# =======================================================================\n",
    "\n",
    "cfg = {\n",
    "    # Model Architecture\n",
    "    \"backbone\": \"mobilenet\",  # Options: \"custom_cnn\", \"mobilenet\"\n",
    "    \n",
    "    # Image Parameters\n",
    "    \"picture_size\": 48,  # Try: 64, 96 for more detail\n",
    "    \"color_mode\": \"grayscale\",  # \"grayscale\" or \"rgb\" (rgb required for transfer learning)\n",
    "    \n",
    "    # Training Parameters\n",
    "    \"batch_size\": 64,  # Reduced from 128 for better gradient estimates\n",
    "    \"epochs\": 1,  # Increased from 30\n",
    "    \"learning_rate\": 0.0001,  # Adjusted learning rate\n",
    "    \"dropout_rate\": 0.25,  # Increased from 0.25\n",
    "     \n",
    "    # Dense layers\n",
    "    \"dense_units\": [512],  # Added an extra dense layer (Improvement #6)\n",
    "    \n",
    "    # Data Augmentation (Improvement #2)\n",
    "    \"aug_level\": \"light\",  # Options: \"none\", \"light\", \"strong\", \"aggressive\"\n",
    "    \n",
    "    # Advanced Training\n",
    "    \"precision\": \"mixed\",  # \"float32\" or \"mixed\"\n",
    "    \"fine_tune\": True,  # Enable fine-tuning for transfer learning\n",
    "    \"fine_tune_epochs\": 0,\n",
    "    \n",
    "    # Optimizer\n",
    "    \"optimizer\": \"adam\",  # Options: \"adam\", \"adamw\", \"sgd\"\n",
    "    \"weight_decay\": 1e-4,  # For AdamW\n",
    "    \n",
    "    # Learning Rate Schedule\n",
    "    \"use_lr_schedule\": True,  # Cosine annealing (Improvement #5)\n",
    "    \n",
    "    # Class Weights (Improvement #3)\n",
    "    \"use_class_weights\": True,\n",
    "    \n",
    "    # Ensemble (Improvement #7)\n",
    "    \"use_ensemble\": False,  # Set True to train multiple models\n",
    "    \"n_ensemble_models\": 3,\n",
    "    \n",
    "    # Model Parameters\n",
    "    \"no_of_classes\": 7,\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Extract commonly used values\n",
    "picture_size = cfg[\"picture_size\"]\n",
    "batch_size = cfg[\"batch_size\"]\n",
    "epochs = cfg[\"epochs\"]\n",
    "learning_rate = cfg[\"learning_rate\"]\n",
    "no_of_classes = cfg[\"no_of_classes\"]\n",
    "dropout_rate = cfg[\"dropout_rate\"]\n",
    "dense_units = cfg[\"dense_units\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# near your cfg definition (apply before building generators)\n",
    "cfg.update({\n",
    "    \"picture_size\": 96,\n",
    "    \"color_mode\": \"rgb\",\n",
    "    \"head_epochs\": 1,\n",
    "    \"fine_tune\": True,\n",
    "    \"fine_tune_epochs\": 2,\n",
    "    \"fine_tune_unfreeze_layers\": 30,\n",
    "})\n",
    "# rebind frequently used variables\n",
    "picture_size = cfg[\"picture_size\"]\n",
    "color_mode = cfg[\"color_mode\"]\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in cfg.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a207a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning grid\n",
    "param_grid = {\n",
    "    'learning_rate': [1e-4, 5e-4, 1e-3],\n",
    "    'dropout_rate': [0.2, 0.3, 0.5],\n",
    "    'dense_units': [[256], [512], [512, 256]],\n",
    "    'picture_size': [48, 64, 96]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_start_config(strategy=\"accuracy\"):\n",
    "    \"\"\"Return optimized config for different strategies.\"\"\"\n",
    "    configs = {\n",
    "        \"accuracy\": {\n",
    "            \"backbone\": \"mobilenet\",\n",
    "            \"fine_tune\": True,\n",
    "            \"picture_size\": 96,\n",
    "            \"aug_level\": \"strong\"\n",
    "        },\n",
    "        \"speed\": {\n",
    "            \"backbone\": \"custom_cnn\",\n",
    "            \"aug_level\": \"light\",\n",
    "            \"picture_size\": 48\n",
    "        },\n",
    "        \"ensemble\": {\n",
    "            \"use_ensemble\": True,\n",
    "            \"n_ensemble_models\": 3\n",
    "        }\n",
    "    }\n",
    "    return configs.get(strategy, configs[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccaa1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple grid search implementation\n",
    "def grid_search(config_grid, train_func):\n",
    "    best_score = 0\n",
    "    best_config = None\n",
    "    \n",
    "    # Generate all combinations\n",
    "    import itertools\n",
    "    keys = config_grid.keys()\n",
    "    values = config_grid.values()\n",
    "    combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    \n",
    "    for config in combinations:\n",
    "        print(f\"Testing config: {config}\")\n",
    "        score = train_func(config)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_config = config\n",
    "    \n",
    "    return best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365719a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Data Augmentation\n",
    "# =======================================================================\n",
    "\n",
    "aug_map = {\n",
    "    \"none\": dict(rescale=1./255),\n",
    "    \n",
    "    \"light\": dict(\n",
    "        rescale=1./255,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    ),\n",
    "    \n",
    "    \"strong\": dict(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        zoom_range=0.2,\n",
    "        brightness_range=[0.7, 1.3],\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    ),\n",
    "    \n",
    "    \"aggressive\": dict(\n",
    "        rescale=1./255,\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.25,\n",
    "        height_shift_range=0.25,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.25,\n",
    "        brightness_range=[0.6, 1.4],\n",
    "        horizontal_flip=True,\n",
    "        channel_shift_range=0.2,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6c80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "aug_map[\"advanced\"] = dict(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.15,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d16744",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create data generators\n",
    "datagen_train = ImageDataGenerator(**aug_map[cfg[\"aug_level\"]])\n",
    "datagen_validation = ImageDataGenerator(**aug_map[cfg[\"aug_level\"]])\n",
    "\n",
    "print(f\"âœ… Using '{cfg['aug_level']}' augmentation level for training data\")\n",
    "print(f\"âœ… Using '{cfg['aug_level']}' augmentation level for validation data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db6c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Load Training and Validation Data\n",
    "# =======================================================================\n",
    "\n",
    "# Determine color mode\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "# Prepare augmentation for train (remove rescale if present)\n",
    "train_aug = aug_map[cfg[\"aug_level\"]].copy()\n",
    "train_aug.pop(\"rescale\", None)\n",
    "train_aug.update({\"preprocessing_function\": preprocess_input})\n",
    "datagen_train = ImageDataGenerator(**train_aug)\n",
    "\n",
    "# Validation: only preprocessing (no augmentation)\n",
    "datagen_validation = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "# Create flows (must use the same picture_size and color_mode)\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    os.path.join(folder_path, \"train\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "validation_set = datagen_validation.flow_from_directory(\n",
    "    os.path.join(folder_path, \"validation\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode=color_mode,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "print(\"Train image shape:\", train_set.image_shape)\n",
    "print(\"Val image shape:  \", validation_set.image_shape)\n",
    "\n",
    "print(f\"\\nâœ… Data loaded successfully\")\n",
    "print (f\"Colour Mode: {color_mode}\")\n",
    "print(f\"Training samples: {train_set.n}\")\n",
    "print(f\"Validation samples: {validation_set.n}\")\n",
    "print(f\"Class indices: {train_set.class_indices}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a196b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Calculate Class Weights for Imbalanced Data\n",
    "# =======================================================================\n",
    "\n",
    "class_weight_dict = None\n",
    "\n",
    "if cfg.get(\"use_class_weights\", False):\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_set.classes),\n",
    "        y=train_set.classes\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    print(\"\\nâœ… Class weights calculated:\")\n",
    "    for emotion, weight in zip(class_labels, class_weights):\n",
    "        print(f\"  {emotion:>10}: {weight:.3f}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Class weights disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7e1dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In your model building function\n",
    "# Add spatial dropout for better regularization\n",
    "from tensorflow.keras.layers import SpatialDropout2D\n",
    "\n",
    "# Add residual connections for better gradient flow\n",
    "def add_residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # Add shortcut if dimensions match\n",
    "    if shortcut.shape[-1] == filters:\n",
    "        x = tf.keras.layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd33301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard callback for monitoring\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir='./logs',\n",
    "    histogram_freq=1,\n",
    "    write_graph=True\n",
    ")\n",
    "\n",
    "# Custom callback for logging\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Epoch {epoch+1}: val_accuracy={logs['val_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a23d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_learning_model(cfg):\n",
    "    backbone_map = {'mobilenet': MobileNetV2, 'efficientnet': EfficientNetB0}\n",
    "    backbone_class = backbone_map.get(cfg['backbone'])\n",
    "    if backbone_class is None:\n",
    "        raise ValueError(\"Unsupported backbone\")\n",
    "\n",
    "    picture_size = cfg['picture_size']\n",
    "    channels = 1 if cfg['color_mode'] == 'grayscale' else 3\n",
    "    inp = Input(shape=(picture_size, picture_size, channels))\n",
    "\n",
    "    # convert grayscale->rgb if needed\n",
    "    x = tf.keras.layers.Lambda(lambda z: tf.image.grayscale_to_rgb(z))(inp) if channels == 1 else inp\n",
    "\n",
    "    base_model = backbone_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(picture_size, picture_size, 3)\n",
    "    )\n",
    "    base_model.trainable = False\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(cfg['dropout_rate'])(x)\n",
    "    for units in cfg['dense_units']:\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        x = Dropout(cfg['dropout_rate'])(x)\n",
    "    out = Dense(cfg['no_of_classes'], activation='softmax', dtype='float32')(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    # Optimizer selection\n",
    "    optimizer_name = cfg.get(\"optimizer\", \"adam\")\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = Adam(learning_rate=cfg[\"learning_rate\"])\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=cfg[\"learning_rate\"],\n",
    "            weight_decay=cfg.get(\"weight_decay\", 1e-4)\n",
    "        )\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=cfg[\"learning_rate\"],\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "    \n",
    "    # Focal loss implementation\n",
    "    def focal_loss(gamma=2., alpha=0.25):\n",
    "        def fl(y_true, y_pred):\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "            pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "            return -alpha * tf.pow(1. - pt, gamma) * tf.math.log(pt)\n",
    "        return fl\n",
    "\n",
    "    # Select loss function\n",
    "    if cfg.get(\"use_focal_loss\", False):\n",
    "        loss_fn = focal_loss()\n",
    "    else:\n",
    "        loss_fn = 'categorical_crossentropy'\n",
    "        \n",
    "    model.compile(optimizer=opt, loss=loss_fn, metrics=['accuracy'])\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa95eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Model Building Functions\n",
    "# =======================================================================\n",
    "\n",
    "\n",
    "def build_improved_cnn(cfg):\n",
    "    \"\"\"Enhanced CNN with better architecture.\"\"\"\n",
    "    \n",
    "    \n",
    "    #color_mode = cfg.get(\"color_mode\", \"grayscale\")\n",
    "    channels = 1 if color_mode == \"grayscale\" else 3\n",
    "    input_shape = (picture_size, picture_size, channels)\n",
    "    \n",
    "    print(f\"Building Improved CNN with input shape: {input_shape}\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Block 1 - Double Conv\n",
    "    model.add(Conv2D(filters=32, \n",
    "                        kernel_size=(3, 3), \n",
    "                        padding='same', \n",
    "                        input_shape=input_shape,\n",
    "                        name='conv2d_1'))\n",
    "\n",
    "    # Block 2 - Double Conv\n",
    "    model.add(Conv2D(filters=128, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_2'))\n",
    "    model.add(BatchNormalization(name='bn_2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_2'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_2'))\n",
    "    \n",
    "    # Block 3 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(5, 5), \n",
    "                      padding='same',\n",
    "                      name='conv2d_3'))\n",
    "    model.add(BatchNormalization(name='bn_3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_3'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_3'))\n",
    "    \n",
    "    # Block 4 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_4'))\n",
    "    model.add(BatchNormalization(name='bn_4'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_4'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_4'))\n",
    "\n",
    "    \n",
    "    # Block 5 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_5'))\n",
    "    model.add(BatchNormalization(name='bn_5'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_5'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_5'))\n",
    "     \n",
    "    \n",
    "    # Output layer\n",
    "    # Add dense layers before output\n",
    "    model.add(Flatten())\n",
    "    for idx, units in enumerate(dense_units):\n",
    "        print(f\"Adding dense layer with {units} units\")\n",
    "        model.add(Dense(units, name=f\"dense_{idx+1}\"))\n",
    "        model.add(BatchNormalization(name=f\"bn_dense_{idx+1}\"))\n",
    "        model.add(Activation('relu', name=f\"act_dense_{idx+1}\"))\n",
    "        model.add(Dropout(dropout_rate, name=f\"dropout_dense_{idx+1}\"))\n",
    "    model.add(Dense(no_of_classes, activation='softmax', dtype='float32', name='output_layer'))\n",
    "\n",
    "   \n",
    "    # Compile with optimizer choice\n",
    "    optimizer_name = cfg.get(\"optimizer\", \"adam\")\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=cfg.get(\"weight_decay\", 1e-4)\n",
    "        )\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    \n",
    "    return model, None\n",
    "\n",
    "# Build model based on configuration\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Building model with '{cfg['backbone']}' backbone...\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2689827",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace the model building section with:\n",
    "if cfg[\"backbone\"] == \"custom_cnn\":\n",
    "    # Ensure grayscale inputs are properly handled\n",
    "    channels = 1 if cfg[\"color_mode\"] == \"grayscale\" else 3\n",
    "    input_shape = (cfg[\"picture_size\"], cfg[\"picture_size\"], channels)\n",
    "elif cfg[\"backbone\"] == \"mobilenet\":\n",
    "    model, base_model = build_transfer_learning_model(cfg)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported backbone: {cfg['backbone']}\")\n",
    "\n",
    "\n",
    "print(f\"\\nâœ… Model built successfully\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265576e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Advanced Training Callbacks\n",
    "# =======================================================================\n",
    "\n",
    "# Checkpoint\n",
    "checkpoint = ModelCheckpoint('best_model.keras',\n",
    "                                    monitor='val_accuracy',\n",
    "                                    save_best_only=True,\n",
    "                                    mode='max',\n",
    "                                    verbose=1\n",
    "                                )\n",
    "\n",
    "# Early stopping with more patience\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                    patience=5,  # Increased from 10\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1\n",
    "                                )\n",
    "\n",
    "# Reduce learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "                    monitor='val_loss',\n",
    "                    factor=0.5,\n",
    "                    patience=2,  # Increased from 5\n",
    "                    min_delta=0.0001,\n",
    "                    verbose=1\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b27ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Cosine annealing schedule (Improvement #5)\n",
    "def cosine_annealing(epoch, lr):\n",
    "    initial_lr = float(cfg.get(\"learning_rate\", 1e-4))\n",
    "    min_lr = 1e-7\n",
    "    warmup = 5\n",
    "    total_epochs = max(int(cfg.get(\"epochs\", 1)), warmup + 1)\n",
    "    \n",
    "    if total_epochs <= 0:\n",
    "        total_epochs = int(cfg.get(\"epochs\", 1))\n",
    "\n",
    "    # If total_epochs is very small, avoid division by zero â€” just do simple schedule\n",
    "    if total_epochs <= warmup:\n",
    "        # linear ramp (or constant) to avoid division by zero\n",
    "        return float(initial_lr * (epoch + 1) / max(1, total_epochs))\n",
    "\n",
    "    # Warmup phase\n",
    "    if epoch < warmup:\n",
    "        return float(initial_lr * (epoch + 1) / warmup)\n",
    "\n",
    "    # Cosine annealing phase\n",
    "    denom = float(total_epochs - warmup)\n",
    "    progress = float(epoch - warmup) / denom\n",
    "    lr_out = min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "    return float(max(lr_out, min_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callbacks_list = [checkpoint, early_stopping, reduce_lr]\n",
    "\n",
    "if cfg.get(\"use_lr_schedule\", False):\n",
    "    lr_scheduler = LearningRateScheduler(cosine_annealing, verbose=1)\n",
    "    callbacks_list.append(lr_scheduler)\n",
    "    print(\"âœ… Using cosine annealing learning rate schedule\")\n",
    "else:\n",
    "    callbacks_list.append(reduce_lr)\n",
    "    print(\"âœ… Using ReduceLROnPlateau\")\n",
    "\n",
    "print(f\"âœ… Callbacks configured: {len(callbacks_list)} callbacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205a7c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cfg picture_size:\", cfg.get(\"picture_size\"))\n",
    "print(\"cfg color_mode:\", cfg.get(\"color_mode\"))\n",
    "print(\"train_set.image_shape:\", getattr(train_set, \"image_shape\", None))\n",
    "print(\"validation_set.image_shape:\", getattr(validation_set, \"image_shape\", None))\n",
    "print(\"model.input_shape:\", getattr(model, \"input_shape\", None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd8386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Model Training\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸš€ STARTING TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Target: {epochs} epochs with early stopping\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Augmentation: {cfg['aug_level']}\")\n",
    "print(f\"Backbone: {cfg['backbone']}\")\n",
    "print(f\"Class weights: {'Enabled' if class_weight_dict else 'Disabled'}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# sanity checks (place before training)\n",
    "assert train_set.image_shape == model.input_shape[1:], \\\n",
    "    f\"Mismatch: Generator {train_set.image_shape} vs Model {model.input_shape[1:]}\"\n",
    "assert validation_set.image_shape == model.input_shape[1:], \\\n",
    "    f\"Mismatch: Validation generator {validation_set.image_shape} vs Model {model.input_shape[1:]}\"\n",
    "    \n",
    "# Full head training (frozen backbone)\n",
    "history_head = model.fit(\n",
    "    train_set,\n",
    "    epochs=cfg[\"head_epochs\"],  # e.g. 10\n",
    "    validation_data=validation_set,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1,\n",
    "    class_weight=class_weight_dict\n",
    ")\n",
    "                        \n",
    "                    \n",
    "# Fine-tuning stage\n",
    "if cfg.get(\"fine_tune\", False) and cfg.get(\"fine_tune_epochs\", 0) > 0 and base_model is not None:\n",
    "    print(\"ðŸ”§ Starting fine-tuning...\")\n",
    "    \n",
    "    # Unfreeze top layers\n",
    "    N = cfg.get(\"fine_tune_unfreeze_layers\", 30)\n",
    "    for layer in base_model.layers[:-N]:\n",
    "        layer.trainable = False\n",
    "    for layer in base_model.layers[-N:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Recompile with lower LR\n",
    "    ft_lr = cfg.get(\"learning_rate\", 1e-4) * 0.1\n",
    "    model.compile(optimizer=Adam(learning_rate=ft_lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Continue training\n",
    "    history_ft = model.fit(\n",
    "        train_set,\n",
    "        epochs=cfg[\"head_epochs\"] + cfg[\"fine_tune_epochs\"],\n",
    "        initial_epoch=cfg[\"head_epochs\"],\n",
    "        validation_data=validation_set,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1,\n",
    "        class_weight=class_weight_dict,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dccf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Save model with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"emotion_recognition_{timestamp}.keras\"\n",
    "\n",
    "output_dir = \"/kaggle/working/\" + timestamp + \"/\" if os.path.exists('/kaggle/input') else \"outputs/\" + timestamp + \"/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "full_path = Path(output_dir) / model_name\n",
    "try:\n",
    "    model.save(str(full_path))\n",
    "    print(f\"\\nðŸ’¾ Model saved: {full_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac059942",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = history_ft if history_ft is not None else history_head\n",
    "\n",
    "# Helper to safely get values\n",
    "def get_vals(h, key):\n",
    "    return h.history.get(key, []) if h is not None else []\n",
    "\n",
    "head_acc = get_vals(history_head, 'accuracy')\n",
    "head_val_acc = get_vals(history_head, 'val_accuracy')\n",
    "head_loss = get_vals(history_head, 'loss')\n",
    "head_val_loss = get_vals(history_head, 'val_loss')\n",
    "\n",
    "ft_acc = get_vals(history_ft, 'accuracy')\n",
    "ft_val_acc = get_vals(history_ft, 'val_accuracy')\n",
    "ft_loss = get_vals(history_ft, 'loss')\n",
    "ft_val_loss = get_vals(history_ft, 'val_loss')\n",
    "\n",
    "train_acc = head_acc + ft_acc\n",
    "val_acc   = head_val_acc + ft_val_acc\n",
    "train_loss = head_loss + ft_loss\n",
    "val_loss = head_val_loss + ft_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e5ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training\n",
    "# history_head, history_ft = train_two_stage(...)\n",
    "\n",
    "print (\"plot both stages combined (recommended so plots show full training curve):\")\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].plot(train_acc, label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(val_acc, label='Val Accuracy', linewidth=2)\n",
    "axes[0].set_title('Model Accuracy'); axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(train_loss, label='Train Loss', linewidth=2)\n",
    "axes[1].plot(val_loss, label='Val Loss', linewidth=2)\n",
    "axes[1].set_title('Model Loss'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'training_curve_head.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "#print(f\"ðŸ“Š Final Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9c3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Training History Visualization\n",
    "# =======================================================================\n",
    "\n",
    "# 1) Get scalar evaluation results from the model (safe)\n",
    "train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "val_loss, val_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"Training Loss:       {train_loss:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 2) Consolidate histories (head + fine-tune) for plotting if present\n",
    "def _get_hist_list(h):\n",
    "    return h.history if h is not None else {}\n",
    "\n",
    "head_hist = history_head if 'history_head' in globals() else None\n",
    "ft_hist = history_ft if 'history_ft' in globals() else None\n",
    "\n",
    "# Combine metrics safely\n",
    "def _concat_metric(metric):\n",
    "    vals = []\n",
    "    if head_hist is not None:\n",
    "        vals += head_hist.history.get(metric, [])\n",
    "    if ft_hist is not None:\n",
    "        vals += ft_hist.history.get(metric, [])\n",
    "    return vals\n",
    "\n",
    "train_acc_list = _concat_metric('accuracy')\n",
    "val_acc_list = _concat_metric('val_accuracy')\n",
    "train_loss_list = _concat_metric('loss')\n",
    "val_loss_list = _concat_metric('val_loss')\n",
    "\n",
    "def plot_training_history(history_data):\n",
    "    \"\"\"Plot training history with better formatting.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0].plot(history_data['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "    axes[0].plot(history_data['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "    axes[0].set_title('Model Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[1].plot(history_data['loss'], label='Train Loss', linewidth=2)\n",
    "    axes[1].plot(history_data['val_loss'], label='Val Loss', linewidth=2)\n",
    "    axes[1].set_title('Model Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_curve_full.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Prepare history data dictionary for plotting\n",
    "history_data = {\n",
    "    'accuracy': train_acc_list,\n",
    "    'val_accuracy': val_acc_list,\n",
    "    'loss': train_loss_list,\n",
    "    'val_loss': val_loss_list\n",
    "}\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š FINAL EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "val_loss, val_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "print(f\"Training Loss:       {train_loss:.4f}\")\n",
    "print(f\"Validation Loss:     {val_loss:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be851daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Performance Visualizations\n",
    "# =======================================================================\n",
    "# Add this before the visualization section\n",
    "# before visualization\n",
    "validation_set.reset()\n",
    "preds = model.predict(validation_set, verbose=0)\n",
    "predicted_classes = np.argmax(preds, axis=1)\n",
    "true_classes = validation_set.classes\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            linewidths=0.5,\n",
    "            linecolor='gray')\n",
    "plt.title('Confusion Matrix - Emotion Recognition', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Normalized Confusion Matrix\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.title('Normalized Confusion Matrix (%)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(full_path)+'confusion_matrix_normalized.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“ˆ DETAILED CLASSIFICATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(true_classes, predicted_classes, \n",
    "                           target_names=class_labels,\n",
    "                           digits=4))\n",
    "\n",
    "# 4. Per-Class Metrics Bar Chart\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    true_classes, predicted_classes, labels=range(7)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "x = np.arange(len(class_labels))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, precision, width, label='Precision', color='#3498db')\n",
    "bars2 = ax.bar(x, recall, width, label='Recall', color='#2ecc71')\n",
    "bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "ax.set_xlabel('Emotion', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Emotion Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_labels, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.set_ylim([0, 1.1])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'per_class_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 5. Summary Table\n",
    "summary_df = pd.DataFrame({\n",
    "    'Emotion': class_labels,\n",
    "    'Precision': [f'{p:.2%}' for p in precision],\n",
    "    'Recall': [f'{r:.2%}' for r in recall],\n",
    "    'F1-Score': [f'{f:.2%}' for f in f1],\n",
    "    'Support': support.astype(int)\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ“Š PERFORMANCE SUMMARY TABLE\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 6. Misclassification Analysis\n",
    "misclassified = cm.copy()\n",
    "np.fill_diagonal(misclassified, 0)\n",
    "\n",
    "top_confusions = []\n",
    "for i in range(7):\n",
    "    for j in range(7):\n",
    "        if i != j:\n",
    "            top_confusions.append((class_labels[i], class_labels[j], misclassified[i, j]))\n",
    "\n",
    "top_confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"âš ï¸  TOP 5 MISCLASSIFICATION PAIRS\")\n",
    "print(\"=\"*70)\n",
    "for true_label, pred_label, count in top_confusions[:5]:\n",
    "    print(f\"{true_label:>10} â†’ {pred_label:<10} : {count:>4} times\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af891569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Sample Predictions Visualization\n",
    "# =======================================================================\n",
    "import random\n",
    "\n",
    "# Get a random batch from validation set\n",
    "random_batch_idx = random.randint(0, len(validation_set) - 1)\n",
    "sample_images, sample_labels = validation_set[random_batch_idx]\n",
    "\n",
    "# Predict\n",
    "sample_preds = model.predict(sample_images, verbose=0)\n",
    "sample_pred_classes = np.argmax(sample_preds, axis=1)\n",
    "sample_true_classes = np.argmax(sample_labels, axis=1)\n",
    "\n",
    "# Randomly select 16 indices from the batch\n",
    "num_samples = min(16, len(sample_images))\n",
    "random_indices = random.sample(range(len(sample_images)), num_samples)\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    axes[i].imshow(sample_images[idx].squeeze(), cmap='gray')\n",
    "    \n",
    "    true_label = class_labels[sample_true_classes[idx]]\n",
    "    pred_label = class_labels[sample_pred_classes[idx]]\n",
    "    confidence = sample_preds[idx][sample_pred_classes[idx]] * 100\n",
    "\n",
    "    # Color: green if correct, red if wrong\n",
    "    color = 'green' if sample_true_classes[idx] == sample_pred_classes[idx] else 'red'\n",
    "    \n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                     color=color, fontweight='bold')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b900ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Global policy:\", mixed_precision.global_policy())\n",
    "print(\"Model dtype policy:\", model.dtype_policy)   # or model.input_dtype / model.output_dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
