{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Or force flush after prints\n",
    "print(\"Your message\", flush=True)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# At the top of your notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "import socket\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten, Dropout, Dense, Input, GlobalAveragePooling2D, Conv2D,\n",
    "    BatchNormalization, Activation, MaxPooling2D, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
    ")\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Enable mixed precision\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54233655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostname():\n",
    "    \n",
    "    print(\"Available devices:\")\n",
    "    print(tf.config.list_physical_devices())\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    return hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7e9239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all available devices\n",
    "\n",
    "# Check GPU specifically\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs found: {len(gpus)}\")\n",
    "for gpu in gpus:\n",
    "    print(f\"  {gpu}\")\n",
    "\n",
    "if get_hostname() == \"xscape7x\":\n",
    "    print(\"üîÑ Snapdragon X Elite detected - optimizing for CPU\")\n",
    "    \n",
    "    # Use environment variables instead (set before TF init)\n",
    "    os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "    os.environ['OMP_NUM_THREADS'] = '8'\n",
    "    os.environ['TF_NUM_INTEROP_THREADS'] = '8'\n",
    "    os.environ['TF_NUM_INTRAOP_THREADS'] = '8'\n",
    "    \n",
    "    # Restart Python kernel after setting these\n",
    "    print(\"‚úÖ Environment configured - restart kernel for full effect\")\n",
    "else:\n",
    "    # Standard GPU check\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        mixed_precision.set_global_policy('mixed_float16')\n",
    "    else:\n",
    "        mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# === NOW SAFE TO USE TENSORFLOW ===\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n",
    "print(\"GPUs found:\", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Final policy:\", mixed_precision.global_policy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92809734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =========================================================================\n",
    "# Clean Configuration System\n",
    "# =========================================================================\n",
    "\n",
    "class ModelConfig:\n",
    "    \"\"\"Clean configuration class with backbone-specific presets\"\"\"\n",
    "    \n",
    "    BACKBONE_PRESETS = {\n",
    "        'custom_cnn': {\n",
    "            'picture_size': 48,\n",
    "            'color_mode': 'grayscale',\n",
    "            'batch_size': 64,\n",
    "            'epochs': 50,\n",
    "            'learning_rate': 0.001,\n",
    "            'dropout_rate': 0.25,\n",
    "            'dense_units': [512],\n",
    "            'aug_level': 'light',\n",
    "            'use_class_weights': True,\n",
    "            'use_lr_schedule': True,\n",
    "            'weight_decay':1e-4\n",
    "        },\n",
    "        'mobilenet': {\n",
    "            'picture_size': 96,\n",
    "            'color_mode': 'rgb',\n",
    "            'batch_size': 32,\n",
    "            'epochs': 30,\n",
    "            'learning_rate': 0.0001,\n",
    "            'dropout_rate': 0.3,\n",
    "            'dense_units': [512, 256],\n",
    "            'aug_level': 'strong',\n",
    "            'use_class_weights': True,\n",
    "            'use_lr_schedule': True,\n",
    "            'fine_tune': True,\n",
    "            'fine_tune_epochs': 20,\n",
    "            'fine_tune_unfreeze_layers': 30,\n",
    "            'weight_decay':1e-4\n",
    "        },\n",
    "        'efficientnet': {\n",
    "            'picture_size': 128,\n",
    "            'color_mode': 'rgb',\n",
    "            'batch_size': 16,\n",
    "            'epochs': 40,\n",
    "            'learning_rate': 0.0001,\n",
    "            'dropout_rate': 0.4,\n",
    "            'dense_units': [1024, 512],\n",
    "            'aug_level': 'strong',\n",
    "            'use_class_weights': True,\n",
    "            'use_lr_schedule': True,\n",
    "            'fine_tune': True,\n",
    "            'fine_tune_epochs': 25,\n",
    "            'fine_tune_unfreeze_layers': 50,\n",
    "            'weight_decay':1e-4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    AUGMENTATION_LEVELS = {\n",
    "        'none': dict(rescale=1./255),\n",
    "        'light': dict(\n",
    "            rescale=1./255,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True\n",
    "        ),\n",
    "        'strong': dict(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.15,\n",
    "            zoom_range=0.2,\n",
    "            brightness_range=[0.7, 1.3],\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    def __init__(self, backbone='mobilenet'):\n",
    "        \"\"\"Initialize with backbone preset\"\"\"\n",
    "        if backbone not in self.BACKBONE_PRESETS:\n",
    "            raise ValueError(f\"Backbone must be one of {list(self.BACKBONE_PRESETS.keys())}\")\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.config = self.BACKBONE_PRESETS[backbone].copy()\n",
    "        self.config['backbone'] = backbone\n",
    "        self.config['no_of_classes'] = 7\n",
    "        \n",
    "    def get(self, key, default=None):\n",
    "        \"\"\"Get configuration value\"\"\"\n",
    "        return self.config.get(key, default)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        \"\"\"Set configuration value\"\"\"\n",
    "        self.config[key] = value\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Return full configuration\"\"\"\n",
    "        return self.config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0f3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================================\n",
    "# Data Loading Utilities\n",
    "# =========================================================================\n",
    "def is_on_kaggle():\n",
    "    \"\"\"Detect if running on Kaggle.\"\"\"\n",
    "    return os.path.exists('/kaggle/input')\n",
    "\n",
    "def get_data_path():\n",
    "    \"\"\"Detect environment and return appropriate data path.\"\"\"\n",
    "    if is_on_kaggle():\n",
    "        print(\"üåê Running on Kaggle\")\n",
    "        import kagglehub\n",
    "        image_path = kagglehub.dataset_download(\"jonathanoheix/face-expression-recognition-dataset\")\n",
    "        folder_path = os.path.join(image_path, \"images\")\n",
    "    else:\n",
    "        print(\"üíª Running on local machine\")\n",
    "        folder_path = \"data/images/\"\n",
    "    \n",
    "    return folder_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_data_generators(config, data_path):\n",
    "    \"\"\"Create clean data generators\"\"\"\n",
    "    \n",
    "    # Get augmentation parameters\n",
    "    aug_params = config.AUGMENTATION_LEVELS[config.get('aug_level', 'light')]\n",
    "    \n",
    "    # Handle preprocessing based on backbone\n",
    "    if config.backbone in ['mobilenet', 'efficientnet']:\n",
    "        from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "        aug_params = aug_params.copy()\n",
    "        aug_params['preprocessing_function'] = preprocess_input\n",
    "    \n",
    "    # Create generators\n",
    "    train_gen = ImageDataGenerator(**aug_params)\n",
    "    val_gen = ImageDataGenerator(**aug_params)\n",
    "    \n",
    "    # Determine color mode\n",
    "    color_mode = config.get('color_mode', 'grayscale')\n",
    "    \n",
    "    train_set = train_gen.flow_from_directory(\n",
    "        os.path.join(data_path, \"train\"),\n",
    "        target_size=(config.get('picture_size'), config.get('picture_size')),\n",
    "        color_mode=color_mode,\n",
    "        batch_size=config.get('batch_size'),\n",
    "        class_mode='categorical',\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    validation_set = val_gen.flow_from_directory(\n",
    "        os.path.join(data_path, \"validation\"),\n",
    "        target_size=(config.get('picture_size'), config.get('picture_size')),\n",
    "        color_mode=color_mode,\n",
    "        batch_size=config.get('batch_size'),\n",
    "        class_mode='categorical',\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    print(\"Train image shape:\", train_set.image_shape)\n",
    "    print(\"Val image shape:  \", validation_set.image_shape)\n",
    "\n",
    "    print(f\"\\n‚úÖ Data loaded successfully\")\n",
    "    print (f\"Colour Mode: {color_mode}\")\n",
    "    print(f\"Training samples: {train_set.n}\")\n",
    "    print(f\"Validation samples: {validation_set.n}\")\n",
    "    print(f\"Class indices: {train_set.class_indices}\")\n",
    "    \n",
    "    return train_set, validation_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee255750",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================================\n",
    "# Model Building\n",
    "# =========================================================================\n",
    "\n",
    "def build_custom_cnn(config):\n",
    "    \"\"\"Build custom CNN model\"\"\"\n",
    "    picture_size = config.get('picture_size')\n",
    "    color_mode = config.get('color_mode')\n",
    "    channels = 1 if color_mode == 'grayscale' else 3\n",
    "    input_shape=(picture_size,picture_size,channels)\n",
    "    dropout_rate = config.get('dropout_rate')\n",
    "    \n",
    "    model = Sequential()\n",
    "   # Block 1 - Double Conv\n",
    "    model.add(Conv2D(filters=32, \n",
    "                        kernel_size=(3, 3), \n",
    "                        padding='same', \n",
    "                        input_shape=input_shape,\n",
    "                        name='conv2d_1'))\n",
    "\n",
    "    # Block 2 - Double Conv\n",
    "    model.add(Conv2D(filters=128, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_2'))\n",
    "    model.add(BatchNormalization(name='bn_2'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_2'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_2'))\n",
    "    \n",
    "    # Block 3 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(5, 5), \n",
    "                      padding='same',\n",
    "                      name='conv2d_3'))\n",
    "    model.add(BatchNormalization(name='bn_3'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_3'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_3'))\n",
    "    \n",
    "    # Block 4 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_4'))\n",
    "    model.add(BatchNormalization(name='bn_4'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_4'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_4'))\n",
    "\n",
    "    \n",
    "    # Block 5 - Double Conv\n",
    "    model.add(Conv2D(filters=512, \n",
    "                      kernel_size=(3, 3), \n",
    "                      padding='same',\n",
    "                      name='conv2d_5'))\n",
    "    model.add(BatchNormalization(name='bn_5'))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),name='maxpool2d_5'))\n",
    "    model.add(Dropout(dropout_rate,name='dropout_5'))\n",
    " \n",
    "        \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in config.get('dense_units', [512]):\n",
    "        model.add(Dense(units, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(config.get('dropout_rate')))\n",
    "    \n",
    "    model.add(Dense(config.get('no_of_classes'), activation='softmax', dtype='float32'))\n",
    "    \n",
    "    return model, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2ae43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_transfer_model(config, backbone_class):\n",
    "    \"\"\"Build transfer learning model\"\"\"\n",
    "    picture_size = config.get('picture_size')\n",
    "    color_mode = config.get('color_mode')\n",
    "    channels = 1 if color_mode == 'grayscale' else 3\n",
    "    \n",
    "    inputs = Input(shape=(picture_size, picture_size, channels))\n",
    "    \n",
    "    # Convert grayscale to RGB if needed\n",
    "    if channels == 1:\n",
    "        x = Lambda(lambda z: tf.image.grayscale_to_rgb(z))(inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "    \n",
    "    # Load pre-trained backbone\n",
    "    base_model = backbone_class(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(picture_size, picture_size, 3)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = False\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(config.get('dropout_rate'))(x)\n",
    "    \n",
    "    # Add dense layers\n",
    "    for units in config.get('dense_units', [512]):\n",
    "        x = Dense(units, activation='relu')(x)\n",
    "        x = Dropout(config.get('dropout_rate'))(x)\n",
    "    \n",
    "    outputs = Dense(config.get('no_of_classes'), activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model, base_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd3fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(config):\n",
    "    \"\"\"Build model based on configuration\"\"\"\n",
    "    backbone = config.backbone\n",
    "    \n",
    "    if backbone == 'custom_cnn':\n",
    "        return build_custom_cnn(config)\n",
    "    elif backbone == 'mobilenet':\n",
    "        return build_transfer_model(config, MobileNetV2)\n",
    "    elif backbone == 'efficientnet':\n",
    "        return build_transfer_model(config, EfficientNetB0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ff386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================================================\n",
    "# Training Utilities\n",
    "# =========================================================================\n",
    "\n",
    "def create_callbacks(config):\n",
    "    \"\"\"Create training callbacks\"\"\"\n",
    "    callbacks = [\n",
    "        ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True),\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_delta=0.0001)\n",
    "    ]\n",
    "    \n",
    "    if config.get('use_lr_schedule'):\n",
    "        def cosine_annealing(epoch, lr):\n",
    "            initial_lr = float(config.get('learning_rate'))\n",
    "            min_lr = 1e-7\n",
    "            warmup = 5\n",
    "            total_epochs = config.get('epochs')\n",
    "            \n",
    "            if epoch < warmup:\n",
    "                return initial_lr * (epoch + 1) / warmup\n",
    "            \n",
    "            progress = (epoch - warmup) / (total_epochs - warmup)\n",
    "            lr_out = min_lr + (initial_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "            return max(lr_out, min_lr)\n",
    "        \n",
    "        callbacks.append(LearningRateScheduler(cosine_annealing))\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "def compute_class_weights(train_set):\n",
    "    \"\"\"Compute class weights for imbalanced data\"\"\"\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=np.unique(train_set.classes),\n",
    "        y=train_set.classes\n",
    "    )\n",
    "    return dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e6981b",
   "metadata": {},
   "source": [
    "<h1> Run Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_host_config_overides(config, dev=True):\n",
    "    \n",
    "    hostname = get_hostname() \n",
    "\n",
    "    if  hostname == \"xscape7x\": # local machine\n",
    "        print(\"Hostname:\", hostname)\n",
    "        print(\"Overiding settings for xscape7x...\")\n",
    "        # Override any settings\n",
    "        if dev==True: \n",
    "            config.set(\"epochs\", 1)\n",
    "            config.set(\"batch_size\", 32)\n",
    "            config.set(\"fine_tune_epochs\", 1)\n",
    "            config.set(\"fine_tune_unfreeze_layers\", 1)\n",
    "        \n",
    "    else:                                           # run on remote server\n",
    "        pass  # No overrides\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Configuration: {config.backbone.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "    for key, value in config.config.items():\n",
    "        print(f\"{key:20}: {value}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ef876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_dir():\n",
    "    # - create output folder and provide output path\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        output_dir = \"/kaggle/working/\" + timestamp + \"/\"  \n",
    "    else:\n",
    "        output_dir = \"outputs/\" + timestamp + \"/\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print (f\"Output directory: {output_dir}\")\n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef5cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name_prefix):\n",
    "    model_name = f\"{model_name_prefix}_{timestamp}.keras\"\n",
    "    out_path = Path(output_dir) / model_name\n",
    "    try:\n",
    "        model.save(str(out_path))\n",
    "        print(f\"\\nüíæ Model saved: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35159155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Train the Model Stage 1\n",
    "# =========================================================================\n",
    "# Class weights\n",
    "def train_stage1(config, model,train_set, validation_set, class_weights, callbacks):\n",
    "\n",
    "    # Training\n",
    "    print(f\"Training {config.backbone} model...\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üöÄ STARTING TRAINING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Target: {config.get('epochs')} epochs with early stopping\")\n",
    "    print(f\"Batch size: {config.get('batch_size')}\")\n",
    "    print(f\"Learning rate: {config.get('learning_rate')}\")\n",
    "    print(f\"Augmentation: {config.get('aug_level')}\")\n",
    "    print(f\"Backbone: {config.get('backbone')}\")\n",
    "    print(f\"Class weights: {'Enabled' if class_weights else 'Disabled'}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    \n",
    "    # Stage 1: Train head\n",
    "    history_head = model.fit(\n",
    "        train_set,\n",
    "        epochs=config.get(\"epochs\"),\n",
    "        validation_data=validation_set,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2c15ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Train the Model Stage 2 - Fine Tuning\n",
    "# =========================================================================\n",
    "\n",
    "def train_stage2(config, base_model, model, train_set, validation_set, class_weights, callbacks):\n",
    "    # Stage 2: Fine-tune (if transfer learning)\n",
    "    if base_model and config.get(\"fine_tune\"):\n",
    "        print(\"Fine-tuning...\")\n",
    "\n",
    "        # Unfreeze layers\n",
    "        unfreeze_layers = config.get(\"fine_tune_unfreeze_layers\", 30)\n",
    "        for layer in base_model.layers[unfreeze_layers:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "        # Recompile with lower LR\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=config.get(\"learning_rate\") * 0.1),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        # Continue training\n",
    "        history_ft = model.fit(\n",
    "            train_set,\n",
    "            epochs=config.get(\"epochs\") + config.get(\"fine_tune_epochs\", 0),\n",
    "            initial_epoch=config.get(\"epochs\"),\n",
    "            validation_data=validation_set,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weights,\n",
    "            verbose=1,\n",
    "        )\n",
    "    return history_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c896cd",
   "metadata": {},
   "source": [
    "<h1>Load Model and Generate Reports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b398f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_path():\n",
    "    import os,re, glob\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime\n",
    "    \n",
    "     # ensure these are always defined\n",
    "    latest_ft = None\n",
    "    latest_hd = None\n",
    "    latest_dir = None\n",
    "\n",
    "    # Find all valid .keras files\n",
    "    valid_files = [\n",
    "        f for f in glob.glob(\"outputs/*/*.keras\")\n",
    "        if re.match(r'^\\d{8}_\\d{6}$', os.path.basename(os.path.dirname(f)))\n",
    "    ]\n",
    "\n",
    "    if not valid_files:\n",
    "        print(\"‚ùå No valid .keras files found\")\n",
    "        \n",
    "    else:\n",
    "        # Find latest timestamp\n",
    "        latest = max(valid_files, key=lambda x: datetime.strptime(\n",
    "            os.path.basename(os.path.dirname(x)), \"%Y%m%d_%H%M%S\"\n",
    "        ))\n",
    "        latest_dir = os.path.dirname(latest)\n",
    "\n",
    "        # Look for hd and ft variants in the same latest directory\n",
    "        hd_pattern = os.path.join(latest_dir, \"*_hd_*.keras\")\n",
    "        ft_pattern = os.path.join(latest_dir, \"*_ft_*.keras\")\n",
    "\n",
    "        hd_files = glob.glob(hd_pattern)\n",
    "        ft_files = glob.glob(ft_pattern)\n",
    "\n",
    "        latest_hd = hd_files[0] if hd_files else None\n",
    "        latest_ft = ft_files[0] if ft_files else None\n",
    "\n",
    "        print(f\"Latest HD: {latest_hd}\")\n",
    "        print(f\"Latest FT: {latest_ft}\")\n",
    "    \n",
    "    return latest_ft, latest_hd, latest_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086c0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models_from_file(path_hd, path_ft):\n",
    "    from tensorflow.keras.models import load_model\n",
    "    model_hd=load_model(path_hd)\n",
    "    model_ft=load_model(path_ft)\n",
    "\n",
    "    return model_hd, model_ft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee0ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training\n",
    "# history_head, history_ft = train_two_stage(...)\n",
    "def plot_training_curve_head(train_set, val_acc, train_loss, val_loss):\n",
    "    print (\"plot both stages combined (recommended so plots show full training curve):\")\n",
    "    # 1) Get scalar evaluation results from the model (safe)\n",
    "\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    axes[0].plot(train_set, label='Train Accuracy', linewidth=2)\n",
    "    axes[0].plot(val_acc, label='Val Accuracy', linewidth=2)\n",
    "    axes[0].set_title('Model Accuracy'); axes[0].legend(); axes[0].grid(True)\n",
    "\n",
    "    axes[1].plot(train_loss, label='Train Loss', linewidth=2)\n",
    "    axes[1].plot(val_loss, label='Val Loss', linewidth=2)\n",
    "    axes[1].set_title('Model Loss'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "   \n",
    "    #print(f\"üìä Final Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    return plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Training History Visualization\n",
    "# =======================================================================\n",
    "def plot_training_history(history_hd, history_ft):\n",
    "    # 1) Get scalar evaluation results from the model (safe)\n",
    "    # train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "    # val_loss, val_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "    # 2) Consolidate histories (head + fine-tune) for plotting if present\n",
    "    def _get_hist_list(h):\n",
    "        return h.history if h is not None else {}\n",
    "\n",
    "    head_hist = history_hd if 'history_head' in globals() else None\n",
    "    ft_hist = history_ft if 'history_ft' in globals() else None\n",
    "\n",
    "    # Combine metrics safely\n",
    "    def _concat_metric(metric):\n",
    "        vals = []\n",
    "        if head_hist is not None:\n",
    "            vals += head_hist.history.get(metric, [])\n",
    "        if ft_hist is not None:\n",
    "            vals += ft_hist.history.get(metric, [])\n",
    "        return vals\n",
    "\n",
    "    train_acc_list = _concat_metric('accuracy')\n",
    "    val_acc_list = _concat_metric('val_accuracy')\n",
    "    train_loss_list = _concat_metric('loss')\n",
    "    val_loss_list = _concat_metric('val_loss')\n",
    "\n",
    "    def plot_training_history(history_data):\n",
    "        \"\"\"Plot training history with better formatting.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy plot\n",
    "        axes[0].plot(history_data['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(history_data['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_title('Model Accuracy')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss plot\n",
    "        axes[1].plot(history_data['loss'], label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(history_data['val_loss'], label='Val Loss', linewidth=2)\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "\n",
    "\n",
    "\n",
    "    # Prepare history data dictionary for plotting\n",
    "    history_data = {\n",
    "        'accuracy': train_acc_list,\n",
    "        'val_accuracy': val_acc_list,\n",
    "        'loss': train_loss_list,\n",
    "        'val_loss': val_loss_list\n",
    "    }\n",
    "\n",
    "    # Plot the training history\n",
    "    plot_training_history(history_data)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12a7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_final_evaluation(train_acc, val_acc, train_loss, val_loss):\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä FINAL EVALUATION\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Training Accuracy:   {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
    "    print(f\"Training Loss:       {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss:     {val_loss:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80015392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Enhanced Performance Visualizations - confusion matrix\n",
    "# =======================================================================\n",
    "# Add this before the visualization section\n",
    "# before visualization\n",
    "\n",
    "def generate_confusion_matrix_reports(validation_set, class_labels, out_path):\n",
    "    #validation_set.reset()\n",
    "   \n",
    "    predicted_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = validation_set.classes\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "    # 1. Confusion Matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='YlOrRd',\n",
    "                xticklabels=class_labels,\n",
    "                yticklabels=class_labels,\n",
    "                cbar_kws={'label': 'Count'},\n",
    "                linewidths=0.5,\n",
    "                linecolor='gray')\n",
    "    plt.title('Confusion Matrix - Emotion Recognition', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "    plt.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_path, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Normalized Confusion Matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt_norm.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=class_labels,\n",
    "                yticklabels=class_labels,\n",
    "                cbar_kws={'label': 'Percentage'})\n",
    "    plt_norm.title('Normalized Confusion Matrix (%)', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt_norm.xlabel('Predicted Emotion', fontsize=13, fontweight='bold')\n",
    "    plt_norm.ylabel('True Emotion', fontsize=13, fontweight='bold')\n",
    "    plt_norm.xticks(rotation=45, ha='right')\n",
    "    plt_norm.yticks(rotation=0)\n",
    "    plt_norm.tight_layout()\n",
    "    plt_norm.savefig(os.path.join(out_path, 'confusion_matrix_normalized.png'), dpi=300, bbox_inches='tight')\n",
    "    plt_norm.show()\n",
    "\n",
    "    # 3. Classification Report\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìà DETAILED CLASSIFICATION METRICS\")\n",
    "    print(\"=\"*70)\n",
    "    print(classification_report(true_classes, predicted_classes, \n",
    "                            target_names=class_labels,\n",
    "                            digits=4))\n",
    "    return plt, plt_norm, cm, cm_normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_per_class_report(model, validation_set, class_labels, output_dir):\n",
    "    # 4. Per-Class Metrics Bar Chart\n",
    "    \n",
    "    #validation_set.reset()\n",
    "    preds = model.predict(validation_set, verbose=0)\n",
    "    predicted_classes = np.argmax(preds, axis=1)\n",
    "    true_classes = validation_set.classes\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        true_classes, predicted_classes, labels=range(7)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 5))\n",
    "    x = np.arange(len(class_labels))\n",
    "    width = 0.25\n",
    "\n",
    "    bars1 = ax.bar(x - width, precision, width, label='Precision', color='#3498db')\n",
    "    bars2 = ax.bar(x, recall, width, label='Recall', color='#2ecc71')\n",
    "    bars3 = ax.bar(x + width, f1, width, label='F1-Score', color='#e74c3c')\n",
    "\n",
    "    ax.set_xlabel('Emotion', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Per-Emotion Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(class_labels, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.2f}',\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "    \n",
    "def performace_summary(class_labels, precision, recall, f1, support, cm):\n",
    "    # 5. Summary Table\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Emotion': class_labels,\n",
    "        'Precision': [f'{p:.2%}' for p in precision],\n",
    "        'Recall': [f'{r:.2%}' for r in recall],\n",
    "        'F1-Score': [f'{f:.2%}' for f in f1],\n",
    "        'Support': support.astype(int)\n",
    "    })\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä PERFORMANCE SUMMARY TABLE\")\n",
    "    print(\"=\"*70)\n",
    "    print(summary_df.to_string(index=False))\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 6. Misclassification Analysis\n",
    "    misclassified = cm.copy()\n",
    "    np.fill_diagonal(misclassified, 0)\n",
    "\n",
    "    top_confusions = []\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "            if i != j:\n",
    "                top_confusions.append((class_labels[i], class_labels[j], misclassified[i, j]))\n",
    "\n",
    "    top_confusions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚ö†Ô∏è  TOP 5 MISCLASSIFICATION PAIRS\")\n",
    "    print(\"=\"*70)\n",
    "    for true_label, pred_label, count in top_confusions[:5]:\n",
    "        print(f\"{true_label:>10} ‚Üí {pred_label:<10} : {count:>4} times\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef6bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Sample Predictions Visualization\n",
    "# =======================================================================\n",
    "\n",
    "def test_random_predictions(model, validation_set, class_labels):\n",
    "\n",
    "    import random\n",
    "\n",
    "    # Get a random batch from validation set\n",
    "    random_batch_idx = random.randint(0, len(validation_set) - 1)\n",
    "    sample_images, sample_labels = validation_set[random_batch_idx]\n",
    "\n",
    "    # Predict\n",
    "    sample_preds = model.predict(sample_images, verbose=0)\n",
    "    sample_pred_classes = np.argmax(sample_preds, axis=1)\n",
    "    sample_true_classes = np.argmax(sample_labels, axis=1)\n",
    "\n",
    "    # Randomly select 16 indices from the batch\n",
    "    num_samples = min(16, len(sample_images))\n",
    "    random_indices = random.sample(range(len(sample_images)), num_samples)\n",
    "\n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        axes[i].imshow(sample_images[idx].squeeze(), cmap='gray')\n",
    "        \n",
    "        true_label = class_labels[sample_true_classes[idx]]\n",
    "        pred_label = class_labels[sample_pred_classes[idx]]\n",
    "        confidence = sample_preds[idx][sample_pred_classes[idx]] * 100\n",
    "\n",
    "        # Color: green if correct, red if wrong\n",
    "        color = 'green' if sample_true_classes[idx] == sample_pred_classes[idx] else 'red'\n",
    "        \n",
    "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)', \n",
    "                        color=color, fontweight='bold')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle('Sample Predictions on Validation Set', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588437e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reports():\n",
    "\n",
    "    class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "    publish_final_evaluation (train_acc,val_acc,train_loss,val_loss)\n",
    "\n",
    "    #plot_training_curve_head = plot_training_curve_head(train_set,val_acc, train_loss, val_loss)\n",
    "    #plot_training_curve_head.savefig(os.path.join(output_dir, 'training_curve_head.png'), dpi=300, bbox_inches='tight')\n",
    "    #plot_training_curve_head.show()\n",
    "\n",
    "    training_history_plt=plot_training_history(history_head, history_ft)\n",
    "    training_history_plt.savefig(os.path.join(output_dir, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    training_history_plt.show\n",
    "\n",
    "    per_class_plt=generate_per_class_report(model,history_ft, validation_set,class_labels,output_dir)\n",
    "    per_class_plt.savefig(os.path.join(output_dir, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "    per_class_plt.show\n",
    "\n",
    "    cm_plt,cm_norm_plt,cm, cm_norm=generate_confusion_matrix_reports(history_ft, validation_set,class_labels)\n",
    "    cm_plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    cm_norm_plt.savefig(os.path.join(output_dir, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "    cm_plt.show\n",
    "    cm_norm_plt.show\n",
    "\n",
    "    random_predictions_plt=test_random_predictions(validation_set,class_labels)\n",
    "    random_predictions_plt.savefig(os.path.join(output_dir, 'random_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "    random_predictions_plt.show\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192618b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(plot, output_dir):\n",
    "    if getattr(plot, \"_suptitle\", None) is not None:\n",
    "        plot_name = plot._suptitle.get_text()\n",
    "    else:\n",
    "        axes=plot.get_axes()\n",
    "        if axes:\n",
    "            plot_name = axes[0].get_title()\n",
    "    \n",
    "    \n",
    "    out_path = Path(output_dir) / plot_name\n",
    "    try:\n",
    "        plt.savefig(os.path.join(out_path,f\"{plot_name}.png\"),dpi=300,bbox_inches='tight')\n",
    "        print(f\"\\nüíæ Plot saved: {out_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving plot: {e}\")\n",
    "    \n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5916b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Enhanced Model Evaluation and Reporting\n",
    "# =========================================================================\n",
    "\n",
    "class ModelEvaluator:\n",
    "    \"\"\"Comprehensive model evaluation and report generation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_set, class_labels, output_dir=None, history_hd=None, history_ft=None):\n",
    "        self.model = model\n",
    "        self.test_set = test_set\n",
    "        self.class_labels = class_labels\n",
    "        self.output_dir = output_dir\n",
    "        self.history_hd = history_hd  \n",
    "        self.history_ft = history_ft\n",
    "        \n",
    "        # Initialize prediction attributes\n",
    "        self.predictions = None\n",
    "        self.true_labels = None\n",
    "        self.pred_labels = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def generate_predictions(self):\n",
    "        \"\"\"Generate predictions on test set\"\"\"\n",
    "        print(\"üîÑ Generating predictions...\")\n",
    "        self.test_set.reset()\n",
    "        \n",
    "        self.predictions = self.model.predict(self.test_set, verbose=1)\n",
    "        self.true_labels = self.test_set.classes\n",
    "        self.pred_labels = np.argmax(self.predictions, axis=1)\n",
    "        \n",
    "        return self.predictions, self.true_labels\n",
    "    \n",
    "    def create_comprehensive_report(self):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        if self.predictions is None:\n",
    "            self.generate_predictions()\n",
    "        \n",
    "          \n",
    "        # Generate all reports\n",
    "        self.plot_training_history()\n",
    "        self.plot_confusion_matrix()\n",
    "        self.plot_normalized_confusion_matrix()\n",
    "        self.generate_classification_report()\n",
    "        self.plot_per_class_metrics()\n",
    "        self.plot_roc_curves()\n",
    "        self.plot_precision_recall_curves()\n",
    "        self.generate_error_analysis()\n",
    "        self.create_summary_report()\n",
    "    \n",
    "    \"\"\"\n",
    "    Traning history chart will only get returned if training history in memeory\n",
    "    \"\"\"    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training history with head and fine-tune stages\"\"\"\n",
    "        if self.history_hd is None and self.history_ft is None:\n",
    "            print(\"‚ö†Ô∏è No training history provided\")\n",
    "            return None\n",
    "            \n",
    "        # Combine histories\n",
    "        def _concat_metric(metric):\n",
    "            vals = []\n",
    "            if self.history_hd is not None:\n",
    "                vals += self.history_hd.history.get(metric, [])\n",
    "            if self.history_ft is not None:\n",
    "                vals += self.history_ft.history.get(metric, [])\n",
    "            return vals\n",
    "\n",
    "        train_acc = _concat_metric('accuracy')\n",
    "        val_acc = _concat_metric('val_accuracy')\n",
    "        train_loss = _concat_metric('loss')\n",
    "        val_loss = _concat_metric('val_loss')\n",
    "        \n",
    "        if not any([train_acc, val_acc, train_loss, val_loss]):\n",
    "            print(\"‚ö†Ô∏è No training metrics found in history\")\n",
    "            return None\n",
    "            \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Accuracy plot\n",
    "        if train_acc:\n",
    "            axes[0].plot(train_acc, label='Train Accuracy', linewidth=2, color='blue')\n",
    "        if val_acc:\n",
    "            axes[0].plot(val_acc, label='Validation Accuracy', linewidth=2, color='red')\n",
    "        axes[0].set_title('Model Accuracy Over Training')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].set_ylabel('Accuracy')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Loss plot\n",
    "        if train_loss:\n",
    "            axes[1].plot(train_loss, label='Train Loss', linewidth=2, color='blue')\n",
    "        if val_loss:\n",
    "            axes[1].plot(val_loss, label='Validation Loss', linewidth=2, color='red')\n",
    "        axes[1].set_title('Model Loss Over Training')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add vertical line to show fine-tuning start if applicable\n",
    "        if self.history_hd is not None and self.history_ft is not None:\n",
    "            hd_epochs = len(self.history_hd.history.get('loss', []))\n",
    "            axes[0].axvline(x=hd_epochs, color='green', linestyle='--', alpha=0.7, label='Fine-tune start')\n",
    "            axes[1].axvline(x=hd_epochs, color='green', linestyle='--', alpha=0.7, label='Fine-tune start')\n",
    "            axes[0].legend()\n",
    "            axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return plt\n",
    "    \n",
    "    def plot_confusion_matrix(self):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(self.true_labels, self.pred_labels)\n",
    "        \n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_labels,\n",
    "                   yticklabels=self.class_labels)\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "        \n",
    "    def plot_normalized_confusion_matrix(self):\n",
    "        \"\"\"Plot normalized confusion matrix\"\"\"\n",
    "        cm = confusion_matrix(self.true_labels, self.pred_labels)\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                   xticklabels=self.class_labels,\n",
    "                   yticklabels=self.class_labels)\n",
    "        plt.title('Confusion Matrix-Normalized')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "        \n",
    "    def generate_classification_report(self):\n",
    "        \"\"\"Generate detailed classification report\"\"\"\n",
    "        report = classification_report(\n",
    "            self.true_labels, \n",
    "            self.pred_labels,\n",
    "            target_names=self.class_labels,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Save as JSON\n",
    "        with open(os.path.join(self.output_dir, 'classification_report.json'), 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "            \n",
    "        # Save as CSV\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        report_df.to_csv(os.path.join(self.output_dir, 'classification_report.csv'))\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nüìä Classification Report:\")\n",
    "        print(classification_report(\n",
    "            self.true_labels, \n",
    "            self.pred_labels,\n",
    "            target_names=self.class_labels\n",
    "        ))\n",
    "        \n",
    "    def plot_per_class_metrics(self):\n",
    "        \"\"\"Plot precision, recall, and F1-score per class\"\"\"\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            self.true_labels, self.pred_labels, average=None\n",
    "        )\n",
    "        \n",
    "        x = np.arange(len(self.class_labels))\n",
    "        width = 0.25\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        bars1 = ax.bar(x - width, precision, width, label='Precision', alpha=0.8)\n",
    "        bars2 = ax.bar(x, recall, width, label='Recall', alpha=0.8)\n",
    "        bars3 = ax.bar(x + width, f1, width, label='F1-Score', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Emotion Classes')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_title('Per-Class Performance Metrics')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(self.class_labels, rotation=45)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bars in [bars1, bars2, bars3]:\n",
    "            for bar in bars:\n",
    "                height = bar.get_height()\n",
    "                ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.2f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt_name = 'per_class_metrics.png'\n",
    "        return plt\n",
    "        \n",
    "    def plot_roc_curves(self):\n",
    "        \"\"\"Plot ROC curves for multi-class classification\"\"\"\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        \n",
    "        # Binarize labels\n",
    "        y_test_bin = label_binarize(self.true_labels, classes=range(len(self.class_labels)))\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(len(self.class_labels)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], self.predictions[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(self.class_labels)))\n",
    "        \n",
    "        for i, color in zip(range(len(self.class_labels)), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                    label=f'{self.class_labels[i]} (AUC = {roc_auc[i]:0.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curves-Multi-class')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        return plt\n",
    "        \n",
    "    \n",
    "    def plot_precision_recall_curves(self):\n",
    "        \"\"\"Plot precision-recall curves for multi-class classification\"\"\"\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "        \n",
    "        # Binarize labels\n",
    "        y_test_bin = label_binarize(self.true_labels, classes=range(len(self.class_labels)))\n",
    "        \n",
    "        # Compute precision-recall curve and average precision for each class\n",
    "        precision = dict()\n",
    "        recall = dict()\n",
    "        average_precision = dict()\n",
    "        \n",
    "        for i in range(len(self.class_labels)):\n",
    "            precision[i], recall[i], _ = precision_recall_curve(\n",
    "                y_test_bin[:, i], self.predictions[:, i]\n",
    "            )\n",
    "            average_precision[i] = average_precision_score(\n",
    "                y_test_bin[:, i], self.predictions[:, i]\n",
    "            )\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(self.class_labels)))\n",
    "        \n",
    "        for i, color in zip(range(len(self.class_labels)), colors):\n",
    "            plt.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                    label=f'{self.class_labels[i]} (AP = {average_precision[i]:0.2f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves-Multi-class')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt_prc=plt\n",
    "        \n",
    "        \n",
    "        # Also create micro-average precision-recall curve\n",
    "        precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(\n",
    "            y_test_bin.ravel(), self.predictions.ravel()\n",
    "        )\n",
    "        average_precision[\"micro\"] = average_precision_score(\n",
    "            y_test_bin, self.predictions, average=\"micro\"\n",
    "        )\n",
    "        \n",
    "        # Plot micro-average\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2,\n",
    "                label=f'Micro-average (AP = {average_precision[\"micro\"]:0.2f})')\n",
    "        \n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve-Micro-average')\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt_prm=plt\n",
    "        \n",
    "        return plt_prm, plt_prc      \n",
    "      \n",
    "    def generate_error_analysis(self):\n",
    "        \"\"\"Analyze misclassifications\"\"\"\n",
    "        errors = self.true_labels != self.pred_labels\n",
    "        error_indices = np.where(errors)[0]\n",
    "        \n",
    "        if len(error_indices) > 0:\n",
    "            # Get error details\n",
    "            error_details = []\n",
    "            for idx in error_indices[:50]:  # Limit to first 50 errors\n",
    "                true_class = self.class_labels[self.true_labels[idx]]\n",
    "                pred_class = self.class_labels[self.pred_labels[idx]]\n",
    "                confidence = self.predictions[idx][self.pred_labels[idx]]\n",
    "                \n",
    "                error_details.append({\n",
    "                    'index': idx,\n",
    "                    'true_class': true_class,\n",
    "                    'pred_class': pred_class,\n",
    "                    'confidence': confidence,\n",
    "                    'image_path': self.test_set.filepaths[idx] if hasattr(self.test_set, 'filepaths') else None\n",
    "                })\n",
    "            \n",
    "            # Save error analysis\n",
    "            error_df = pd.DataFrame(error_details)\n",
    "            error_df.to_csv(os.path.join(self.output_dir, 'error_analysis.csv'), index=False)\n",
    "            \n",
    "            # Print top misclassifications\n",
    "            print(\"\\nüîç Top 5 Misclassifications:\")\n",
    "            print(\"=\"*50)\n",
    "            for _, row in error_df.head().iterrows():\n",
    "                print(f\"True: {row['true_class']} ‚Üí Pred: {row['pred_class']} \"\n",
    "                      f\"(Confidence: {row['confidence']:.2%})\")\n",
    "                        \n",
    "    def create_summary_report(self):\n",
    "        \"\"\"Create comprehensive summary report\"\"\"\n",
    "        summary = {\n",
    "            'model_name': self.model.name,\n",
    "            'test_samples': len(self.true_labels),\n",
    "            'overall_accuracy': np.mean(self.true_labels == self.pred_labels),\n",
    "            'class_labels': self.class_labels,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            self.true_labels, self.pred_labels, average=None\n",
    "        )\n",
    "        \n",
    "        summary['per_class_metrics'] = {\n",
    "            label: {\n",
    "                'precision': float(p),\n",
    "                'recall': float(r),\n",
    "                'f1_score': float(f),\n",
    "                'support': int(s)\n",
    "            }\n",
    "            for label, p, r, f, s in zip(self.class_labels, precision, recall, f1, support)\n",
    "        }\n",
    "        \n",
    "        # Save summary\n",
    "        with open(os.path.join(self.output_dir, 'summary_report.json'), 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "            \n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f60a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(config, train_set, validation_set, class_labels, data_path):\n",
    "    \n",
    "    \n",
    "    # Load model\n",
    "    model_path_ft, model_path_hd, output_dir = load_model_path()\n",
    "    model_ft, _ = load_models_from_file(model_path_hd, model_path_ft)\n",
    "    \n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluator = ModelEvaluator(model_ft, validation_set, class_labels, output_dir)\n",
    "    predictions, true_labels = evaluator.generate_predictions()\n",
    "    \n",
    "    plt_train_hist=evaluator.plot_training_history()\n",
    "    plt_cm= evaluator.plot_confusion_matrix()\n",
    "    plt_cm_norm = evaluator.plot_normalized_confusion_matrix()\n",
    "    plt_prm, plt_prc=evaluator.plot_precision_recall_curves()\n",
    "    plt_class= evaluator.plot_per_class_metrics()\n",
    "    plt_roc= evaluator.plot_roc_curves()\n",
    "    \n",
    "    plots=[plt_train_hist, plt_cm, plt_cm_norm, plt_prm, plt_prc, plt_class, plt_roc]\n",
    "    \n",
    "    \n",
    "    for fig in plots:\n",
    "        fig.show\n",
    "        #save_plot(fig, output_dir)\n",
    "        fig.close\n",
    "    \n",
    "    #evaluator.create_comprehensive_report()\n",
    "    \n",
    "\n",
    "    random_predictions_plt= test_random_predictions(model_ft,validation_set, class_labels)\n",
    "    random_predictions_plt.show\n",
    "    print(f\"‚úÖ Evaluation complete! Reports saved to: {output_dir}\")\n",
    "    \n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config, train_set, validation_set):\n",
    "\n",
    "    # =========================================================================\n",
    "    # Load Data & Compile Model  \n",
    "    # =========================================================================\n",
    "\n",
    "    # Generate training and validation Set\n",
    "    print(\"Train image shape:\", train_set.image_shape)\n",
    "    print(\"Val image shape:  \", validation_set.image_shape)\n",
    "\n",
    "    print(f\"\\n‚úÖ Data loaded successfully\")\n",
    "    print (f\"Colour Mode: {config.config.get('color_mode')}\")    \n",
    "    print(f\"Training samples: {train_set.n}\")\n",
    "    print(f\"Validation samples: {validation_set.n}\")\n",
    "    print(f\"Class indices: {train_set.class_indices}\")\n",
    "\n",
    "    # Build model\n",
    "    model, base_model = build_model(config)\n",
    "\n",
    "    # Validate shapes\n",
    "    assert train_set.image_shape == model.input_shape[1:], (\n",
    "        f\"Shape mismatch: {train_set.image_shape} vs {model.input_shape[1:]}\")\n",
    "\n",
    "    # Compile with optimizer choice\n",
    "    optimizer_name =  \"adam\"\n",
    "    if optimizer_name == \"adam\":\n",
    "        opt = Adam(learning_rate=config.get('learning_rate'))\n",
    "    elif optimizer_name == \"adamw\":\n",
    "        opt = tf.keras.optimizers.AdamW(\n",
    "            opt = Adam(learning_rate=config.get('learningrate')),\n",
    "            weight_decay=config.get(\"weight_decay\", 1e-4)\n",
    "        )\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=config.get('learningrate'),\n",
    "            momentum=0.9,\n",
    "            nesterov=True\n",
    "        )\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    #print (\"== Base Model ====\")\n",
    "    #base_model.summary()\n",
    "    print (\"== Full Model ====\")\n",
    "    model.summary()\n",
    "\n",
    "    if config.get(\"use_class_weights\"):\n",
    "        class_weights = compute_class_weights(train_set)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = create_callbacks(config)\n",
    "    print(f\"‚úÖ Callbacks configured: {len(callbacks)} callbacks\")\n",
    "    \n",
    "\n",
    "    # Training\n",
    "    print(f\"Training {config.backbone} model...\")\n",
    "    history_hd= train_stage1(config, model, train_set, validation_set, class_weights, callbacks)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    #print (\"== Base Model ====\")\n",
    "    #base_model.summary()\n",
    "    print (\"== Full Model ====\")\n",
    "    model.summary()\n",
    "    \n",
    "    # Stage 2: Fine-tune (if transfer learning)    \n",
    "    history_ft=train_stage2(config, base_model, model, train_set, validation_set, class_weights, callbacks)\n",
    "    \n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Building model with '{config.get('backbone')}' backbone...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    return model, history_hd, history_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11156a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run \n",
    "if __name__ == \"__main__\":\n",
    "    # setup\n",
    "    config = ModelConfig(\"mobilenet\")  # or 'custom_cnn', 'efficientnet'\n",
    "    set_host_config_overides (config=config,dev=True)\n",
    "    data_path=get_data_path()\n",
    "    train_set, validation_set=create_data_generators(config, data_path)\n",
    "    class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "    \n",
    "    model, history_hd, history_ft = train_model(config, train_set, validation_set)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999883e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_training_history(history_hd, history_ft)\n",
    "plt.show\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617d11ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# Extract Values from the Model\n",
    "# =========================================================================\n",
    "\n",
    "\n",
    "# Helper to safely get values\n",
    "def get_vals(h, key):\n",
    "    return h.history.get(key, []) if h is not None else []\n",
    "\n",
    "head_acc = get_vals(history_hd, 'val_accuracy')\n",
    "head_val_acc = get_vals(history_hd, 'val_accuracy')\n",
    "head_loss = get_vals(history_hd, 'loss')\n",
    "head_val_loss = get_vals(history_hd, 'val_loss')\n",
    "\n",
    "ft_acc = get_vals(history_ft, 'accuracy')\n",
    "ft_val_acc = get_vals(history_ft, 'val_accuracy')\n",
    "ft_loss = get_vals(history_ft, 'loss')\n",
    "ft_val_loss = get_vals(history_ft, 'val_loss')\n",
    "\n",
    "train_acc = head_acc + ft_acc\n",
    "val_acc   = head_val_acc + ft_val_acc\n",
    "train_loss = head_loss + ft_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88925bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper: try several metric key variants\n",
    "def pick_metric(h, names):\n",
    "    if h is None: return []\n",
    "    for n in names:\n",
    "        if n in h.history:\n",
    "            return h.history[n]\n",
    "    return []\n",
    "\n",
    "train_acc = pick_metric(history_hd, ['accuracy', 'acc'])\n",
    "val_acc   = pick_metric(history_hd, ['val_accuracy', 'val_acc'])\n",
    "# if fine-tune added, append those too\n",
    "train_acc += pick_metric(history_ft, ['accuracy', 'acc'])\n",
    "val_acc   += pick_metric(history_ft, ['val_accuracy', 'val_acc'])\n",
    "\n",
    "train_loss = pick_metric(history_hd, ['loss'])\n",
    "val_loss   = pick_metric(history_hd, ['val_loss'])\n",
    "train_loss += pick_metric(history_ft, ['loss'])\n",
    "val_loss   += pick_metric(history_ft, ['val_loss'])\n",
    "\n",
    "# warn if empty\n",
    "if not any([train_acc, val_acc, train_loss, val_loss]):\n",
    "    print(\"‚ö†Ô∏è No training metrics found. Check history objects and metric names. Available keys:\")\n",
    "    if history_hd: print(\"head keys:\", list(history_hd.history.keys()))\n",
    "    if history_ft: print(\"ft keys:  \", list(history_ft.history.keys()))\n",
    "    \n",
    "\n",
    "    # create x-axis explicitly\n",
    "    epochs = max(len(train_acc), len(val_acc), len(train_loss), len(val_loss))\n",
    "    x = range(1, epochs + 1)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    if train_acc or val_acc:\n",
    "        axes[0].plot(x[:len(train_acc)], train_acc, label='Train Accuracy', linewidth=2)\n",
    "        axes[0].plot(x[:len(val_acc)], val_acc, label='Val Accuracy', linewidth=2)\n",
    "        axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('Accuracy'); axes[0].legend(); axes[0].grid(True)\n",
    "    if train_loss or val_loss:\n",
    "        axes[1].plot(x[:len(train_loss)], train_loss, label='Train Loss', linewidth=2)\n",
    "        axes[1].plot(x[:len(val_loss)], val_loss, label='Val Loss', linewidth=2)\n",
    "        axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Loss'); axes[1].legend(); axes[1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()   # <-- call it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    #print(\"Global policy:\", mixed_precision.global_policy())\n",
    "    #print(\"Model dtype policy:\", model.dtype_policy)   # or model.input_dtype / model.output_dtype  \n",
    "    \n",
    "    #evaluator = run_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
