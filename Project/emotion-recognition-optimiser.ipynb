{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc5c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.1\n",
      "Python executable: c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# Facial Emotion Recognition Model - Optimized for Snapdragon X Elite\n",
    "# =======================================================================\n",
    "# Configuration: MOBILENET with optimized hyperparameters\n",
    "# Target: <1 hour training time with maximum accuracy\n",
    "\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# Import TensorFlow and configure for optimal performance\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Import deep learning libraries\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Flatten, Dropout, Dense, Input, \n",
    "                                     GlobalAveragePooling2D, Conv2D, \n",
    "                                     BatchNormalization, Activation, MaxPooling2D)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d2d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data folder path: data/images/\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect folder path\n",
    "if sys.executable.startswith('/anaconda/envs/azureml_py38_PT_TF/bin/python'):\n",
    "    folder_path = \"Users/martyn.frank/IATD_Deeplearning/Project/data/images/\"\n",
    "else:\n",
    "    folder_path = 'data/images/'\n",
    "    \n",
    "print(f\"Data folder path: {folder_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72f69506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mixed precision training enabled - expect 2-3x speedup!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Configuration Parameters\n",
    "# =======================================================================\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "def load_cfg(path=\"config.yaml\"):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    return data\n",
    "\n",
    "cfg = load_cfg(\"config.yaml\")\n",
    "\n",
    "# optionally, map top-level training params from cfg\n",
    "picture_size   = cfg.get(\"picture_size\", 48)\n",
    "batch_size     = cfg.get(\"batch_size\", 128)\n",
    "epochs         = cfg.get(\"epochs\", 30)\n",
    "no_of_classes  = cfg.get(\"no_of_classes\", 7)\n",
    "learning_rate  = cfg.get(\"learning_rate\", 1e-4)\n",
    "\n",
    "\n",
    "# Enable mixed precision for faster training on Snapdragon X Elite\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Precision Switch\n",
    "if cfg.get(\"precision\", \"float32\") == \"mixed\":\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"Mixed precision training enabled - expect 2-3x speedup!\")\n",
    "else:\n",
    "    mixed_precision.set_global_policy('float32')\n",
    "    print(\"Using float32 precision\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95fe93d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Image Validation and Cleaning\n",
    "# =======================================================================\n",
    "\n",
    "def is_image_valid(filepath):\n",
    "    \"\"\"Validate image integrity.\"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        return False\n",
    "\n",
    "def delete_if_corrupt(filepath):\n",
    "    \"\"\"Delete corrupted images.\"\"\"\n",
    "    try:\n",
    "        with Image.open(filepath) as img:\n",
    "            img.verify()\n",
    "        return False\n",
    "    except (UnidentifiedImageError, OSError):\n",
    "        print(f\"Deleting corrupted image: {filepath}\")\n",
    "        os.remove(filepath)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2933bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Advanced Image Preprocessing\n",
    "# =======================================================================\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"\n",
    "    Apply histogram equalization for better feature extraction.\n",
    "    Improves contrast and enhances facial features.\n",
    "    \"\"\"\n",
    "    # Convert to uint8 if needed\n",
    "    if image.dtype != np.uint8:\n",
    "        image = (image * 255).astype(np.uint8)\n",
    "    \n",
    "    # Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    if len(image.shape) == 2:  # Grayscale\n",
    "        enhanced = clahe.apply(image)\n",
    "    else:  # If RGB, convert to grayscale first\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        enhanced = clahe.apply(gray)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    return enhanced.astype(np.float32) / 255.0\n",
    "\n",
    "class PreprocessingImageDataGenerator(ImageDataGenerator):\n",
    "    \"\"\"Custom generator with advanced preprocessing.\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, preprocessing_function=None, **kwargs):\n",
    "        super().__init__(*args, preprocessing_function=preprocessing_function, **kwargs)\n",
    "    \n",
    "    def standardize(self, x):\n",
    "        x = super().standardize(x)\n",
    "        # Apply additional preprocessing\n",
    "        return preprocess_image(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1b1339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n",
      "Training samples: 28821\n",
      "Validation samples: 7066\n",
      "Class indices: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Data Augmentation and Loading\n",
    "# =======================================================================\n",
    "\n",
    "aug_map = {\n",
    "    \"none\": dict(rescale=1./255),\n",
    "    \"light\": dict(rescale=1./255, rotation_range=10, width_shift_range=0.1,\n",
    "                  height_shift_range=0.1, zoom_range=0.1, horizontal_flip=True),\n",
    "    \"strong\": dict(rescale=1./255, rotation_range=20, width_shift_range=0.15,\n",
    "                   height_shift_range=0.15, shear_range=0.15, zoom_range=0.15,\n",
    "                   brightness_range=[0.8,1.2], horizontal_flip=True)\n",
    "}\n",
    "\n",
    "# Training data generator with standard augmentation\n",
    "datagen_train = ImageDataGenerator(**aug_map[cfg[\"aug_level\"]])\n",
    "\n",
    "datagen_validation = ImageDataGenerator(rescale=1./255)                                        \n",
    "\n",
    "# Create training set\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    os.path.join(folder_path, \"train\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# Create validation set\n",
    "validation_set = datagen_validation.flow_from_directory(\n",
    "    os.path.join(folder_path, \"validation\"),\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode='rgb',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False,  # stable evaluation\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Training samples: {train_set.n}\")\n",
    "print(f\"Validation samples: {validation_set.n}\")\n",
    "print(f\"Class indices: {train_set.class_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aaac0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building model with mobilenet backbone...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Model Architecture - MobileNetV2 (Transfer Learning)\n",
    "# =======================================================================\n",
    "from tensorflow.keras import layers\n",
    "# Build model (one path only)\n",
    "\n",
    "def build_model(backbone=\"mobilenet\", input_shape=(48, 48, 3)):\n",
    "    \"\"\"Build emotion recognition model with specified backbone.\"\"\"\n",
    "    \n",
    "    x = Input(shape=input_shape)\n",
    "    \n",
    "    if backbone == \"mobilenet\":\n",
    "        base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        # Freeze base model initially\n",
    "        base.trainable = False\n",
    "        y = base(x)\n",
    "        \n",
    "    elif backbone == \"efficientnet\":\n",
    "        base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "        base.trainable = False\n",
    "        y = base(x)\n",
    "        \n",
    "    elif backbone == \"custom_cnn\":\n",
    "        # Define your own small CNN\n",
    "        y = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "        y = layers.MaxPooling2D()(y)\n",
    "        y = layers.Conv2D(64, 3, activation='relu', padding='same')(y)\n",
    "        y = layers.MaxPooling2D()(y)\n",
    "        y = layers.Conv2D(128, 3, activation='relu', padding='same')(y)\n",
    "        y = layers.MaxPooling2D()(y)\n",
    "        base = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "    \n",
    "    # Classification head\n",
    "    y = GlobalAveragePooling2D()(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.5)(y)\n",
    "    y = Dense(256, activation='relu')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(no_of_classes, activation='softmax', dtype='float32')(y)  # Force float32 for stability\n",
    "    \n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model, base\n",
    "\n",
    "\n",
    "# Build model\n",
    "backbone_type = cfg.get(\"backbone\", \"mobilenet\")\n",
    "print(f\"\\nBuilding model with {backbone_type} backbone...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2629dd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marty\\AppData\\Local\\Temp\\ipykernel_32516\\4195345029.py:13: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model built with mobilenet backbone\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m3\u001b[0m)      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_224            â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m2,257,984\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)                    â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚         \u001b[38;5;34m5,120\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚       \u001b[38;5;34m327,936\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚         \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              â”‚         \u001b[38;5;34m1,799\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,593,863</span> (9.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,593,863\u001b[0m (9.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">332,807</span> (1.27 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m332,807\u001b[0m (1.27 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,261,056</span> (8.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,261,056\u001b[0m (8.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model, base_model = build_model(backbone=backbone_type, input_shape=(picture_size, picture_size, 3))\n",
    "\n",
    "print(f\"\\nModel built with {backbone_type} backbone\")\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "762e2d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with learning rate: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Compile Model\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Model compiled with learning rate: {learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a835adde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Callbacks configured\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Training Callbacks\n",
    "# =======================================================================\n",
    "\n",
    "# Create callbacks for training\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks_list= [checkpoint, early_stopping, reduce_lr]\n",
    "print(\"Callbacks configured\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473c7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Target: 20 epochs with early stopping\n",
      "Batch size: 128\n",
      "Learning rate: 0.0001\n",
      "Mixed precision: ENABLED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 716ms/step - accuracy: 0.1726 - loss: 2.6426\n",
      "Epoch 1: val_accuracy improved from None to 0.29162, saving model to best_model.keras\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 900ms/step - accuracy: 0.1887 - loss: 2.5231 - val_accuracy: 0.2916 - val_loss: 1.9062 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:32\u001b[0m 683ms/step - accuracy: 0.1719 - loss: 2.3840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Model Training\n",
    "# =======================================================================\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Target: {epochs} epochs with early stopping\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"Mixed precision: ENABLED\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.n // train_set.batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=validation_set.n // validation_set.batch_size,\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"âœ… Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Phase 2: fine-tune (switch applied here) ----\n",
    "# Unfreeze the base model for fine-tuning\n",
    "if base_model is not None and cfg.get(\"fine_tune\", False):\n",
    "    print(\"\\nStarting fine-tuning phase...\")\n",
    "    \n",
    "    # Unfreeze the last few layers\n",
    "    base_model.trainable = True\n",
    "    fine_tune_at = len(base_model.layers) - 20  # Unfreeze last 20 layers\n",
    "    \n",
    "    for layer in base_model.layers[:fine_tune_at]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate/10),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Fine-tune for additional epochs\n",
    "    fine_tune_epochs = cfg.get(\"fine_tune_epochs\", 10)\n",
    "    history_fine = model.fit(\n",
    "        train_set,\n",
    "        validation_data=validation_set,\n",
    "        epochs=fine_tune_epochs,\n",
    "        callbacks=callbacks_list,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4420334",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Save Model (Versioned) - Enhanced with models/ directory\n",
    "# =======================================================================\n",
    "import re, os, json\n",
    "from pathlib import Path\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    \"\"\"Keep alphanumeric and a few safe chars.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z0-9._-]+', '', str(s))\n",
    "\n",
    "def cfg_tag(cfg: dict, keys=('backbone','aug_level','optimizer','lr','precision','freeze','fine_tune_layers')):\n",
    "    \"\"\"Build compact tag like: b-mbv2_aug-strong_opt-adam_lr-1e-4_prec-mixed_ft-part40\"\"\"\n",
    "    kmap = {\n",
    "        'backbone': 'b',\n",
    "        'aug_level': 'aug',\n",
    "        'optimizer': 'opt',\n",
    "        'lr': 'lr',\n",
    "        'precision': 'prec',\n",
    "        'freeze': 'frz',\n",
    "        'fine_tune_layers': 'ftl',\n",
    "    }\n",
    "    parts = []\n",
    "    for k in keys:\n",
    "        if k in cfg:\n",
    "            v = cfg[k]\n",
    "            # Shorten common values\n",
    "            if k == 'backbone':\n",
    "                if v == 'mobilenet_v2' or v == 'mobilenet':\n",
    "                    v = 'mbv2'\n",
    "                elif v == 'efficientnet':\n",
    "                    v = 'effb0'\n",
    "                elif v == 'custom_cnn':\n",
    "                    v = 'cnn'\n",
    "            elif k == 'optimizer':\n",
    "                if v == 'adam':\n",
    "                    v = 'adam'\n",
    "                elif v == 'sgd':\n",
    "                    v = 'sgd'\n",
    "            elif k == 'precision':\n",
    "                if v == 'mixed':\n",
    "                    v = 'mp'\n",
    "                elif v == 'float32':\n",
    "                    v = 'fp32'\n",
    "            \n",
    "            parts.append(f\"{kmap.get(k,k)}-{sanitize(v)}\")\n",
    "    return \"_\".join(parts)\n",
    "\n",
    "def save_model_versioned(model, base_name='emotion_model', extension='.keras',\n",
    "                         directory='models', save_weights=True, cfg=None, \n",
    "                         save_config=True):\n",
    "    \"\"\"\n",
    "    Save model with versioning to models/ directory.\n",
    "    \n",
    "    Args:\n",
    "        model: Keras model to save\n",
    "        base_name: Base name for the model file\n",
    "        extension: File extension (.keras or .h5)\n",
    "        directory: Directory to save to (default: 'models')\n",
    "        save_weights: Whether to save weights separately\n",
    "        cfg: Configuration dict to include in filename and save as JSON\n",
    "        save_config: Whether to save config as separate JSON file\n",
    "    \n",
    "    Returns:\n",
    "        dict: Paths to saved files\n",
    "    \"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"ğŸ“ Saving to directory: {directory}/\")\n",
    "    \n",
    "    # Build filename with config tag\n",
    "    tag = cfg_tag(cfg) if cfg else None\n",
    "    name_with_tag = f\"{base_name}_{tag}\" if tag else base_name\n",
    "    \n",
    "    # Find existing versions\n",
    "    existing = [f for f in os.listdir(directory)\n",
    "                if re.match(rf'{re.escape(name_with_tag)}_v\\d+{re.escape(extension)}$', f)]\n",
    "    \n",
    "    if existing:\n",
    "        vers = [int(re.search(rf'_v(\\d+){re.escape(extension)}$', f).group(1)) for f in existing]\n",
    "        next_v = max(vers) + 1\n",
    "    else:\n",
    "        next_v = 1\n",
    "    \n",
    "    paths = {}\n",
    "    \n",
    "    # Save full model\n",
    "    model_filename = f\"{name_with_tag}_v{next_v}{extension}\"\n",
    "    model_path = os.path.join(directory, model_filename)\n",
    "    model.save(model_path)\n",
    "    print(f\"ğŸ’¾ Saved model: {model_path}\")\n",
    "    paths['model'] = model_path\n",
    "    \n",
    "    # Save weights separately\n",
    "    if save_weights:\n",
    "        weights_filename = f\"{name_with_tag}_v{next_v}.weights.h5\"\n",
    "        weights_path = os.path.join(directory, weights_filename)\n",
    "        model.save_weights(weights_path)\n",
    "        print(f\"ğŸ’¾ Saved weights: {weights_path}\")\n",
    "        paths['weights'] = weights_path\n",
    "    \n",
    "    # Save configuration as JSON\n",
    "    if cfg and save_config:\n",
    "        cfg_filename = f\"{name_with_tag}_v{next_v}.cfg.json\"\n",
    "        cfg_path = os.path.join(directory, cfg_filename)\n",
    "        with open(cfg_path, 'w') as f:\n",
    "            json.dump(cfg, f, indent=2)\n",
    "        print(f\"ğŸ’¾ Saved config: {cfg_path}\")\n",
    "        paths['cfg'] = cfg_path\n",
    "    \n",
    "    # Save training summary\n",
    "    summary_filename = f\"{name_with_tag}_v{next_v}.summary.txt\"\n",
    "    summary_path = os.path.join(directory, summary_filename)\n",
    "    with open(summary_path, 'w') as f:\n",
    "        model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "    print(f\"ğŸ’¾ Saved summary: {summary_path}\")\n",
    "    paths['summary'] = summary_path\n",
    "    \n",
    "    print(f\"âœ… All files saved with version: v{next_v}\")\n",
    "    return paths\n",
    "\n",
    "# =======================================================================\n",
    "# Usage Example\n",
    "# =======================================================================\n",
    "\n",
    "# After training, save the model\n",
    "saved_paths = save_model_versioned(\n",
    "    model=model,\n",
    "    base_name='emotion_model',\n",
    "    extension='.keras',\n",
    "    directory='models',\n",
    "    save_weights=True,\n",
    "    cfg=cfg,\n",
    "    save_config=True\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ“¦ Saved files:\")\n",
    "for key, path in saved_paths.items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Evaluation and Visualization\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss')\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Final evaluation\n",
    "print(\"\\nFinal Evaluation:\")\n",
    "train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "val_loss, val_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "model.save('emotion_recognition_final.keras')\n",
    "print(\"\\nModel saved as 'emotion_recognition_final.keras'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b26579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Training Visualization\n",
    "# =======================================================================\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "plt.title('Model Loss', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "plt.title('Model Accuracy', fontsize=14)\n",
    "plt.ylabel('Accuracy', fontsize=12)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aef709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Model Evaluation\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "train_loss, train_acc = model.evaluate(train_set, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(validation_set, verbose=0)\n",
    "\n",
    "print(f\"ğŸ“Š FINAL RESULTS\")\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Training Loss: {train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {test_loss:.4f}\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fde5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Detailed Performance Analysis\n",
    "# =======================================================================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras.models import load_model\n",
    "\n",
    "class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "\n",
    "# Load best model\n",
    "my_model = load_model('best_model.keras', compile=False)\n",
    "\n",
    "# Generate predictions on validation set\n",
    "print(\"Generating predictions on validation set...\")\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "for i in range(len(validation_set)):\n",
    "    batch_images, batch_labels = validation_set[i]\n",
    "    batch_predictions = my_model.predict(batch_images, verbose=0)\n",
    "    predictions.extend(np.argmax(batch_predictions, axis=1))\n",
    "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
    "    if i >= validation_set.n // test_set.batch_size:\n",
    "        break\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "true_labels = np.array(true_labels)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='rocket', \n",
    "            xticklabels=class_labels, \n",
    "            yticklabels=class_labels,\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Facial Emotion Recognition', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted Emotion', fontsize=12)\n",
    "plt.ylabel('True Emotion', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ˆ DETAILED CLASSIFICATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(classification_report(true_labels, predictions, target_names=class_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff496014",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Export to ONNX for Snapdragon Optimization\n",
    "# =======================================================================\n",
    "\n",
    "try:\n",
    "    import tf2onnx\n",
    "    import onnx\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸš€ Exporting model to ONNX format for Snapdragon optimization...\")\n",
    "    \n",
    "    # Convert to ONNX\n",
    "    spec = (tf.TensorSpec((None, 48, 48, 1), tf.float32, name=\"input\"),)\n",
    "    output_path = \"emotion_model.onnx\"\n",
    "    \n",
    "    model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=output_path)\n",
    "    \n",
    "    print(f\"âœ… Model exported to {output_path}\")\n",
    "    print(\"This model is optimized for Snapdragon X Elite inference!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âš ï¸ tf2onnx not installed. Run: pip install tf2onnx\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ONNX export failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c42a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nğŸ‰ Training complete! Your optimized model is ready.\")\n",
    "print(f\"â±ï¸ Expected training time: 30-50 minutes on Snapdragon X Elite\")\n",
    "print(paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
