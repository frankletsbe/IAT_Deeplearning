{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d32607d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: 48x48 grayscale\n",
      "Batch size: 128\n",
      "Epochs: 30\n",
      "âœ… XLA compilation enabled\n",
      "âœ… Multi-threading optimized for ARM architecture\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Snapdragon X Elite Optimization Configuration\n",
    "# =======================================================================\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Environment & Optimizations\n",
    "# =======================================================================\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Threading and backend opts (tune \"8\" to your CPU)\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"TF_NUM_INTRAOP_THREADS\"] = \"8\"\n",
    "os.environ[\"TF_NUM_INTEROP_THREADS\"] = \"8\"\n",
    "\n",
    "\n",
    "\n",
    "# Import deep learning libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Configuration Parameters\n",
    "# =======================================================================\n",
    "\n",
    "picture_size = 48  # Input image size (48x48 grayscale)\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "no_of_classes = 7  # Angry, Disgust, Fear, Happy, Neutral, Sad, Surprise\n",
    "learning_rate = 0.0001\n",
    "\n",
    "train_dir = 'data/images/train/'\n",
    "val_dir   = 'data/images/validation/'\n",
    "\n",
    "print(f\"Image size: {picture_size}x{picture_size} grayscale\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Epochs: {epochs}\")\n",
    "\n",
    "\n",
    "# 1. Enable XLA (Accelerated Linear Algebra) compilation\n",
    "tf.config.optimizer.set_jit(True)\n",
    "print(\"âœ… XLA compilation enabled\")\n",
    "\n",
    "# 2. Configure thread pools for ARM big.LITTLE architecture\n",
    "# Snapdragon X Elite has performance and efficiency cores\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8)  # Between operations\n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)  # Within operations\n",
    "print(\"âœ… Multi-threading optimized for ARM architecture\")\n",
    "\n",
    "# 3. Enable oneDNN optimizations (ARM compute library)\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "# 4. Memory growth to prevent OOM on ARM devices\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    print(f\"âœ… Found {len(physical_devices)} GPU/NPU device(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99c3722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mixed precision FP16 enabled\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Mixed precision (already in your code - keep it!)\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "print(\"âœ… Mixed precision FP16 enabled\")\n",
    "\n",
    "# GPU/NPU memory growth (if present)\n",
    "for dev_type in ['GPU', 'TPU']:\n",
    "    devices = tf.config.list_physical_devices(dev_type)\n",
    "    if devices:\n",
    "        try:\n",
    "            for d in devices:\n",
    "                tf.config.experimental.set_memory_growth(d, True)\n",
    "            print(f\"âœ… Memory growth set for {dev_type}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not set memory growth for {dev_type}: {e}\")    \n",
    "\n",
    "\n",
    "\n",
    "# 6. Prefetch and cache dataset for faster loading\n",
    "def optimize_dataset(generator, cache=True):\n",
    "    \"\"\"Optimize data pipeline for Snapdragon NPU\"\"\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    \n",
    "    # Convert generator to tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 7), dtype=tf.float32)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if cache:\n",
    "        dataset = dataset.cache()  # Cache in memory\n",
    "    \n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)  # Prefetch next batch\n",
    "    \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c516572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marty\\AppData\\Local\\Temp\\ipykernel_14240\\3056608763.py:52: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  base_model = MobileNetV2(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,621,255\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Optimized Model Architecture for Snapdragon NPU\n",
    "# =======================================================================\n",
    "\n",
    "def build_optimized_model():\n",
    "    \"\"\"\n",
    "    Lightweight model optimized for Snapdragon X Elite NPU.\n",
    "    Uses depthwise separable convolutions (NPU-friendly).\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.layers import DepthwiseConv2D, SeparableConv2D\n",
    "    \n",
    "    inputs = Input(shape=(picture_size, picture_size, 1))\n",
    "    \n",
    "    # Initial conv\n",
    "    x = Conv2D(32, (3, 3), padding='same', use_bias=False)(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    # Depthwise separable blocks (NPU optimized)\n",
    "    def depthwise_block(x, filters, strides=1):\n",
    "        x = SeparableConv2D(filters, (3, 3), strides=strides, \n",
    "                           padding='same', use_bias=False)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    x = depthwise_block(x, 64, strides=2)   # 24x24\n",
    "    x = depthwise_block(x, 128, strides=2)  # 12x12\n",
    "    x = depthwise_block(x, 256, strides=2)  # 6x6\n",
    "    x = depthwise_block(x, 512)             # 6x6\n",
    "    \n",
    "    # Global pooling and classification\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(7, activation='softmax', dtype='float32')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# Build Model (choose ONE option)\n",
    "# =======================================================================\n",
    "option = 1\n",
    "\n",
    "if option == 1:\n",
    "# OPTION 1: Use your existing MobileNetV2 model (keep your current code)\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(48, 48, 3)\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=(48, 48, 1))\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([inputs, inputs, inputs])\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(7, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "# OR\n",
    "\n",
    "# OPTION 2: Use the lightweight optimized model (faster on Snapdragon)\n",
    "elif option == 2:\n",
    "    model = build_optimized_model()\n",
    "\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fe9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Quantization-Aware Training (for NPU deployment)\n",
    "# =======================================================================\n",
    "\n",
    "def convert_to_tflite_quantized(model, train_generator, output_path='emotion_model_int8.tflite'):\n",
    "    train_generator.reset()\n",
    "    \"\"\"\n",
    "    Convert model to INT8 quantized TFLite for maximum NPU performance.\n",
    "    This will run MUCH faster on Snapdragon NPU.\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Representative dataset for quantization\n",
    "    def representative_dataset():\n",
    "        for i in range(100):\n",
    "            batch, _ = next(iter(train_generator))\n",
    "            yield [batch[:1].astype(np.float32)]\n",
    "    \n",
    "    # Convert to TFLite with INT8 quantization\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save quantized model\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"âœ… INT8 quantized model saved: {output_path}\")\n",
    "    print(f\"   Model size: {len(tflite_model) / 1024:.2f} KB\")\n",
    "    print(\"   This model will run 3-5x faster on Snapdragon NPU!\")\n",
    "    \n",
    "    return tflite_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3ffe8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28821 images belonging to 7 classes.\n",
      "Found 7066 images belonging to 7 classes.\n",
      "Training samples: 28821 | Validation samples: 7066\n",
      "Classes: {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'neutral': 4, 'sad': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Data loaders\n",
    "# =======================================================================\n",
    "# Simple normalization; you can add augmentations as needed\n",
    "datagen_train = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen_val = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_set = datagen_train.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_set = datagen_val.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(picture_size, picture_size),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {train_set.n} | Validation samples: {val_set.n}\")\n",
    "print(f\"Classes: {train_set.class_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b77a99c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)        </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape      </span>â”ƒ<span style=\"font-weight: bold\">    Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to      </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                 â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>) â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       â”‚                   â”‚            â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>,      â”‚  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> â”‚ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        â”‚ <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)      â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ mobilenetv2_1.00â€¦ â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePoolâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚    <span style=\"color: #00af00; text-decoration-color: #00af00\">327,936</span> â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> â”‚ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatioâ€¦</span> â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">903</span> â”‚ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ -                 â”‚\n",
       "â”‚ (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ concatenate         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m3\u001b[0m) â”‚          \u001b[38;5;34m0\u001b[0m â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mConcatenate\u001b[0m)       â”‚                   â”‚            â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”‚                     â”‚                   â”‚            â”‚ input_layer_1[\u001b[38;5;34m0\u001b[0m]â€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ mobilenetv2_1.00_2â€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m,      â”‚  \u001b[38;5;34m2,257,984\u001b[0m â”‚ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] â”‚\n",
       "â”‚ (\u001b[38;5;33mFunctional\u001b[0m)        â”‚ \u001b[38;5;34m1280\u001b[0m)             â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)      â”‚          \u001b[38;5;34m0\u001b[0m â”‚ mobilenetv2_1.00â€¦ â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePoolâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚    \u001b[38;5;34m327,936\u001b[0m â”‚ global_average_pâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚      \u001b[38;5;34m1,024\u001b[0m â”‚ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚     \u001b[38;5;34m32,896\u001b[0m â”‚ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalizatioâ€¦ â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚        \u001b[38;5;34m512\u001b[0m â”‚ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalizatioâ€¦\u001b[0m â”‚                   â”‚            â”‚                   â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m) â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       â”‚          \u001b[38;5;34m0\u001b[0m â”‚ batch_normalizatâ€¦ â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         â”‚        \u001b[38;5;34m903\u001b[0m â”‚ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,621,255</span> (10.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,621,255\u001b[0m (10.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">362,503</span> (1.38 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m362,503\u001b[0m (1.38 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,258,752</span> (8.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,258,752\u001b[0m (8.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Compile (avoid jit_compile=True here for stability on ARM + FP16)\n",
    "# =======================================================================\n",
    "\n",
    "# =======================================================================\n",
    "# Now compile\n",
    "# =======================================================================\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate,\n",
    "    clipnorm=1.0,\n",
    "    epsilon=1e-7\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    "    #jit_compile=True\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42ebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Callbacks\n",
    "# =======================================================================\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=\"best_model.keras\",\n",
    "    monitor='val_accuracy',\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    min_lr=1e-5\n",
    ")\n",
    "\n",
    "class PerformanceCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start = time.time()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start\n",
    "        steps = train_set.n // train_set.batch_size\n",
    "        imgs = steps * train_set.batch_size\n",
    "        sps = imgs / epoch_time if epoch_time > 0 else 0.0\n",
    "        print(f\"\\nâš¡ Epoch time: {epoch_time:.1f}s | Throughput: {sps:.0f} img/s\")\n",
    "\n",
    "callbacks = [checkpoint, early_stopping, reduce_lr, PerformanceCallback()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a80e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 777ms/step - accuracy: 0.1730 - loss: 2.7038\n",
      "Epoch 1: val_accuracy improved from None to 0.29205, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 232.9s | Throughput: 124 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 980ms/step - accuracy: 0.1907 - loss: 2.5837 - val_accuracy: 0.2920 - val_loss: 1.9158 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:53\u001b[0m 774ms/step - accuracy: 0.1562 - loss: 2.3705"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.29205 to 0.29233, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 82.0s | Throughput: 351 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 363ms/step - accuracy: 0.1562 - loss: 2.3705 - val_accuracy: 0.2923 - val_loss: 1.9151 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 769ms/step - accuracy: 0.2191 - loss: 2.3590\n",
      "Epoch 3: val_accuracy improved from 0.29233 to 0.32259, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 214.8s | Throughput: 134 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 955ms/step - accuracy: 0.2227 - loss: 2.3207 - val_accuracy: 0.3226 - val_loss: 1.8246 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:49\u001b[0m 757ms/step - accuracy: 0.2266 - loss: 2.3002\n",
      "Epoch 4: val_accuracy did not improve from 0.32259\n",
      "\n",
      "âš¡ Epoch time: 41.6s | Throughput: 693 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 182ms/step - accuracy: 0.2266 - loss: 2.3002 - val_accuracy: 0.3226 - val_loss: 1.8244 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 776ms/step - accuracy: 0.2398 - loss: 2.1788\n",
      "Epoch 5: val_accuracy improved from 0.32259 to 0.33807, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 217.1s | Throughput: 133 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 965ms/step - accuracy: 0.2446 - loss: 2.1579 - val_accuracy: 0.3381 - val_loss: 1.7627 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:47\u001b[0m 746ms/step - accuracy: 0.2578 - loss: 2.1725\n",
      "Epoch 6: val_accuracy did not improve from 0.33807\n",
      "\n",
      "âš¡ Epoch time: 40.2s | Throughput: 717 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 176ms/step - accuracy: 0.2578 - loss: 2.1725 - val_accuracy: 0.3379 - val_loss: 1.7627 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 744ms/step - accuracy: 0.2487 - loss: 2.0935\n",
      "Epoch 7: val_accuracy improved from 0.33807 to 0.34389, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 208.2s | Throughput: 138 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 926ms/step - accuracy: 0.2545 - loss: 2.0663 - val_accuracy: 0.3439 - val_loss: 1.7357 - learning_rate: 1.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m2:55\u001b[0m 783ms/step - accuracy: 0.2969 - loss: 2.0539\n",
      "Epoch 8: val_accuracy did not improve from 0.34389\n",
      "\n",
      "âš¡ Epoch time: 40.9s | Throughput: 704 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 179ms/step - accuracy: 0.2969 - loss: 2.0539 - val_accuracy: 0.3438 - val_loss: 1.7359 - learning_rate: 1.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20s/step - accuracy: 0.2625 - loss: 2.0087 \n",
      "Epoch 9: val_accuracy improved from 0.34389 to 0.34759, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 4591.7s | Throughput: 6 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4592s\u001b[0m 20s/step - accuracy: 0.2632 - loss: 1.9973 - val_accuracy: 0.3476 - val_loss: 1.7034 - learning_rate: 1.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m13:26\u001b[0m 4s/step - accuracy: 0.2734 - loss: 1.9199\n",
      "Epoch 10: val_accuracy did not improve from 0.34759\n",
      "\n",
      "âš¡ Epoch time: 189.5s | Throughput: 152 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 830ms/step - accuracy: 0.2734 - loss: 1.9199 - val_accuracy: 0.3474 - val_loss: 1.7033 - learning_rate: 1.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2662 - loss: 1.9665\n",
      "Epoch 11: val_accuracy improved from 0.34759 to 0.35099, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 458.8s | Throughput: 63 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 2s/step - accuracy: 0.2708 - loss: 1.9515 - val_accuracy: 0.3510 - val_loss: 1.6828 - learning_rate: 1.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:26\u001b[0m 923ms/step - accuracy: 0.3125 - loss: 1.8238\n",
      "Epoch 12: val_accuracy did not improve from 0.35099\n",
      "\n",
      "âš¡ Epoch time: 47.4s | Throughput: 608 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 207ms/step - accuracy: 0.3125 - loss: 1.8238 - val_accuracy: 0.3506 - val_loss: 1.6827 - learning_rate: 1.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 833ms/step - accuracy: 0.2798 - loss: 1.9238\n",
      "Epoch 13: val_accuracy improved from 0.35099 to 0.35128, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 238.0s | Throughput: 121 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 1s/step - accuracy: 0.2802 - loss: 1.9132 - val_accuracy: 0.3513 - val_loss: 1.6696 - learning_rate: 1.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:34\u001b[0m 960ms/step - accuracy: 0.2734 - loss: 1.9980\n",
      "Epoch 14: val_accuracy improved from 0.35128 to 0.35185, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 48.4s | Throughput: 595 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 212ms/step - accuracy: 0.2734 - loss: 1.9980 - val_accuracy: 0.3518 - val_loss: 1.6696 - learning_rate: 1.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861ms/step - accuracy: 0.2819 - loss: 1.8889\n",
      "Epoch 15: val_accuracy improved from 0.35185 to 0.35426, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 239.3s | Throughput: 120 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 1s/step - accuracy: 0.2830 - loss: 1.8835 - val_accuracy: 0.3543 - val_loss: 1.6588 - learning_rate: 1.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:02\u001b[0m 814ms/step - accuracy: 0.2734 - loss: 1.8606\n",
      "Epoch 16: val_accuracy improved from 0.35426 to 0.35440, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 46.3s | Throughput: 623 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 203ms/step - accuracy: 0.2734 - loss: 1.8606 - val_accuracy: 0.3544 - val_loss: 1.6587 - learning_rate: 1.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 865ms/step - accuracy: 0.2913 - loss: 1.8620\n",
      "Epoch 17: val_accuracy did not improve from 0.35440\n",
      "\n",
      "âš¡ Epoch time: 242.9s | Throughput: 119 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 1s/step - accuracy: 0.2893 - loss: 1.8557 - val_accuracy: 0.3541 - val_loss: 1.6456 - learning_rate: 1.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:09\u001b[0m 847ms/step - accuracy: 0.2969 - loss: 1.7947\n",
      "Epoch 18: val_accuracy did not improve from 0.35440\n",
      "\n",
      "âš¡ Epoch time: 51.2s | Throughput: 562 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 225ms/step - accuracy: 0.2969 - loss: 1.7947 - val_accuracy: 0.3541 - val_loss: 1.6457 - learning_rate: 1.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 850ms/step - accuracy: 0.2933 - loss: 1.8371\n",
      "Epoch 19: val_accuracy improved from 0.35440 to 0.35625, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 237.1s | Throughput: 121 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 1s/step - accuracy: 0.2966 - loss: 1.8293 - val_accuracy: 0.3562 - val_loss: 1.6359 - learning_rate: 1.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m3:02\u001b[0m 815ms/step - accuracy: 0.2422 - loss: 1.8908\n",
      "Epoch 20: val_accuracy did not improve from 0.35625\n",
      "\n",
      "âš¡ Epoch time: 48.7s | Throughput: 591 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 214ms/step - accuracy: 0.2422 - loss: 1.8908 - val_accuracy: 0.3558 - val_loss: 1.6360 - learning_rate: 1.0000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.2994 - loss: 1.8178\n",
      "Epoch 21: val_accuracy improved from 0.35625 to 0.35781, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 428.9s | Throughput: 67 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 2s/step - accuracy: 0.3013 - loss: 1.8111 - val_accuracy: 0.3578 - val_loss: 1.6304 - learning_rate: 1.0000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5:43\u001b[0m 2s/step - accuracy: 0.2656 - loss: 1.9104\n",
      "Epoch 22: val_accuracy did not improve from 0.35781\n",
      "\n",
      "âš¡ Epoch time: 57.8s | Throughput: 498 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 251ms/step - accuracy: 0.2656 - loss: 1.9104 - val_accuracy: 0.3575 - val_loss: 1.6305 - learning_rate: 1.0000e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.3009 - loss: 1.7924\n",
      "Epoch 23: val_accuracy improved from 0.35781 to 0.35838, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 400.7s | Throughput: 72 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 2s/step - accuracy: 0.3045 - loss: 1.7915 - val_accuracy: 0.3584 - val_loss: 1.6239 - learning_rate: 1.0000e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6:01\u001b[0m 2s/step - accuracy: 0.2812 - loss: 1.7977\n",
      "Epoch 24: val_accuracy did not improve from 0.35838\n",
      "\n",
      "âš¡ Epoch time: 82.9s | Throughput: 348 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 363ms/step - accuracy: 0.2812 - loss: 1.7977 - val_accuracy: 0.3581 - val_loss: 1.6239 - learning_rate: 1.0000e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3118 - loss: 1.7879\n",
      "Epoch 25: val_accuracy improved from 0.35838 to 0.36094, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 438.1s | Throughput: 66 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 2s/step - accuracy: 0.3079 - loss: 1.7812 - val_accuracy: 0.3609 - val_loss: 1.6173 - learning_rate: 1.0000e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5:45\u001b[0m 2s/step - accuracy: 0.3047 - loss: 1.7814\n",
      "Epoch 26: val_accuracy did not improve from 0.36094\n",
      "\n",
      "âš¡ Epoch time: 89.3s | Throughput: 323 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 392ms/step - accuracy: 0.3047 - loss: 1.7814 - val_accuracy: 0.3607 - val_loss: 1.6171 - learning_rate: 1.0000e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.3110 - loss: 1.7705\n",
      "Epoch 27: val_accuracy improved from 0.36094 to 0.36364, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 440.8s | Throughput: 65 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m441s\u001b[0m 2s/step - accuracy: 0.3092 - loss: 1.7694 - val_accuracy: 0.3636 - val_loss: 1.6096 - learning_rate: 1.0000e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m6:44\u001b[0m 2s/step - accuracy: 0.3203 - loss: 1.7930\n",
      "Epoch 28: val_accuracy did not improve from 0.36364\n",
      "\n",
      "âš¡ Epoch time: 88.2s | Throughput: 327 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 385ms/step - accuracy: 0.3203 - loss: 1.7930 - val_accuracy: 0.3628 - val_loss: 1.6095 - learning_rate: 1.0000e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3164 - loss: 1.7520\n",
      "Epoch 29: val_accuracy improved from 0.36364 to 0.36477, saving model to best_model.keras\n",
      "\n",
      "âš¡ Epoch time: 797.5s | Throughput: 36 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m797s\u001b[0m 4s/step - accuracy: 0.3144 - loss: 1.7558 - val_accuracy: 0.3648 - val_loss: 1.6082 - learning_rate: 1.0000e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m  1/225\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m5:31\u001b[0m 1s/step - accuracy: 0.3125 - loss: 1.7249\n",
      "Epoch 30: val_accuracy did not improve from 0.36477\n",
      "\n",
      "âš¡ Epoch time: 89.0s | Throughput: 323 img/s\n",
      "\u001b[1m225/225\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 391ms/step - accuracy: 0.3125 - loss: 1.7249 - val_accuracy: 0.3646 - val_loss: 1.6083 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "âœ… Training complete\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Train\n",
    "# =======================================================================\n",
    "print(\"\\nğŸš€ Starting training...\")\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    steps_per_epoch=train_set.n // train_set.batch_size,\n",
    "    validation_data=val_set,\n",
    "    validation_steps=val_set.n // val_set.batch_size,\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"âœ… Training complete\")\n",
    "\n",
    "# Optional: fine-tune some layers for better accuracy\n",
    "# Unfreeze last N blocks if you have time/headroom\n",
    "# base_model.trainable = True\n",
    "# for layer in base_model.layers[:-40]:\n",
    "#     layer.trainable = False\n",
    "# model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# history_ft = model.fit(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5477977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saved model: .\\emotion_model_v5.keras\n",
      "ğŸ’¾ Saved weights: .\\emotion_model_v5.weights.h5\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Save (versioned)\n",
    "# =======================================================================\n",
    "def save_model_versioned(model, base_name='emotion_model', extension='.keras', directory='.', save_weights=True):\n",
    "    import re, os\n",
    "    existing = [f for f in os.listdir(directory) if re.match(rf'{base_name}_v\\d+{re.escape(extension)}', f)]\n",
    "    if existing:\n",
    "        vers = [int(re.search(rf'{base_name}_v(\\d+){re.escape(extension)}', f).group(1)) for f in existing]\n",
    "        next_v = max(vers) + 1\n",
    "    else:\n",
    "        next_v = 1\n",
    "    model_path = os.path.join(directory, f\"{base_name}_v{next_v}{extension}\")\n",
    "    model.save(model_path)\n",
    "    print(f\"ğŸ’¾ Saved model: {model_path}\")\n",
    "    paths = {'model': model_path}\n",
    "    if save_weights:\n",
    "        weights_path = os.path.join(directory, f\"{base_name}_v{next_v}.weights.h5\")\n",
    "        model.save_weights(weights_path)\n",
    "        print(f\"ğŸ’¾ Saved weights: {weights_path}\")\n",
    "        paths['weights'] = weights_path\n",
    "    return paths\n",
    "\n",
    "paths = save_model_versioned(model, base_name='emotion_model', save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1184468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“¦ Exporting models for Snapdragon deployment\n",
      "======================================================================\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\marty\\AppData\\Local\\Temp\\tmp5hm625wl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\marty\\AppData\\Local\\Temp\\tmp5hm625wl\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\marty\\AppData\\Local\\Temp\\tmp5hm625wl'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='keras_tensor_155')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2638993211472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2638993214736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007856400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007856016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007855440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2638993215120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007856976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007856592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007855632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007855056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007857744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007857936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007858320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007857360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639007858128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013609744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013611856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013611664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013611280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013612432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013609552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013613584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013613776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013613008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013611088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013612624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013612240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013612048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013613200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013614736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013614160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013614352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013614544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013613392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013615696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013615120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013615312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013615504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013609936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013616656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013617040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013617232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013616272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013616848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013617808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013617424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013616464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013618768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013615888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013619920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013619344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013619536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013619728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013616080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013620880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013620304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013620496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013620688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013617616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013621840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013621264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013621456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013621648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013619152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013620112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013623760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013623184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013623376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013623568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013621072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013624720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013624144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013624336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013625104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013625680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013624528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013625296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013622992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013625488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013624912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639013623952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014511824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014511632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014511248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014510864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014511440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014513360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014513168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014511056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014514320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014513744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014513936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014514128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014510672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014515280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014514704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014514896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014515088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014512592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014516240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014515664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014515856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014516048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014513552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014516624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014516816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014514512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014518160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014515472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014519120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014518544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014518736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014518928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014516432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014520080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014519504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014519696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014519888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014517392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014521040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014520464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014520656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014520848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014518352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014521424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014521616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014521808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014519312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014520272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014523920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014523344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014523536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014523728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014521232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014524880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014524304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014524496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014524688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014522192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014525840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014525264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014525456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014526224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014523152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014526800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014525648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014526416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014524112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014526608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014526032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639014525072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032190544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032190736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032190352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032190160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032191696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032191120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032191312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032191504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032192656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032192080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032192272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032192464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032189584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032193616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032193040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032193232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032193424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032190928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032191888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032195536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032195152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032195344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032192848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032196496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032195920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032196112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032196304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032193808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032197456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032196880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032197072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032197264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032194768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032197840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032195728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032199376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032199184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032196688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032200336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032199760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032199952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032200144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032197648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032201296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032200720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032200912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032201104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032198608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032202256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032201680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032201872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032202064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032199568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032203216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032202640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032202832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032203024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032200528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032204176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032203600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032203792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032204560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032201488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032205136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032203984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032204752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032202448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032204944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639032204368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048114448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048115600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048115984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048116944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048116752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048116176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048117328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048117136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048114832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048118288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048118096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048117520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048118672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2639048118480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:854: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.ConcatV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %171 = \"tf.ConcatV2\"(%170, %170, %170, %4) {device = \"\"} : (tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> tensor<?x48x48x3xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %174 = \"tf.Conv2D\"(%173, %160) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %179 = \"tf.Relu6\"(%178) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %180 = \"tf.DepthwiseConv2dNative\"(%179, %164) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %185 = \"tf.Relu6\"(%184) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %186 = \"tf.Conv2D\"(%185, %167) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x24x24x16xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %191 = \"tf.Conv2D\"(%190, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x24x24x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %196 = \"tf.Relu6\"(%195) {device = \"\"} : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %197 = \"tf.Pad\"(%196, %7) {device = \"\"} : (tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> tensor<?x25x25x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %198 = \"tf.DepthwiseConv2dNative\"(%197, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x12x12x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %203 = \"tf.Relu6\"(%202) {device = \"\"} : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %204 = \"tf.Conv2D\"(%203, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %209 = \"tf.Conv2D\"(%208, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %214 = \"tf.Relu6\"(%213) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %215 = \"tf.DepthwiseConv2dNative\"(%214, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %220 = \"tf.Relu6\"(%219) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %221 = \"tf.Conv2D\"(%220, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %226 = \"tf.AddV2\"(%208, %225) {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %227 = \"tf.Conv2D\"(%226, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %232 = \"tf.Relu6\"(%231) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %233 = \"tf.Pad\"(%232, %7) {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> tensor<?x13x13x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %234 = \"tf.DepthwiseConv2dNative\"(%233, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x6x6x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %239 = \"tf.Relu6\"(%238) {device = \"\"} : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %240 = \"tf.Conv2D\"(%239, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %245 = \"tf.Conv2D\"(%244, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %250 = \"tf.Relu6\"(%249) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %251 = \"tf.DepthwiseConv2dNative\"(%250, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %256 = \"tf.Relu6\"(%255) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %257 = \"tf.Conv2D\"(%256, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %262 = \"tf.AddV2\"(%244, %261) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %263 = \"tf.Conv2D\"(%262, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %268 = \"tf.Relu6\"(%267) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %269 = \"tf.DepthwiseConv2dNative\"(%268, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %274 = \"tf.Relu6\"(%273) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %275 = \"tf.Conv2D\"(%274, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %280 = \"tf.AddV2\"(%262, %279) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %281 = \"tf.Conv2D\"(%280, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %286 = \"tf.Relu6\"(%285) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %287 = \"tf.Pad\"(%286, %7) {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> tensor<?x7x7x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %288 = \"tf.DepthwiseConv2dNative\"(%287, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x3x3x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %293 = \"tf.Relu6\"(%292) {device = \"\"} : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %294 = \"tf.Conv2D\"(%293, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %299 = \"tf.Conv2D\"(%298, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %304 = \"tf.Relu6\"(%303) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %305 = \"tf.DepthwiseConv2dNative\"(%304, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %310 = \"tf.Relu6\"(%309) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %311 = \"tf.Conv2D\"(%310, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %316 = \"tf.AddV2\"(%298, %315) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %317 = \"tf.Conv2D\"(%316, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %322 = \"tf.Relu6\"(%321) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %323 = \"tf.DepthwiseConv2dNative\"(%322, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %328 = \"tf.Relu6\"(%327) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %329 = \"tf.Conv2D\"(%328, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %334 = \"tf.AddV2\"(%316, %333) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %335 = \"tf.Conv2D\"(%334, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %340 = \"tf.Relu6\"(%339) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %341 = \"tf.DepthwiseConv2dNative\"(%340, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %346 = \"tf.Relu6\"(%345) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %347 = \"tf.Conv2D\"(%346, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %352 = \"tf.AddV2\"(%334, %351) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %353 = \"tf.Conv2D\"(%352, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %358 = \"tf.Relu6\"(%357) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %359 = \"tf.DepthwiseConv2dNative\"(%358, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %364 = \"tf.Relu6\"(%363) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %365 = \"tf.Conv2D\"(%364, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %370 = \"tf.Conv2D\"(%369, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %375 = \"tf.Relu6\"(%374) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %376 = \"tf.DepthwiseConv2dNative\"(%375, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %381 = \"tf.Relu6\"(%380) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %382 = \"tf.Conv2D\"(%381, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %387 = \"tf.AddV2\"(%369, %386) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %388 = \"tf.Conv2D\"(%387, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %393 = \"tf.Relu6\"(%392) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %394 = \"tf.DepthwiseConv2dNative\"(%393, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %399 = \"tf.Relu6\"(%398) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %400 = \"tf.Conv2D\"(%399, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %405 = \"tf.AddV2\"(%387, %404) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %406 = \"tf.Conv2D\"(%405, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %411 = \"tf.Relu6\"(%410) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %412 = \"tf.Pad\"(%411, %6) {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> tensor<?x5x5x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %413 = \"tf.DepthwiseConv2dNative\"(%412, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x2x2x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %418 = \"tf.Relu6\"(%417) {device = \"\"} : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %419 = \"tf.Conv2D\"(%418, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %424 = \"tf.Conv2D\"(%423, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %429 = \"tf.Relu6\"(%428) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %430 = \"tf.DepthwiseConv2dNative\"(%429, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %435 = \"tf.Relu6\"(%434) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %436 = \"tf.Conv2D\"(%435, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %441 = \"tf.AddV2\"(%423, %440) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %442 = \"tf.Conv2D\"(%441, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %447 = \"tf.Relu6\"(%446) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %448 = \"tf.DepthwiseConv2dNative\"(%447, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %453 = \"tf.Relu6\"(%452) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %454 = \"tf.Conv2D\"(%453, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %459 = \"tf.AddV2\"(%441, %458) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %460 = \"tf.Conv2D\"(%459, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %465 = \"tf.Relu6\"(%464) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %466 = \"tf.DepthwiseConv2dNative\"(%465, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %471 = \"tf.Relu6\"(%470) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %472 = \"tf.Conv2D\"(%471, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x2x2x320xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %477 = \"tf.Conv2D\"(%476, %161) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x2x2x1280xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %482 = \"tf.Relu6\"(%481) {device = \"\"} : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %486 = \"tf.MatMul\"(%485, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<256x1280xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %487 = \"tf.BiasAdd\"(%486, %13) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x256xf16>, tensor<256xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %488 = \"tf.Relu\"(%487) {device = \"\"} : (tensor<?x256xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %493 = \"tf.MatMul\"(%492, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x256xf16>, tensor<128x256xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %494 = \"tf.BiasAdd\"(%493, %12) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x128xf16>, tensor<128xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %495 = \"tf.Relu\"(%494) {device = \"\"} : (tensor<?x128xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: AddV2, BiasAdd, ConcatV2, Conv2D, DepthwiseConv2dNative, MatMul, Pad, Relu, Relu6\nDetails:\n\ttf.AddV2(tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> (tensor<?x12x12x24xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> (tensor<?x2x2x160xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> (tensor<?x3x3x64xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> (tensor<?x3x3x96xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> (tensor<?x6x6x32xf16>) : {device = \"\"}\n\ttf.BiasAdd(tensor<?x128xf16>, tensor<128xf16>) -> (tensor<?x128xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.BiasAdd(tensor<?x256xf16>, tensor<256xf16>) -> (tensor<?x256xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.ConcatV2(tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> (tensor<?x48x48x3xf16>) : {device = \"\"}\n\ttf.Conv2D(tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> (tensor<?x12x12x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> (tensor<?x12x12x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> (tensor<?x12x12x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> (tensor<?x24x24x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> (tensor<?x24x24x16xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> (tensor<?x2x2x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> (tensor<?x2x2x1280xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> (tensor<?x2x2x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> (tensor<?x2x2x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> (tensor<?x2x2x320xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> (tensor<?x3x3x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> (tensor<?x3x3x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> (tensor<?x3x3x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> (tensor<?x3x3x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> (tensor<?x3x3x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> (tensor<?x3x3x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x24x24x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> (tensor<?x6x6x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> (tensor<?x6x6x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> (tensor<?x6x6x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.DepthwiseConv2dNative(tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x12x12x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x6x6x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> (tensor<?x24x24x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> (tensor<?x12x12x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> (tensor<?x2x2x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> (tensor<?x3x3x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x3x3x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x2x2x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x6x6x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x3x3x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.MatMul(tensor<?x1280xf16>, tensor<256x1280xf16>) -> (tensor<?x256xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n\ttf.MatMul(tensor<?x256xf16>, tensor<128x256xf16>) -> (tensor<?x128xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n\ttf.Pad(tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> (tensor<?x13x13x144xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> (tensor<?x25x25x96xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> (tensor<?x5x5x576xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> (tensor<?x7x7x192xf16>) : {device = \"\"}\n\ttf.Relu(tensor<?x128xf16>) -> (tensor<?x128xf16>) : {device = \"\"}\n\ttf.Relu(tensor<?x256xf16>) -> (tensor<?x256xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x12x12x144xf16>) -> (tensor<?x12x12x144xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x12x12x96xf16>) -> (tensor<?x12x12x96xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x24x24x32xf16>) -> (tensor<?x24x24x32xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x24x24x96xf16>) -> (tensor<?x24x24x96xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x1280xf16>) -> (tensor<?x2x2x1280xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x576xf16>) -> (tensor<?x2x2x576xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x960xf16>) -> (tensor<?x2x2x960xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x192xf16>) -> (tensor<?x3x3x192xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x384xf16>) -> (tensor<?x3x3x384xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x576xf16>) -> (tensor<?x3x3x576xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x6x6x144xf16>) -> (tensor<?x6x6x144xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x6x6x192xf16>) -> (tensor<?x6x6x192xf16>) : {device = \"\"}\n\n<unknown>:0: note: see current operation: \n\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"keras_tensor_155\"]}], function_type = (tensor<?x48x48x1xf32>) -> tensor<?x7xf32>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n^bb0(%arg0: tensor<?x48x48x1xf32>):\n  %0 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<7x128xf32>}> : () -> tensor<7x128xf32>\n  %1 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128x256xf16>}> : () -> tensor<128x256xf16>\n  %2 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256x1280xf16>}> : () -> tensor<256x1280xf16>\n  %3 = \"arith.constant\"() <{value = dense<[0.0566285178, -0.224917859, 0.0279344376, 0.137221545, 0.115658529, 0.0914542153, -0.118997455]> : tensor<7xf32>}> : () -> tensor<7xf32>\n  %4 = \"arith.constant\"() <{value = dense<-1> : tensor<i32>}> : () -> tensor<i32>\n  %5 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n  %6 = \"arith.constant\"() <{value = dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n  %7 = \"arith.constant\"() <{value = dense<[[0, 0], [0, 1], [0, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n  %8 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf32>}> : () -> tensor<128xf32>\n  %9 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf32>}> : () -> tensor<128xf32>\n  %10 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf32>}> : () -> tensor<256xf32>\n  %11 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf32>}> : () -> tensor<256xf32>\n  %12 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf16>}> : () -> tensor<128xf16>\n  %13 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf16>}> : () -> tensor<256xf16>\n  %14 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %15 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %16 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %17 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %18 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %19 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %20 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x96xf16>}> : () -> tensor<1x1x384x96xf16>\n  %21 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %22 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %23 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %24 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %25 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %26 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %27 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %28 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %29 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n  %30 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %31 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %32 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %33 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %34 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %35 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %36 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %37 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %38 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n  %39 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %40 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %41 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %42 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %43 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %44 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %45 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %46 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %47 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x160xf16>}> : () -> tensor<1x1x576x160xf16>\n  %48 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %49 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %50 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %51 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %52 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %53 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %54 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %55 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %56 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n  %57 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %58 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %59 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %60 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %61 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %62 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %63 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %64 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %65 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n  %66 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %67 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %68 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %69 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %70 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %71 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %72 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %73 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %74 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x320xf16>}> : () -> tensor<1x1x960x320xf16>\n  %75 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n  %76 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n  %77 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x96x1xf16>}> : () -> tensor<3x3x96x1xf16>\n  %78 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %79 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %80 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x16x96xf16>}> : () -> tensor<1x1x16x96xf16>\n  %81 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %82 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %83 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x24xf16>}> : () -> tensor<1x1x96x24xf16>\n  %84 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %85 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %86 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n  %87 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %88 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %89 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n  %90 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %91 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %92 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x24xf16>}> : () -> tensor<1x1x144x24xf16>\n  %93 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %94 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %95 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n  %96 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %97 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %98 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n  %99 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %100 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %101 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x32xf16>}> : () -> tensor<1x1x144x32xf16>\n  %102 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %103 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %104 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %105 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %106 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %107 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %108 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %109 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %110 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n  %111 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %112 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %113 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %114 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %115 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %116 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %117 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %118 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %119 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n  %120 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %121 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %122 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %123 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %124 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %125 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %126 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %127 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %128 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x64xf16>}> : () -> tensor<1x1x192x64xf16>\n  %129 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %130 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %131 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %132 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %133 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %134 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %135 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %136 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %137 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %138 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %139 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %140 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %141 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %142 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %143 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %144 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %145 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %146 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %147 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %148 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %149 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %150 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %151 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %152 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %153 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %154 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %155 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %156 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %157 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %158 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %159 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %160 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n  %161 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x320x1280xf16>}> : () -> tensor<1x1x320x1280xf16>\n  %162 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n  %163 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n  %164 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x32x1xf16>}> : () -> tensor<3x3x32x1xf16>\n  %165 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %166 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %167 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x16xf16>}> : () -> tensor<1x1x32x16xf16>\n  %168 = \"arith.constant\"() <{value = dense<[4.83980179, 7.01497889, 6.59778308, 7.4643445, 3.82265639, 6.78915548, 5.1570487, 4.63491488, 4.46355867, 6.7842741, 7.31219244, 4.33215332, 6.80388832, 6.5691595, 5.91440105, 4.81517363]> : tensor<16xf32>}> : () -> tensor<16xf32>\n  %169 = \"arith.constant\"() <{value = dense<[-1.79617035, 3.17504382, 16.2980614, 24.0359573, 13.8513613, 3.49967337, -0.0168914795, 7.31451178, -13.8333397, 3.81765318, -2.79449153, 18.9408112, -18.930912, -11.2135153, 7.71239185, -2.86968422]> : tensor<16xf32>}> : () -> tensor<16xf32>\n  %170 = \"tfl.cast\"(%arg0) : (tensor<?x48x48x1xf32>) -> tensor<?x48x48x1xf16>\n  %171 = \"tf.ConcatV2\"(%170, %170, %170, %4) {device = \"\"} : (tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> tensor<?x48x48x3xf16>\n  %172 = \"tfl.cast\"(%171) : (tensor<?x48x48x3xf16>) -> tensor<?x48x48x3xf32>\n  %173 = \"tfl.cast\"(%172) : (tensor<?x48x48x3xf32>) -> tensor<?x48x48x3xf16>\n  %174 = \"tf.Conv2D\"(%173, %160) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x24x24x32xf16>\n  %175 = \"tfl.cast\"(%174) : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf32>\n  %176 = \"tfl.mul\"(%175, %158) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %177 = \"tfl.add\"(%176, %159) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %178 = \"tfl.cast\"(%177) : (tensor<?x24x24x32xf32>) -> tensor<?x24x24x32xf16>\n  %179 = \"tf.Relu6\"(%178) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n  %180 = \"tf.DepthwiseConv2dNative\"(%179, %164) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x24x24x32xf16>\n  %181 = \"tfl.cast\"(%180) : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf32>\n  %182 = \"tfl.mul\"(%181, %165) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %183 = \"tfl.add\"(%182, %166) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %184 = \"tfl.cast\"(%183) : (tensor<?x24x24x32xf32>) -> tensor<?x24x24x32xf16>\n  %185 = \"tf.Relu6\"(%184) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n  %186 = \"tf.Conv2D\"(%185, %167) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x24x24x16xf16>\n  %187 = \"tfl.cast\"(%186) : (tensor<?x24x24x16xf16>) -> tensor<?x24x24x16xf32>\n  %188 = \"tfl.mul\"(%187, %168) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x16xf32>, tensor<16xf32>) -> tensor<?x24x24x16xf32>\n  %189 = \"tfl.add\"(%188, %169) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x16xf32>, tensor<16xf32>) -> tensor<?x24x24x16xf32>\n  %190 = \"tfl.cast\"(%189) : (tensor<?x24x24x16xf32>) -> tensor<?x24x24x16xf16>\n  %191 = \"tf.Conv2D\"(%190, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x24x24x96xf16>\n  %192 = \"tfl.cast\"(%191) : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf32>\n  %193 = \"tfl.mul\"(%192, %81) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x96xf32>, tensor<96xf32>) -> tensor<?x24x24x96xf32>\n  %194 = \"tfl.add\"(%193, %82) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x96xf32>, tensor<96xf32>) -> tensor<?x24x24x96xf32>\n  %195 = \"tfl.cast\"(%194) : (tensor<?x24x24x96xf32>) -> tensor<?x24x24x96xf16>\n  %196 = \"tf.Relu6\"(%195) {device = \"\"} : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf16>\n  %197 = \"tf.Pad\"(%196, %7) {device = \"\"} : (tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> tensor<?x25x25x96xf16>\n  %198 = \"tf.DepthwiseConv2dNative\"(%197, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x12x12x96xf16>\n  %199 = \"tfl.cast\"(%198) : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf32>\n  %200 = \"tfl.mul\"(%199, %78) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x96xf32>, tensor<96xf32>) -> tensor<?x12x12x96xf32>\n  %201 = \"tfl.add\"(%200, %79) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x96xf32>, tensor<96xf32>) -> tensor<?x12x12x96xf32>\n  %202 = \"tfl.cast\"(%201) : (tensor<?x12x12x96xf32>) -> tensor<?x12x12x96xf16>\n  %203 = \"tf.Relu6\"(%202) {device = \"\"} : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf16>\n  %204 = \"tf.Conv2D\"(%203, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x12x12x24xf16>\n  %205 = \"tfl.cast\"(%204) : (tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf32>\n  %206 = \"tfl.mul\"(%205, %84) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %207 = \"tfl.add\"(%206, %85) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %208 = \"tfl.cast\"(%207) : (tensor<?x12x12x24xf32>) -> tensor<?x12x12x24xf16>\n  %209 = \"tf.Conv2D\"(%208, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n  %210 = \"tfl.cast\"(%209) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %211 = \"tfl.mul\"(%210, %90) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %212 = \"tfl.add\"(%211, %91) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %213 = \"tfl.cast\"(%212) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %214 = \"tf.Relu6\"(%213) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %215 = \"tf.DepthwiseConv2dNative\"(%214, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x12x12x144xf16>\n  %216 = \"tfl.cast\"(%215) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %217 = \"tfl.mul\"(%216, %87) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %218 = \"tfl.add\"(%217, %88) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %219 = \"tfl.cast\"(%218) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %220 = \"tf.Relu6\"(%219) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %221 = \"tf.Conv2D\"(%220, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x12x12x24xf16>\n  %222 = \"tfl.cast\"(%221) : (tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf32>\n  %223 = \"tfl.mul\"(%222, %93) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %224 = \"tfl.add\"(%223, %94) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %225 = \"tfl.cast\"(%224) : (tensor<?x12x12x24xf32>) -> tensor<?x12x12x24xf16>\n  %226 = \"tf.AddV2\"(%208, %225) {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf16>\n  %227 = \"tf.Conv2D\"(%226, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n  %228 = \"tfl.cast\"(%227) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %229 = \"tfl.mul\"(%228, %99) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %230 = \"tfl.add\"(%229, %100) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %231 = \"tfl.cast\"(%230) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %232 = \"tf.Relu6\"(%231) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %233 = \"tf.Pad\"(%232, %7) {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> tensor<?x13x13x144xf16>\n  %234 = \"tf.DepthwiseConv2dNative\"(%233, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x6x6x144xf16>\n  %235 = \"tfl.cast\"(%234) : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf32>\n  %236 = \"tfl.mul\"(%235, %96) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x144xf32>, tensor<144xf32>) -> tensor<?x6x6x144xf32>\n  %237 = \"tfl.add\"(%236, %97) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x144xf32>, tensor<144xf32>) -> tensor<?x6x6x144xf32>\n  %238 = \"tfl.cast\"(%237) : (tensor<?x6x6x144xf32>) -> tensor<?x6x6x144xf16>\n  %239 = \"tf.Relu6\"(%238) {device = \"\"} : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf16>\n  %240 = \"tf.Conv2D\"(%239, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x6x6x32xf16>\n  %241 = \"tfl.cast\"(%240) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %242 = \"tfl.mul\"(%241, %102) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %243 = \"tfl.add\"(%242, %103) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %244 = \"tfl.cast\"(%243) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %245 = \"tf.Conv2D\"(%244, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %246 = \"tfl.cast\"(%245) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %247 = \"tfl.mul\"(%246, %108) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %248 = \"tfl.add\"(%247, %109) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %249 = \"tfl.cast\"(%248) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %250 = \"tf.Relu6\"(%249) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %251 = \"tf.DepthwiseConv2dNative\"(%250, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n  %252 = \"tfl.cast\"(%251) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %253 = \"tfl.mul\"(%252, %105) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %254 = \"tfl.add\"(%253, %106) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %255 = \"tfl.cast\"(%254) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %256 = \"tf.Relu6\"(%255) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %257 = \"tf.Conv2D\"(%256, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n  %258 = \"tfl.cast\"(%257) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %259 = \"tfl.mul\"(%258, %111) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %260 = \"tfl.add\"(%259, %112) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %261 = \"tfl.cast\"(%260) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %262 = \"tf.AddV2\"(%244, %261) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n  %263 = \"tf.Conv2D\"(%262, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %264 = \"tfl.cast\"(%263) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %265 = \"tfl.mul\"(%264, %117) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %266 = \"tfl.add\"(%265, %118) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %267 = \"tfl.cast\"(%266) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %268 = \"tf.Relu6\"(%267) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %269 = \"tf.DepthwiseConv2dNative\"(%268, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n  %270 = \"tfl.cast\"(%269) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %271 = \"tfl.mul\"(%270, %114) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %272 = \"tfl.add\"(%271, %115) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %273 = \"tfl.cast\"(%272) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %274 = \"tf.Relu6\"(%273) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %275 = \"tf.Conv2D\"(%274, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n  %276 = \"tfl.cast\"(%275) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %277 = \"tfl.mul\"(%276, %120) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %278 = \"tfl.add\"(%277, %121) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %279 = \"tfl.cast\"(%278) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %280 = \"tf.AddV2\"(%262, %279) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n  %281 = \"tf.Conv2D\"(%280, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %282 = \"tfl.cast\"(%281) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %283 = \"tfl.mul\"(%282, %126) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %284 = \"tfl.add\"(%283, %127) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %285 = \"tfl.cast\"(%284) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %286 = \"tf.Relu6\"(%285) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %287 = \"tf.Pad\"(%286, %7) {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> tensor<?x7x7x192xf16>\n  %288 = \"tf.DepthwiseConv2dNative\"(%287, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x3x3x192xf16>\n  %289 = \"tfl.cast\"(%288) : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf32>\n  %290 = \"tfl.mul\"(%289, %123) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x192xf32>, tensor<192xf32>) -> tensor<?x3x3x192xf32>\n  %291 = \"tfl.add\"(%290, %124) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x192xf32>, tensor<192xf32>) -> tensor<?x3x3x192xf32>\n  %292 = \"tfl.cast\"(%291) : (tensor<?x3x3x192xf32>) -> tensor<?x3x3x192xf16>\n  %293 = \"tf.Relu6\"(%292) {device = \"\"} : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf16>\n  %294 = \"tf.Conv2D\"(%293, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x3x3x64xf16>\n  %295 = \"tfl.cast\"(%294) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %296 = \"tfl.mul\"(%295, %129) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %297 = \"tfl.add\"(%296, %130) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %298 = \"tfl.cast\"(%297) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %299 = \"tf.Conv2D\"(%298, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %300 = \"tfl.cast\"(%299) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %301 = \"tfl.mul\"(%300, %135) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %302 = \"tfl.add\"(%301, %136) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %303 = \"tfl.cast\"(%302) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %304 = \"tf.Relu6\"(%303) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %305 = \"tf.DepthwiseConv2dNative\"(%304, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %306 = \"tfl.cast\"(%305) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %307 = \"tfl.mul\"(%306, %132) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %308 = \"tfl.add\"(%307, %133) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %309 = \"tfl.cast\"(%308) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %310 = \"tf.Relu6\"(%309) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %311 = \"tf.Conv2D\"(%310, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %312 = \"tfl.cast\"(%311) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %313 = \"tfl.mul\"(%312, %138) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %314 = \"tfl.add\"(%313, %139) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %315 = \"tfl.cast\"(%314) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %316 = \"tf.AddV2\"(%298, %315) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %317 = \"tf.Conv2D\"(%316, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %318 = \"tfl.cast\"(%317) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %319 = \"tfl.mul\"(%318, %144) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %320 = \"tfl.add\"(%319, %145) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %321 = \"tfl.cast\"(%320) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %322 = \"tf.Relu6\"(%321) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %323 = \"tf.DepthwiseConv2dNative\"(%322, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %324 = \"tfl.cast\"(%323) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %325 = \"tfl.mul\"(%324, %141) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %326 = \"tfl.add\"(%325, %142) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %327 = \"tfl.cast\"(%326) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %328 = \"tf.Relu6\"(%327) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %329 = \"tf.Conv2D\"(%328, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %330 = \"tfl.cast\"(%329) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %331 = \"tfl.mul\"(%330, %147) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %332 = \"tfl.add\"(%331, %148) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %333 = \"tfl.cast\"(%332) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %334 = \"tf.AddV2\"(%316, %333) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %335 = \"tf.Conv2D\"(%334, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %336 = \"tfl.cast\"(%335) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %337 = \"tfl.mul\"(%336, %153) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %338 = \"tfl.add\"(%337, %154) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %339 = \"tfl.cast\"(%338) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %340 = \"tf.Relu6\"(%339) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %341 = \"tf.DepthwiseConv2dNative\"(%340, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %342 = \"tfl.cast\"(%341) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %343 = \"tfl.mul\"(%342, %150) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %344 = \"tfl.add\"(%343, %151) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %345 = \"tfl.cast\"(%344) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %346 = \"tf.Relu6\"(%345) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %347 = \"tf.Conv2D\"(%346, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %348 = \"tfl.cast\"(%347) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %349 = \"tfl.mul\"(%348, %156) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %350 = \"tfl.add\"(%349, %157) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %351 = \"tfl.cast\"(%350) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %352 = \"tf.AddV2\"(%334, %351) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %353 = \"tf.Conv2D\"(%352, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %354 = \"tfl.cast\"(%353) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %355 = \"tfl.mul\"(%354, %18) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %356 = \"tfl.add\"(%355, %19) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %357 = \"tfl.cast\"(%356) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %358 = \"tf.Relu6\"(%357) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %359 = \"tf.DepthwiseConv2dNative\"(%358, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %360 = \"tfl.cast\"(%359) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %361 = \"tfl.mul\"(%360, %15) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %362 = \"tfl.add\"(%361, %16) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %363 = \"tfl.cast\"(%362) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %364 = \"tf.Relu6\"(%363) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %365 = \"tf.Conv2D\"(%364, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x3x3x96xf16>\n  %366 = \"tfl.cast\"(%365) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %367 = \"tfl.mul\"(%366, %21) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %368 = \"tfl.add\"(%367, %22) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %369 = \"tfl.cast\"(%368) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %370 = \"tf.Conv2D\"(%369, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %371 = \"tfl.cast\"(%370) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %372 = \"tfl.mul\"(%371, %27) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %373 = \"tfl.add\"(%372, %28) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %374 = \"tfl.cast\"(%373) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %375 = \"tf.Relu6\"(%374) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %376 = \"tf.DepthwiseConv2dNative\"(%375, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n  %377 = \"tfl.cast\"(%376) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %378 = \"tfl.mul\"(%377, %24) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %379 = \"tfl.add\"(%378, %25) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %380 = \"tfl.cast\"(%379) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %381 = \"tf.Relu6\"(%380) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %382 = \"tf.Conv2D\"(%381, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n  %383 = \"tfl.cast\"(%382) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %384 = \"tfl.mul\"(%383, %30) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %385 = \"tfl.add\"(%384, %31) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %386 = \"tfl.cast\"(%385) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %387 = \"tf.AddV2\"(%369, %386) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n  %388 = \"tf.Conv2D\"(%387, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %389 = \"tfl.cast\"(%388) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %390 = \"tfl.mul\"(%389, %36) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %391 = \"tfl.add\"(%390, %37) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %392 = \"tfl.cast\"(%391) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %393 = \"tf.Relu6\"(%392) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %394 = \"tf.DepthwiseConv2dNative\"(%393, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n  %395 = \"tfl.cast\"(%394) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %396 = \"tfl.mul\"(%395, %33) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %397 = \"tfl.add\"(%396, %34) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %398 = \"tfl.cast\"(%397) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %399 = \"tf.Relu6\"(%398) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %400 = \"tf.Conv2D\"(%399, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n  %401 = \"tfl.cast\"(%400) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %402 = \"tfl.mul\"(%401, %39) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %403 = \"tfl.add\"(%402, %40) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %404 = \"tfl.cast\"(%403) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %405 = \"tf.AddV2\"(%387, %404) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n  %406 = \"tf.Conv2D\"(%405, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %407 = \"tfl.cast\"(%406) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %408 = \"tfl.mul\"(%407, %45) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %409 = \"tfl.add\"(%408, %46) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %410 = \"tfl.cast\"(%409) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %411 = \"tf.Relu6\"(%410) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %412 = \"tf.Pad\"(%411, %6) {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> tensor<?x5x5x576xf16>\n  %413 = \"tf.DepthwiseConv2dNative\"(%412, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x2x2x576xf16>\n  %414 = \"tfl.cast\"(%413) : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf32>\n  %415 = \"tfl.mul\"(%414, %42) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x576xf32>, tensor<576xf32>) -> tensor<?x2x2x576xf32>\n  %416 = \"tfl.add\"(%415, %43) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x576xf32>, tensor<576xf32>) -> tensor<?x2x2x576xf32>\n  %417 = \"tfl.cast\"(%416) : (tensor<?x2x2x576xf32>) -> tensor<?x2x2x576xf16>\n  %418 = \"tf.Relu6\"(%417) {device = \"\"} : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf16>\n  %419 = \"tf.Conv2D\"(%418, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x2x2x160xf16>\n  %420 = \"tfl.cast\"(%419) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %421 = \"tfl.mul\"(%420, %48) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %422 = \"tfl.add\"(%421, %49) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %423 = \"tfl.cast\"(%422) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %424 = \"tf.Conv2D\"(%423, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %425 = \"tfl.cast\"(%424) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %426 = \"tfl.mul\"(%425, %54) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %427 = \"tfl.add\"(%426, %55) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %428 = \"tfl.cast\"(%427) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %429 = \"tf.Relu6\"(%428) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %430 = \"tf.DepthwiseConv2dNative\"(%429, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %431 = \"tfl.cast\"(%430) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %432 = \"tfl.mul\"(%431, %51) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %433 = \"tfl.add\"(%432, %52) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %434 = \"tfl.cast\"(%433) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %435 = \"tf.Relu6\"(%434) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %436 = \"tf.Conv2D\"(%435, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n  %437 = \"tfl.cast\"(%436) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %438 = \"tfl.mul\"(%437, %57) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %439 = \"tfl.add\"(%438, %58) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %440 = \"tfl.cast\"(%439) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %441 = \"tf.AddV2\"(%423, %440) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n  %442 = \"tf.Conv2D\"(%441, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %443 = \"tfl.cast\"(%442) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %444 = \"tfl.mul\"(%443, %63) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %445 = \"tfl.add\"(%444, %64) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %446 = \"tfl.cast\"(%445) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %447 = \"tf.Relu6\"(%446) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %448 = \"tf.DepthwiseConv2dNative\"(%447, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %449 = \"tfl.cast\"(%448) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %450 = \"tfl.mul\"(%449, %60) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %451 = \"tfl.add\"(%450, %61) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %452 = \"tfl.cast\"(%451) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %453 = \"tf.Relu6\"(%452) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %454 = \"tf.Conv2D\"(%453, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n  %455 = \"tfl.cast\"(%454) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %456 = \"tfl.mul\"(%455, %66) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %457 = \"tfl.add\"(%456, %67) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %458 = \"tfl.cast\"(%457) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %459 = \"tf.AddV2\"(%441, %458) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n  %460 = \"tf.Conv2D\"(%459, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %461 = \"tfl.cast\"(%460) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %462 = \"tfl.mul\"(%461, %72) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %463 = \"tfl.add\"(%462, %73) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %464 = \"tfl.cast\"(%463) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %465 = \"tf.Relu6\"(%464) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %466 = \"tf.DepthwiseConv2dNative\"(%465, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %467 = \"tfl.cast\"(%466) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %468 = \"tfl.mul\"(%467, %69) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %469 = \"tfl.add\"(%468, %70) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %470 = \"tfl.cast\"(%469) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %471 = \"tf.Relu6\"(%470) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %472 = \"tf.Conv2D\"(%471, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x2x2x320xf16>\n  %473 = \"tfl.cast\"(%472) : (tensor<?x2x2x320xf16>) -> tensor<?x2x2x320xf32>\n  %474 = \"tfl.mul\"(%473, %75) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x320xf32>, tensor<320xf32>) -> tensor<?x2x2x320xf32>\n  %475 = \"tfl.add\"(%474, %76) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x320xf32>, tensor<320xf32>) -> tensor<?x2x2x320xf32>\n  %476 = \"tfl.cast\"(%475) : (tensor<?x2x2x320xf32>) -> tensor<?x2x2x320xf16>\n  %477 = \"tf.Conv2D\"(%476, %161) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x2x2x1280xf16>\n  %478 = \"tfl.cast\"(%477) : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf32>\n  %479 = \"tfl.mul\"(%478, %162) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x1280xf32>, tensor<1280xf32>) -> tensor<?x2x2x1280xf32>\n  %480 = \"tfl.add\"(%479, %163) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x1280xf32>, tensor<1280xf32>) -> tensor<?x2x2x1280xf32>\n  %481 = \"tfl.cast\"(%480) : (tensor<?x2x2x1280xf32>) -> tensor<?x2x2x1280xf16>\n  %482 = \"tf.Relu6\"(%481) {device = \"\"} : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf16>\n  %483 = \"tfl.cast\"(%482) : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf32>\n  %484 = \"tfl.mean\"(%483, %5) <{keep_dims = false}> : (tensor<?x2x2x1280xf32>, tensor<2xi32>) -> tensor<?x1280xf32>\n  %485 = \"tfl.cast\"(%484) : (tensor<?x1280xf32>) -> tensor<?x1280xf16>\n  %486 = \"tf.MatMul\"(%485, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<256x1280xf16>) -> tensor<?x256xf16>\n  %487 = \"tf.BiasAdd\"(%486, %13) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x256xf16>, tensor<256xf16>) -> tensor<?x256xf16>\n  %488 = \"tf.Relu\"(%487) {device = \"\"} : (tensor<?x256xf16>) -> tensor<?x256xf16>\n  %489 = \"tfl.cast\"(%488) : (tensor<?x256xf16>) -> tensor<?x256xf32>\n  %490 = \"tfl.mul\"(%489, %10) <{fused_activation_function = \"NONE\"}> : (tensor<?x256xf32>, tensor<256xf32>) -> tensor<?x256xf32>\n  %491 = \"tfl.add\"(%490, %11) <{fused_activation_function = \"NONE\"}> : (tensor<?x256xf32>, tensor<256xf32>) -> tensor<?x256xf32>\n  %492 = \"tfl.cast\"(%491) : (tensor<?x256xf32>) -> tensor<?x256xf16>\n  %493 = \"tf.MatMul\"(%492, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x256xf16>, tensor<128x256xf16>) -> tensor<?x128xf16>\n  %494 = \"tf.BiasAdd\"(%493, %12) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x128xf16>, tensor<128xf16>) -> tensor<?x128xf16>\n  %495 = \"tf.Relu\"(%494) {device = \"\"} : (tensor<?x128xf16>) -> tensor<?x128xf16>\n  %496 = \"tfl.cast\"(%495) : (tensor<?x128xf16>) -> tensor<?x128xf32>\n  %497 = \"tfl.mul\"(%496, %8) <{fused_activation_function = \"NONE\"}> : (tensor<?x128xf32>, tensor<128xf32>) -> tensor<?x128xf32>\n  %498 = \"tfl.add\"(%497, %9) <{fused_activation_function = \"NONE\"}> : (tensor<?x128xf32>, tensor<128xf32>) -> tensor<?x128xf32>\n  %499 = \"tfl.cast\"(%498) : (tensor<?x128xf32>) -> tensor<?x128xf16>\n  %500 = \"tfl.cast\"(%499) : (tensor<?x128xf16>) -> tensor<?x128xf32>\n  %501 = \"tfl.fully_connected\"(%500, %0, %3) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<?x128xf32>, tensor<7x128xf32>, tensor<7xf32>) -> tensor<?x7xf32>\n  %502 = \"tfl.softmax\"(%501) <{beta = 1.000000e+00 : f32}> : (tensor<?x7xf32>) -> tensor<?x7xf32>\n  \"func.return\"(%502) : (tensor<?x7xf32>) -> ()\n}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_keras_tensor_155:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConverterError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m         f.write(tfl)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… INT8 TFLite saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tfl)/\u001b[32m1024\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m KB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mconvert_to_tflite_int8\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memotion_model_int8.tflite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 2) TFLite FP16 (good balance)\u001b[39;00m\n\u001b[32m     34\u001b[39m conv = tf.lite.TFLiteConverter.from_keras_model(model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mconvert_to_tflite_int8\u001b[39m\u001b[34m(model, train_generator, output_path, reps)\u001b[39m\n\u001b[32m     24\u001b[39m conv.inference_input_type = tf.uint8\n\u001b[32m     25\u001b[39m conv.inference_output_type = tf.uint8\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m tfl = \u001b[43mconv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     28\u001b[39m     f.write(tfl)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1250\u001b[39m, in \u001b[36m_export_metrics.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1247\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(convert_func)\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1249\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_and_export_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1202\u001b[39m, in \u001b[36mTFLiteConverterBase._convert_and_export_metrics\u001b[39m\u001b[34m(self, convert_func, *args, **kwargs)\u001b[39m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28mself\u001b[39m._save_conversion_params_metric()\n\u001b[32m   1201\u001b[39m start_time = time.process_time()\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m result = \u001b[43mconvert_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1203\u001b[39m elapsed_time_ms = (time.process_time() - start_time) * \u001b[32m1000\u001b[39m\n\u001b[32m   1204\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1768\u001b[39m, in \u001b[36mTFLiteKerasModelConverterV2.convert\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1755\u001b[39m \u001b[38;5;129m@_export_metrics\u001b[39m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1757\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[32m   1758\u001b[39m \n\u001b[32m   1759\u001b[39m \u001b[33;03m  Returns:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1766\u001b[39m \u001b[33;03m      Invalid quantization parameters.\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1768\u001b[39m   saved_model_convert_result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_as_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1769\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m saved_model_convert_result:\n\u001b[32m   1770\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_convert_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1749\u001b[39m, in \u001b[36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1745\u001b[39m   graph_def, input_tensors, output_tensors = (\n\u001b[32m   1746\u001b[39m       \u001b[38;5;28mself\u001b[39m._convert_keras_to_saved_model(temp_dir)\n\u001b[32m   1747\u001b[39m   )\n\u001b[32m   1748\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.saved_model_dir:\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTFLiteKerasModelConverterV2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensors\u001b[49m\n\u001b[32m   1751\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1753\u001b[39m   shutil.rmtree(temp_dir, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1487\u001b[39m, in \u001b[36mTFLiteConverterBaseV2.convert\u001b[39m\u001b[34m(self, graph_def, input_tensors, output_tensors)\u001b[39m\n\u001b[32m   1480\u001b[39m   logging.info(\n\u001b[32m   1481\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUsing new converter: If you encounter a problem \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1482\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mplease file a bug. You can opt-out \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1483\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mby setting experimental_new_converter=False\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1484\u001b[39m   )\n\u001b[32m   1486\u001b[39m \u001b[38;5;66;03m# Converts model.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1487\u001b[39m result = \u001b[43m_convert_graphdef\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1488\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgraph_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconverter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._optimize_tflite_model(\n\u001b[32m   1495\u001b[39m     result,\n\u001b[32m   1496\u001b[39m     \u001b[38;5;28mself\u001b[39m._quant_mode,\n\u001b[32m   1497\u001b[39m     _build_conversion_flags(**converter_kwargs).debug_options,\n\u001b[32m   1498\u001b[39m     quant_io=\u001b[38;5;28mself\u001b[39m.experimental_new_quantizer,\n\u001b[32m   1499\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:212\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    211\u001b[39m     report_error_message(\u001b[38;5;28mstr\u001b[39m(converter_error))\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m converter_error \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Re-throws the exception.\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m    214\u001b[39m   report_error_message(\u001b[38;5;28mstr\u001b[39m(error))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py:205\u001b[39m, in \u001b[36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    204\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m ConverterError \u001b[38;5;28;01mas\u001b[39;00m converter_error:\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m converter_error.errors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:885\u001b[39m, in \u001b[36mconvert_graphdef\u001b[39m\u001b[34m(input_data, input_tensors, output_tensors, **kwargs)\u001b[39m\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    883\u001b[39m     model_flags.output_arrays.append(util.get_tensor_name(output_tensor))\n\u001b[32m--> \u001b[39m\u001b[32m885\u001b[39m data = \u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconversion_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug_info_str\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdebug_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marty\\anaconda3\\envs\\tf311_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:350\u001b[39m, in \u001b[36mconvert\u001b[39m\u001b[34m(model_flags, conversion_flags, input_data_str, debug_info_str)\u001b[39m\n\u001b[32m    343\u001b[39m     conversion_flags.guarantee_all_funcs_one_use = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert(\n\u001b[32m    345\u001b[39m         model_flags,\n\u001b[32m    346\u001b[39m         conversion_flags,\n\u001b[32m    347\u001b[39m         input_data_str,\n\u001b[32m    348\u001b[39m         debug_info_str,\n\u001b[32m    349\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m converter_error\n",
      "\u001b[31mConverterError\u001b[39m: Could not translate MLIR to FlatBuffer.<unknown>:0: error: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.ConcatV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %171 = \"tf.ConcatV2\"(%170, %170, %170, %4) {device = \"\"} : (tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> tensor<?x48x48x3xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"ConcatV2:\", \"functional_1/concatenate_1/concat@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %174 = \"tf.Conv2D\"(%173, %160) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %179 = \"tf.Relu6\"(%178) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/Conv1_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %180 = \"tf.DepthwiseConv2dNative\"(%179, %164) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %185 = \"tf.Relu6\"(%184) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %186 = \"tf.Conv2D\"(%185, %167) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x24x24x16xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/expanded_conv_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %191 = \"tf.Conv2D\"(%190, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x24x24x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %196 = \"tf.Relu6\"(%195) {device = \"\"} : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %197 = \"tf.Pad\"(%196, %7) {device = \"\"} : (tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> tensor<?x25x25x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %198 = \"tf.DepthwiseConv2dNative\"(%197, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x12x12x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %203 = \"tf.Relu6\"(%202) {device = \"\"} : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %204 = \"tf.Conv2D\"(%203, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_1_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %209 = \"tf.Conv2D\"(%208, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %214 = \"tf.Relu6\"(%213) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %215 = \"tf.DepthwiseConv2dNative\"(%214, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %220 = \"tf.Relu6\"(%219) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %221 = \"tf.Conv2D\"(%220, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %226 = \"tf.AddV2\"(%208, %225) {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_2_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %227 = \"tf.Conv2D\"(%226, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %232 = \"tf.Relu6\"(%231) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %233 = \"tf.Pad\"(%232, %7) {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> tensor<?x13x13x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %234 = \"tf.DepthwiseConv2dNative\"(%233, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x6x6x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %239 = \"tf.Relu6\"(%238) {device = \"\"} : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %240 = \"tf.Conv2D\"(%239, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_3_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %245 = \"tf.Conv2D\"(%244, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %250 = \"tf.Relu6\"(%249) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %251 = \"tf.DepthwiseConv2dNative\"(%250, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %256 = \"tf.Relu6\"(%255) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %257 = \"tf.Conv2D\"(%256, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %262 = \"tf.AddV2\"(%244, %261) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_4_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %263 = \"tf.Conv2D\"(%262, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %268 = \"tf.Relu6\"(%267) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %269 = \"tf.DepthwiseConv2dNative\"(%268, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %274 = \"tf.Relu6\"(%273) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %275 = \"tf.Conv2D\"(%274, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %280 = \"tf.AddV2\"(%262, %279) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_5_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %281 = \"tf.Conv2D\"(%280, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %286 = \"tf.Relu6\"(%285) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %287 = \"tf.Pad\"(%286, %7) {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> tensor<?x7x7x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %288 = \"tf.DepthwiseConv2dNative\"(%287, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x3x3x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %293 = \"tf.Relu6\"(%292) {device = \"\"} : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %294 = \"tf.Conv2D\"(%293, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_6_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %299 = \"tf.Conv2D\"(%298, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %304 = \"tf.Relu6\"(%303) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %305 = \"tf.DepthwiseConv2dNative\"(%304, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %310 = \"tf.Relu6\"(%309) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %311 = \"tf.Conv2D\"(%310, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %316 = \"tf.AddV2\"(%298, %315) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_7_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %317 = \"tf.Conv2D\"(%316, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %322 = \"tf.Relu6\"(%321) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %323 = \"tf.DepthwiseConv2dNative\"(%322, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %328 = \"tf.Relu6\"(%327) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %329 = \"tf.Conv2D\"(%328, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %334 = \"tf.AddV2\"(%316, %333) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_8_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %335 = \"tf.Conv2D\"(%334, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %340 = \"tf.Relu6\"(%339) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %341 = \"tf.DepthwiseConv2dNative\"(%340, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %346 = \"tf.Relu6\"(%345) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %347 = \"tf.Conv2D\"(%346, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %352 = \"tf.AddV2\"(%334, %351) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_9_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %353 = \"tf.Conv2D\"(%352, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %358 = \"tf.Relu6\"(%357) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %359 = \"tf.DepthwiseConv2dNative\"(%358, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %364 = \"tf.Relu6\"(%363) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %365 = \"tf.Conv2D\"(%364, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_10_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %370 = \"tf.Conv2D\"(%369, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %375 = \"tf.Relu6\"(%374) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %376 = \"tf.DepthwiseConv2dNative\"(%375, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %381 = \"tf.Relu6\"(%380) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %382 = \"tf.Conv2D\"(%381, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %387 = \"tf.AddV2\"(%369, %386) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_11_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %388 = \"tf.Conv2D\"(%387, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %393 = \"tf.Relu6\"(%392) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %394 = \"tf.DepthwiseConv2dNative\"(%393, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %399 = \"tf.Relu6\"(%398) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %400 = \"tf.Conv2D\"(%399, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %405 = \"tf.AddV2\"(%387, %404) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_12_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %406 = \"tf.Conv2D\"(%405, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %411 = \"tf.Relu6\"(%410) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Pad' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %412 = \"tf.Pad\"(%411, %6) {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> tensor<?x5x5x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Pad:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_pad_1/Pad@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %413 = \"tf.DepthwiseConv2dNative\"(%412, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x2x2x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %418 = \"tf.Relu6\"(%417) {device = \"\"} : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %419 = \"tf.Conv2D\"(%418, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_13_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %424 = \"tf.Conv2D\"(%423, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %429 = \"tf.Relu6\"(%428) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %430 = \"tf.DepthwiseConv2dNative\"(%429, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %435 = \"tf.Relu6\"(%434) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %436 = \"tf.Conv2D\"(%435, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %441 = \"tf.AddV2\"(%423, %440) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_14_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %442 = \"tf.Conv2D\"(%441, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %447 = \"tf.Relu6\"(%446) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %448 = \"tf.DepthwiseConv2dNative\"(%447, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %453 = \"tf.Relu6\"(%452) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %454 = \"tf.Conv2D\"(%453, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.AddV2' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %459 = \"tf.AddV2\"(%441, %458) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"AddV2:\", \"functional_1/mobilenetv2_1.00_224_1/block_15_add_1/Add@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %460 = \"tf.Conv2D\"(%459, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %465 = \"tf.Relu6\"(%464) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_expand_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.DepthwiseConv2dNative' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %466 = \"tf.DepthwiseConv2dNative\"(%465, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"DepthwiseConv2dNative:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_1/depthwise@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %471 = \"tf.Relu6\"(%470) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_depthwise_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %472 = \"tf.Conv2D\"(%471, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x2x2x320xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/block_16_project_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Conv2D' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %477 = \"tf.Conv2D\"(%476, %161) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x2x2x1280xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Conv2D:\", \"functional_1/mobilenetv2_1.00_224_1/Conv_1_1/convolution@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu6' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %482 = \"tf.Relu6\"(%481) {device = \"\"} : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu6:\", \"functional_1/mobilenetv2_1.00_224_1/out_relu_1/Relu6@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %486 = \"tf.MatMul\"(%485, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<256x1280xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %487 = \"tf.BiasAdd\"(%486, %13) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x256xf16>, tensor<256xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %488 = \"tf.Relu\"(%487) {device = \"\"} : (tensor<?x256xf16>) -> tensor<?x256xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.MatMul' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %493 = \"tf.MatMul\"(%492, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x256xf16>, tensor<128x256xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"MatMul:\", \"functional_1/dense_1_2/MatMul@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.BiasAdd' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %494 = \"tf.BiasAdd\"(%493, %12) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x128xf16>, tensor<128xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"BiasAdd:\", \"functional_1/dense_1_2/BiasAdd@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): 'tf.Relu' op is neither a custom op nor a flex op\n<unknown>:0: note: loc(fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"]): called from\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): see current operation: %495 = \"tf.Relu\"(%494) {device = \"\"} : (tensor<?x128xf16>) -> tensor<?x128xf16>\n<unknown>:0: note: loc(callsite(callsite(fused[\"Relu:\", \"functional_1/dense_1_2/Relu@__inference_function_104153\"] at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall@__inference_signature_wrapper_105260\"]) at fused[\"StatefulPartitionedCall:\", \"StatefulPartitionedCall_1\"])): Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: AddV2, BiasAdd, ConcatV2, Conv2D, DepthwiseConv2dNative, MatMul, Pad, Relu, Relu6\nDetails:\n\ttf.AddV2(tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> (tensor<?x12x12x24xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> (tensor<?x2x2x160xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> (tensor<?x3x3x64xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> (tensor<?x3x3x96xf16>) : {device = \"\"}\n\ttf.AddV2(tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> (tensor<?x6x6x32xf16>) : {device = \"\"}\n\ttf.BiasAdd(tensor<?x128xf16>, tensor<128xf16>) -> (tensor<?x128xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.BiasAdd(tensor<?x256xf16>, tensor<256xf16>) -> (tensor<?x256xf16>) : {data_format = \"NHWC\", device = \"\"}\n\ttf.ConcatV2(tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> (tensor<?x48x48x3xf16>) : {device = \"\"}\n\ttf.Conv2D(tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> (tensor<?x12x12x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> (tensor<?x12x12x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> (tensor<?x12x12x24xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> (tensor<?x24x24x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> (tensor<?x24x24x16xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> (tensor<?x2x2x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> (tensor<?x2x2x1280xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> (tensor<?x2x2x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> (tensor<?x2x2x160xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> (tensor<?x2x2x320xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> (tensor<?x3x3x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> (tensor<?x3x3x64xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> (tensor<?x3x3x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> (tensor<?x3x3x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> (tensor<?x3x3x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> (tensor<?x3x3x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> (tensor<?x24x24x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> (tensor<?x6x6x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> (tensor<?x6x6x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.Conv2D(tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> (tensor<?x6x6x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n\ttf.DepthwiseConv2dNative(tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x12x12x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> (tensor<?x6x6x144xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> (tensor<?x24x24x32xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> (tensor<?x12x12x96xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> (tensor<?x2x2x960xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> (tensor<?x3x3x384xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x3x3x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> (tensor<?x2x2x576xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x6x6x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}\n\ttf.DepthwiseConv2dNative(tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> (tensor<?x3x3x192xf16>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}\n\ttf.MatMul(tensor<?x1280xf16>, tensor<256x1280xf16>) -> (tensor<?x256xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n\ttf.MatMul(tensor<?x256xf16>, tensor<128x256xf16>) -> (tensor<?x128xf16>) : {grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}\n\ttf.Pad(tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> (tensor<?x13x13x144xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> (tensor<?x25x25x96xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> (tensor<?x5x5x576xf16>) : {device = \"\"}\n\ttf.Pad(tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> (tensor<?x7x7x192xf16>) : {device = \"\"}\n\ttf.Relu(tensor<?x128xf16>) -> (tensor<?x128xf16>) : {device = \"\"}\n\ttf.Relu(tensor<?x256xf16>) -> (tensor<?x256xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x12x12x144xf16>) -> (tensor<?x12x12x144xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x12x12x96xf16>) -> (tensor<?x12x12x96xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x24x24x32xf16>) -> (tensor<?x24x24x32xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x24x24x96xf16>) -> (tensor<?x24x24x96xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x1280xf16>) -> (tensor<?x2x2x1280xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x576xf16>) -> (tensor<?x2x2x576xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x2x2x960xf16>) -> (tensor<?x2x2x960xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x192xf16>) -> (tensor<?x3x3x192xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x384xf16>) -> (tensor<?x3x3x384xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x3x3x576xf16>) -> (tensor<?x3x3x576xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x6x6x144xf16>) -> (tensor<?x6x6x144xf16>) : {device = \"\"}\n\ttf.Relu6(tensor<?x6x6x192xf16>) -> (tensor<?x6x6x192xf16>) : {device = \"\"}\n\n<unknown>:0: note: see current operation: \n\"func.func\"() <{arg_attrs = [{tf_saved_model.index_path = [\"keras_tensor_155\"]}], function_type = (tensor<?x48x48x1xf32>) -> tensor<?x7xf32>, res_attrs = [{tf_saved_model.index_path = [\"output_0\"]}], sym_name = \"main\"}> ({\n^bb0(%arg0: tensor<?x48x48x1xf32>):\n  %0 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<7x128xf32>}> : () -> tensor<7x128xf32>\n  %1 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128x256xf16>}> : () -> tensor<128x256xf16>\n  %2 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256x1280xf16>}> : () -> tensor<256x1280xf16>\n  %3 = \"arith.constant\"() <{value = dense<[0.0566285178, -0.224917859, 0.0279344376, 0.137221545, 0.115658529, 0.0914542153, -0.118997455]> : tensor<7xf32>}> : () -> tensor<7xf32>\n  %4 = \"arith.constant\"() <{value = dense<-1> : tensor<i32>}> : () -> tensor<i32>\n  %5 = \"arith.constant\"() <{value = dense<[1, 2]> : tensor<2xi32>}> : () -> tensor<2xi32>\n  %6 = \"arith.constant\"() <{value = dense<[[0, 0], [1, 1], [1, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n  %7 = \"arith.constant\"() <{value = dense<[[0, 0], [0, 1], [0, 1], [0, 0]]> : tensor<4x2xi32>}> : () -> tensor<4x2xi32>\n  %8 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf32>}> : () -> tensor<128xf32>\n  %9 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf32>}> : () -> tensor<128xf32>\n  %10 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf32>}> : () -> tensor<256xf32>\n  %11 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf32>}> : () -> tensor<256xf32>\n  %12 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<128xf16>}> : () -> tensor<128xf16>\n  %13 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<256xf16>}> : () -> tensor<256xf16>\n  %14 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %15 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %16 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %17 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %18 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %19 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %20 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x96xf16>}> : () -> tensor<1x1x384x96xf16>\n  %21 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %22 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %23 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %24 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %25 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %26 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %27 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %28 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %29 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n  %30 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %31 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %32 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %33 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %34 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %35 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %36 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %37 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %38 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x96xf16>}> : () -> tensor<1x1x576x96xf16>\n  %39 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %40 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %41 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x576x1xf16>}> : () -> tensor<3x3x576x1xf16>\n  %42 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %43 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %44 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x576xf16>}> : () -> tensor<1x1x96x576xf16>\n  %45 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %46 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<576xf32>}> : () -> tensor<576xf32>\n  %47 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x576x160xf16>}> : () -> tensor<1x1x576x160xf16>\n  %48 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %49 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %50 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %51 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %52 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %53 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %54 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %55 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %56 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n  %57 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %58 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %59 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %60 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %61 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %62 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %63 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %64 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %65 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x160xf16>}> : () -> tensor<1x1x960x160xf16>\n  %66 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %67 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<160xf32>}> : () -> tensor<160xf32>\n  %68 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x960x1xf16>}> : () -> tensor<3x3x960x1xf16>\n  %69 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %70 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %71 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x160x960xf16>}> : () -> tensor<1x1x160x960xf16>\n  %72 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %73 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<960xf32>}> : () -> tensor<960xf32>\n  %74 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x960x320xf16>}> : () -> tensor<1x1x960x320xf16>\n  %75 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n  %76 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<320xf32>}> : () -> tensor<320xf32>\n  %77 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x96x1xf16>}> : () -> tensor<3x3x96x1xf16>\n  %78 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %79 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %80 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x16x96xf16>}> : () -> tensor<1x1x16x96xf16>\n  %81 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %82 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<96xf32>}> : () -> tensor<96xf32>\n  %83 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x96x24xf16>}> : () -> tensor<1x1x96x24xf16>\n  %84 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %85 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %86 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n  %87 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %88 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %89 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n  %90 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %91 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %92 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x24xf16>}> : () -> tensor<1x1x144x24xf16>\n  %93 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %94 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<24xf32>}> : () -> tensor<24xf32>\n  %95 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x144x1xf16>}> : () -> tensor<3x3x144x1xf16>\n  %96 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %97 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %98 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x24x144xf16>}> : () -> tensor<1x1x24x144xf16>\n  %99 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %100 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<144xf32>}> : () -> tensor<144xf32>\n  %101 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x144x32xf16>}> : () -> tensor<1x1x144x32xf16>\n  %102 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %103 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %104 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %105 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %106 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %107 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %108 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %109 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %110 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n  %111 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %112 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %113 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %114 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %115 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %116 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %117 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %118 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %119 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x32xf16>}> : () -> tensor<1x1x192x32xf16>\n  %120 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %121 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %122 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x192x1xf16>}> : () -> tensor<3x3x192x1xf16>\n  %123 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %124 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %125 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x192xf16>}> : () -> tensor<1x1x32x192xf16>\n  %126 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %127 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<192xf32>}> : () -> tensor<192xf32>\n  %128 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x192x64xf16>}> : () -> tensor<1x1x192x64xf16>\n  %129 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %130 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %131 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %132 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %133 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %134 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %135 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %136 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %137 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %138 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %139 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %140 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %141 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %142 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %143 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %144 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %145 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %146 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %147 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %148 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %149 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x384x1xf16>}> : () -> tensor<3x3x384x1xf16>\n  %150 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %151 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %152 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x64x384xf16>}> : () -> tensor<1x1x64x384xf16>\n  %153 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %154 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<384xf32>}> : () -> tensor<384xf32>\n  %155 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x384x64xf16>}> : () -> tensor<1x1x384x64xf16>\n  %156 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %157 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<64xf32>}> : () -> tensor<64xf32>\n  %158 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %159 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %160 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x3x32xf16>}> : () -> tensor<3x3x3x32xf16>\n  %161 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x320x1280xf16>}> : () -> tensor<1x1x320x1280xf16>\n  %162 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n  %163 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1280xf32>}> : () -> tensor<1280xf32>\n  %164 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<3x3x32x1xf16>}> : () -> tensor<3x3x32x1xf16>\n  %165 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %166 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<32xf32>}> : () -> tensor<32xf32>\n  %167 = \"arith.constant\"() <{value = dense_resource<__elided__> : tensor<1x1x32x16xf16>}> : () -> tensor<1x1x32x16xf16>\n  %168 = \"arith.constant\"() <{value = dense<[4.83980179, 7.01497889, 6.59778308, 7.4643445, 3.82265639, 6.78915548, 5.1570487, 4.63491488, 4.46355867, 6.7842741, 7.31219244, 4.33215332, 6.80388832, 6.5691595, 5.91440105, 4.81517363]> : tensor<16xf32>}> : () -> tensor<16xf32>\n  %169 = \"arith.constant\"() <{value = dense<[-1.79617035, 3.17504382, 16.2980614, 24.0359573, 13.8513613, 3.49967337, -0.0168914795, 7.31451178, -13.8333397, 3.81765318, -2.79449153, 18.9408112, -18.930912, -11.2135153, 7.71239185, -2.86968422]> : tensor<16xf32>}> : () -> tensor<16xf32>\n  %170 = \"tfl.cast\"(%arg0) : (tensor<?x48x48x1xf32>) -> tensor<?x48x48x1xf16>\n  %171 = \"tf.ConcatV2\"(%170, %170, %170, %4) {device = \"\"} : (tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<?x48x48x1xf16>, tensor<i32>) -> tensor<?x48x48x3xf16>\n  %172 = \"tfl.cast\"(%171) : (tensor<?x48x48x3xf16>) -> tensor<?x48x48x3xf32>\n  %173 = \"tfl.cast\"(%172) : (tensor<?x48x48x3xf32>) -> tensor<?x48x48x3xf16>\n  %174 = \"tf.Conv2D\"(%173, %160) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 2, 2, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x48x48x3xf16>, tensor<3x3x3x32xf16>) -> tensor<?x24x24x32xf16>\n  %175 = \"tfl.cast\"(%174) : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf32>\n  %176 = \"tfl.mul\"(%175, %158) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %177 = \"tfl.add\"(%176, %159) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %178 = \"tfl.cast\"(%177) : (tensor<?x24x24x32xf32>) -> tensor<?x24x24x32xf16>\n  %179 = \"tf.Relu6\"(%178) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n  %180 = \"tf.DepthwiseConv2dNative\"(%179, %164) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<3x3x32x1xf16>) -> tensor<?x24x24x32xf16>\n  %181 = \"tfl.cast\"(%180) : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf32>\n  %182 = \"tfl.mul\"(%181, %165) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %183 = \"tfl.add\"(%182, %166) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x32xf32>, tensor<32xf32>) -> tensor<?x24x24x32xf32>\n  %184 = \"tfl.cast\"(%183) : (tensor<?x24x24x32xf32>) -> tensor<?x24x24x32xf16>\n  %185 = \"tf.Relu6\"(%184) {device = \"\"} : (tensor<?x24x24x32xf16>) -> tensor<?x24x24x32xf16>\n  %186 = \"tf.Conv2D\"(%185, %167) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x32xf16>, tensor<1x1x32x16xf16>) -> tensor<?x24x24x16xf16>\n  %187 = \"tfl.cast\"(%186) : (tensor<?x24x24x16xf16>) -> tensor<?x24x24x16xf32>\n  %188 = \"tfl.mul\"(%187, %168) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x16xf32>, tensor<16xf32>) -> tensor<?x24x24x16xf32>\n  %189 = \"tfl.add\"(%188, %169) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x16xf32>, tensor<16xf32>) -> tensor<?x24x24x16xf32>\n  %190 = \"tfl.cast\"(%189) : (tensor<?x24x24x16xf32>) -> tensor<?x24x24x16xf16>\n  %191 = \"tf.Conv2D\"(%190, %80) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x24x24x16xf16>, tensor<1x1x16x96xf16>) -> tensor<?x24x24x96xf16>\n  %192 = \"tfl.cast\"(%191) : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf32>\n  %193 = \"tfl.mul\"(%192, %81) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x96xf32>, tensor<96xf32>) -> tensor<?x24x24x96xf32>\n  %194 = \"tfl.add\"(%193, %82) <{fused_activation_function = \"NONE\"}> : (tensor<?x24x24x96xf32>, tensor<96xf32>) -> tensor<?x24x24x96xf32>\n  %195 = \"tfl.cast\"(%194) : (tensor<?x24x24x96xf32>) -> tensor<?x24x24x96xf16>\n  %196 = \"tf.Relu6\"(%195) {device = \"\"} : (tensor<?x24x24x96xf16>) -> tensor<?x24x24x96xf16>\n  %197 = \"tf.Pad\"(%196, %7) {device = \"\"} : (tensor<?x24x24x96xf16>, tensor<4x2xi32>) -> tensor<?x25x25x96xf16>\n  %198 = \"tf.DepthwiseConv2dNative\"(%197, %77) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x25x25x96xf16>, tensor<3x3x96x1xf16>) -> tensor<?x12x12x96xf16>\n  %199 = \"tfl.cast\"(%198) : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf32>\n  %200 = \"tfl.mul\"(%199, %78) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x96xf32>, tensor<96xf32>) -> tensor<?x12x12x96xf32>\n  %201 = \"tfl.add\"(%200, %79) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x96xf32>, tensor<96xf32>) -> tensor<?x12x12x96xf32>\n  %202 = \"tfl.cast\"(%201) : (tensor<?x12x12x96xf32>) -> tensor<?x12x12x96xf16>\n  %203 = \"tf.Relu6\"(%202) {device = \"\"} : (tensor<?x12x12x96xf16>) -> tensor<?x12x12x96xf16>\n  %204 = \"tf.Conv2D\"(%203, %83) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x96xf16>, tensor<1x1x96x24xf16>) -> tensor<?x12x12x24xf16>\n  %205 = \"tfl.cast\"(%204) : (tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf32>\n  %206 = \"tfl.mul\"(%205, %84) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %207 = \"tfl.add\"(%206, %85) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %208 = \"tfl.cast\"(%207) : (tensor<?x12x12x24xf32>) -> tensor<?x12x12x24xf16>\n  %209 = \"tf.Conv2D\"(%208, %89) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n  %210 = \"tfl.cast\"(%209) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %211 = \"tfl.mul\"(%210, %90) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %212 = \"tfl.add\"(%211, %91) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %213 = \"tfl.cast\"(%212) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %214 = \"tf.Relu6\"(%213) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %215 = \"tf.DepthwiseConv2dNative\"(%214, %86) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x12x12x144xf16>\n  %216 = \"tfl.cast\"(%215) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %217 = \"tfl.mul\"(%216, %87) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %218 = \"tfl.add\"(%217, %88) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %219 = \"tfl.cast\"(%218) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %220 = \"tf.Relu6\"(%219) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %221 = \"tf.Conv2D\"(%220, %92) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<1x1x144x24xf16>) -> tensor<?x12x12x24xf16>\n  %222 = \"tfl.cast\"(%221) : (tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf32>\n  %223 = \"tfl.mul\"(%222, %93) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %224 = \"tfl.add\"(%223, %94) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x24xf32>, tensor<24xf32>) -> tensor<?x12x12x24xf32>\n  %225 = \"tfl.cast\"(%224) : (tensor<?x12x12x24xf32>) -> tensor<?x12x12x24xf16>\n  %226 = \"tf.AddV2\"(%208, %225) {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<?x12x12x24xf16>) -> tensor<?x12x12x24xf16>\n  %227 = \"tf.Conv2D\"(%226, %98) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x12x12x24xf16>, tensor<1x1x24x144xf16>) -> tensor<?x12x12x144xf16>\n  %228 = \"tfl.cast\"(%227) : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf32>\n  %229 = \"tfl.mul\"(%228, %99) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %230 = \"tfl.add\"(%229, %100) <{fused_activation_function = \"NONE\"}> : (tensor<?x12x12x144xf32>, tensor<144xf32>) -> tensor<?x12x12x144xf32>\n  %231 = \"tfl.cast\"(%230) : (tensor<?x12x12x144xf32>) -> tensor<?x12x12x144xf16>\n  %232 = \"tf.Relu6\"(%231) {device = \"\"} : (tensor<?x12x12x144xf16>) -> tensor<?x12x12x144xf16>\n  %233 = \"tf.Pad\"(%232, %7) {device = \"\"} : (tensor<?x12x12x144xf16>, tensor<4x2xi32>) -> tensor<?x13x13x144xf16>\n  %234 = \"tf.DepthwiseConv2dNative\"(%233, %95) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x13x13x144xf16>, tensor<3x3x144x1xf16>) -> tensor<?x6x6x144xf16>\n  %235 = \"tfl.cast\"(%234) : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf32>\n  %236 = \"tfl.mul\"(%235, %96) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x144xf32>, tensor<144xf32>) -> tensor<?x6x6x144xf32>\n  %237 = \"tfl.add\"(%236, %97) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x144xf32>, tensor<144xf32>) -> tensor<?x6x6x144xf32>\n  %238 = \"tfl.cast\"(%237) : (tensor<?x6x6x144xf32>) -> tensor<?x6x6x144xf16>\n  %239 = \"tf.Relu6\"(%238) {device = \"\"} : (tensor<?x6x6x144xf16>) -> tensor<?x6x6x144xf16>\n  %240 = \"tf.Conv2D\"(%239, %101) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x144xf16>, tensor<1x1x144x32xf16>) -> tensor<?x6x6x32xf16>\n  %241 = \"tfl.cast\"(%240) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %242 = \"tfl.mul\"(%241, %102) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %243 = \"tfl.add\"(%242, %103) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %244 = \"tfl.cast\"(%243) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %245 = \"tf.Conv2D\"(%244, %107) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %246 = \"tfl.cast\"(%245) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %247 = \"tfl.mul\"(%246, %108) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %248 = \"tfl.add\"(%247, %109) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %249 = \"tfl.cast\"(%248) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %250 = \"tf.Relu6\"(%249) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %251 = \"tf.DepthwiseConv2dNative\"(%250, %104) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n  %252 = \"tfl.cast\"(%251) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %253 = \"tfl.mul\"(%252, %105) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %254 = \"tfl.add\"(%253, %106) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %255 = \"tfl.cast\"(%254) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %256 = \"tf.Relu6\"(%255) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %257 = \"tf.Conv2D\"(%256, %110) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n  %258 = \"tfl.cast\"(%257) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %259 = \"tfl.mul\"(%258, %111) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %260 = \"tfl.add\"(%259, %112) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %261 = \"tfl.cast\"(%260) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %262 = \"tf.AddV2\"(%244, %261) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n  %263 = \"tf.Conv2D\"(%262, %116) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %264 = \"tfl.cast\"(%263) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %265 = \"tfl.mul\"(%264, %117) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %266 = \"tfl.add\"(%265, %118) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %267 = \"tfl.cast\"(%266) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %268 = \"tf.Relu6\"(%267) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %269 = \"tf.DepthwiseConv2dNative\"(%268, %113) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x6x6x192xf16>\n  %270 = \"tfl.cast\"(%269) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %271 = \"tfl.mul\"(%270, %114) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %272 = \"tfl.add\"(%271, %115) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %273 = \"tfl.cast\"(%272) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %274 = \"tf.Relu6\"(%273) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %275 = \"tf.Conv2D\"(%274, %119) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<1x1x192x32xf16>) -> tensor<?x6x6x32xf16>\n  %276 = \"tfl.cast\"(%275) : (tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf32>\n  %277 = \"tfl.mul\"(%276, %120) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %278 = \"tfl.add\"(%277, %121) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x32xf32>, tensor<32xf32>) -> tensor<?x6x6x32xf32>\n  %279 = \"tfl.cast\"(%278) : (tensor<?x6x6x32xf32>) -> tensor<?x6x6x32xf16>\n  %280 = \"tf.AddV2\"(%262, %279) {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<?x6x6x32xf16>) -> tensor<?x6x6x32xf16>\n  %281 = \"tf.Conv2D\"(%280, %125) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x6x6x32xf16>, tensor<1x1x32x192xf16>) -> tensor<?x6x6x192xf16>\n  %282 = \"tfl.cast\"(%281) : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf32>\n  %283 = \"tfl.mul\"(%282, %126) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %284 = \"tfl.add\"(%283, %127) <{fused_activation_function = \"NONE\"}> : (tensor<?x6x6x192xf32>, tensor<192xf32>) -> tensor<?x6x6x192xf32>\n  %285 = \"tfl.cast\"(%284) : (tensor<?x6x6x192xf32>) -> tensor<?x6x6x192xf16>\n  %286 = \"tf.Relu6\"(%285) {device = \"\"} : (tensor<?x6x6x192xf16>) -> tensor<?x6x6x192xf16>\n  %287 = \"tf.Pad\"(%286, %7) {device = \"\"} : (tensor<?x6x6x192xf16>, tensor<4x2xi32>) -> tensor<?x7x7x192xf16>\n  %288 = \"tf.DepthwiseConv2dNative\"(%287, %122) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x7x7x192xf16>, tensor<3x3x192x1xf16>) -> tensor<?x3x3x192xf16>\n  %289 = \"tfl.cast\"(%288) : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf32>\n  %290 = \"tfl.mul\"(%289, %123) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x192xf32>, tensor<192xf32>) -> tensor<?x3x3x192xf32>\n  %291 = \"tfl.add\"(%290, %124) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x192xf32>, tensor<192xf32>) -> tensor<?x3x3x192xf32>\n  %292 = \"tfl.cast\"(%291) : (tensor<?x3x3x192xf32>) -> tensor<?x3x3x192xf16>\n  %293 = \"tf.Relu6\"(%292) {device = \"\"} : (tensor<?x3x3x192xf16>) -> tensor<?x3x3x192xf16>\n  %294 = \"tf.Conv2D\"(%293, %128) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x192xf16>, tensor<1x1x192x64xf16>) -> tensor<?x3x3x64xf16>\n  %295 = \"tfl.cast\"(%294) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %296 = \"tfl.mul\"(%295, %129) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %297 = \"tfl.add\"(%296, %130) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %298 = \"tfl.cast\"(%297) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %299 = \"tf.Conv2D\"(%298, %134) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %300 = \"tfl.cast\"(%299) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %301 = \"tfl.mul\"(%300, %135) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %302 = \"tfl.add\"(%301, %136) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %303 = \"tfl.cast\"(%302) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %304 = \"tf.Relu6\"(%303) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %305 = \"tf.DepthwiseConv2dNative\"(%304, %131) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %306 = \"tfl.cast\"(%305) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %307 = \"tfl.mul\"(%306, %132) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %308 = \"tfl.add\"(%307, %133) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %309 = \"tfl.cast\"(%308) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %310 = \"tf.Relu6\"(%309) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %311 = \"tf.Conv2D\"(%310, %137) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %312 = \"tfl.cast\"(%311) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %313 = \"tfl.mul\"(%312, %138) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %314 = \"tfl.add\"(%313, %139) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %315 = \"tfl.cast\"(%314) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %316 = \"tf.AddV2\"(%298, %315) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %317 = \"tf.Conv2D\"(%316, %143) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %318 = \"tfl.cast\"(%317) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %319 = \"tfl.mul\"(%318, %144) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %320 = \"tfl.add\"(%319, %145) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %321 = \"tfl.cast\"(%320) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %322 = \"tf.Relu6\"(%321) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %323 = \"tf.DepthwiseConv2dNative\"(%322, %140) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %324 = \"tfl.cast\"(%323) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %325 = \"tfl.mul\"(%324, %141) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %326 = \"tfl.add\"(%325, %142) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %327 = \"tfl.cast\"(%326) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %328 = \"tf.Relu6\"(%327) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %329 = \"tf.Conv2D\"(%328, %146) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %330 = \"tfl.cast\"(%329) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %331 = \"tfl.mul\"(%330, %147) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %332 = \"tfl.add\"(%331, %148) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %333 = \"tfl.cast\"(%332) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %334 = \"tf.AddV2\"(%316, %333) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %335 = \"tf.Conv2D\"(%334, %152) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %336 = \"tfl.cast\"(%335) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %337 = \"tfl.mul\"(%336, %153) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %338 = \"tfl.add\"(%337, %154) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %339 = \"tfl.cast\"(%338) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %340 = \"tf.Relu6\"(%339) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %341 = \"tf.DepthwiseConv2dNative\"(%340, %149) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %342 = \"tfl.cast\"(%341) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %343 = \"tfl.mul\"(%342, %150) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %344 = \"tfl.add\"(%343, %151) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %345 = \"tfl.cast\"(%344) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %346 = \"tf.Relu6\"(%345) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %347 = \"tf.Conv2D\"(%346, %155) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x64xf16>) -> tensor<?x3x3x64xf16>\n  %348 = \"tfl.cast\"(%347) : (tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf32>\n  %349 = \"tfl.mul\"(%348, %156) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %350 = \"tfl.add\"(%349, %157) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x64xf32>, tensor<64xf32>) -> tensor<?x3x3x64xf32>\n  %351 = \"tfl.cast\"(%350) : (tensor<?x3x3x64xf32>) -> tensor<?x3x3x64xf16>\n  %352 = \"tf.AddV2\"(%334, %351) {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<?x3x3x64xf16>) -> tensor<?x3x3x64xf16>\n  %353 = \"tf.Conv2D\"(%352, %17) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x64xf16>, tensor<1x1x64x384xf16>) -> tensor<?x3x3x384xf16>\n  %354 = \"tfl.cast\"(%353) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %355 = \"tfl.mul\"(%354, %18) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %356 = \"tfl.add\"(%355, %19) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %357 = \"tfl.cast\"(%356) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %358 = \"tf.Relu6\"(%357) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %359 = \"tf.DepthwiseConv2dNative\"(%358, %14) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<3x3x384x1xf16>) -> tensor<?x3x3x384xf16>\n  %360 = \"tfl.cast\"(%359) : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf32>\n  %361 = \"tfl.mul\"(%360, %15) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %362 = \"tfl.add\"(%361, %16) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x384xf32>, tensor<384xf32>) -> tensor<?x3x3x384xf32>\n  %363 = \"tfl.cast\"(%362) : (tensor<?x3x3x384xf32>) -> tensor<?x3x3x384xf16>\n  %364 = \"tf.Relu6\"(%363) {device = \"\"} : (tensor<?x3x3x384xf16>) -> tensor<?x3x3x384xf16>\n  %365 = \"tf.Conv2D\"(%364, %20) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x384xf16>, tensor<1x1x384x96xf16>) -> tensor<?x3x3x96xf16>\n  %366 = \"tfl.cast\"(%365) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %367 = \"tfl.mul\"(%366, %21) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %368 = \"tfl.add\"(%367, %22) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %369 = \"tfl.cast\"(%368) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %370 = \"tf.Conv2D\"(%369, %26) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %371 = \"tfl.cast\"(%370) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %372 = \"tfl.mul\"(%371, %27) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %373 = \"tfl.add\"(%372, %28) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %374 = \"tfl.cast\"(%373) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %375 = \"tf.Relu6\"(%374) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %376 = \"tf.DepthwiseConv2dNative\"(%375, %23) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n  %377 = \"tfl.cast\"(%376) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %378 = \"tfl.mul\"(%377, %24) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %379 = \"tfl.add\"(%378, %25) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %380 = \"tfl.cast\"(%379) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %381 = \"tf.Relu6\"(%380) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %382 = \"tf.Conv2D\"(%381, %29) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n  %383 = \"tfl.cast\"(%382) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %384 = \"tfl.mul\"(%383, %30) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %385 = \"tfl.add\"(%384, %31) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %386 = \"tfl.cast\"(%385) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %387 = \"tf.AddV2\"(%369, %386) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n  %388 = \"tf.Conv2D\"(%387, %35) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %389 = \"tfl.cast\"(%388) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %390 = \"tfl.mul\"(%389, %36) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %391 = \"tfl.add\"(%390, %37) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %392 = \"tfl.cast\"(%391) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %393 = \"tf.Relu6\"(%392) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %394 = \"tf.DepthwiseConv2dNative\"(%393, %32) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x3x3x576xf16>\n  %395 = \"tfl.cast\"(%394) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %396 = \"tfl.mul\"(%395, %33) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %397 = \"tfl.add\"(%396, %34) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %398 = \"tfl.cast\"(%397) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %399 = \"tf.Relu6\"(%398) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %400 = \"tf.Conv2D\"(%399, %38) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<1x1x576x96xf16>) -> tensor<?x3x3x96xf16>\n  %401 = \"tfl.cast\"(%400) : (tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf32>\n  %402 = \"tfl.mul\"(%401, %39) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %403 = \"tfl.add\"(%402, %40) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x96xf32>, tensor<96xf32>) -> tensor<?x3x3x96xf32>\n  %404 = \"tfl.cast\"(%403) : (tensor<?x3x3x96xf32>) -> tensor<?x3x3x96xf16>\n  %405 = \"tf.AddV2\"(%387, %404) {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<?x3x3x96xf16>) -> tensor<?x3x3x96xf16>\n  %406 = \"tf.Conv2D\"(%405, %44) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x3x3x96xf16>, tensor<1x1x96x576xf16>) -> tensor<?x3x3x576xf16>\n  %407 = \"tfl.cast\"(%406) : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf32>\n  %408 = \"tfl.mul\"(%407, %45) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %409 = \"tfl.add\"(%408, %46) <{fused_activation_function = \"NONE\"}> : (tensor<?x3x3x576xf32>, tensor<576xf32>) -> tensor<?x3x3x576xf32>\n  %410 = \"tfl.cast\"(%409) : (tensor<?x3x3x576xf32>) -> tensor<?x3x3x576xf16>\n  %411 = \"tf.Relu6\"(%410) {device = \"\"} : (tensor<?x3x3x576xf16>) -> tensor<?x3x3x576xf16>\n  %412 = \"tf.Pad\"(%411, %6) {device = \"\"} : (tensor<?x3x3x576xf16>, tensor<4x2xi32>) -> tensor<?x5x5x576xf16>\n  %413 = \"tf.DepthwiseConv2dNative\"(%412, %41) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 2, 2, 1]}> {device = \"\"} : (tensor<?x5x5x576xf16>, tensor<3x3x576x1xf16>) -> tensor<?x2x2x576xf16>\n  %414 = \"tfl.cast\"(%413) : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf32>\n  %415 = \"tfl.mul\"(%414, %42) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x576xf32>, tensor<576xf32>) -> tensor<?x2x2x576xf32>\n  %416 = \"tfl.add\"(%415, %43) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x576xf32>, tensor<576xf32>) -> tensor<?x2x2x576xf32>\n  %417 = \"tfl.cast\"(%416) : (tensor<?x2x2x576xf32>) -> tensor<?x2x2x576xf16>\n  %418 = \"tf.Relu6\"(%417) {device = \"\"} : (tensor<?x2x2x576xf16>) -> tensor<?x2x2x576xf16>\n  %419 = \"tf.Conv2D\"(%418, %47) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x576xf16>, tensor<1x1x576x160xf16>) -> tensor<?x2x2x160xf16>\n  %420 = \"tfl.cast\"(%419) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %421 = \"tfl.mul\"(%420, %48) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %422 = \"tfl.add\"(%421, %49) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %423 = \"tfl.cast\"(%422) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %424 = \"tf.Conv2D\"(%423, %53) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %425 = \"tfl.cast\"(%424) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %426 = \"tfl.mul\"(%425, %54) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %427 = \"tfl.add\"(%426, %55) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %428 = \"tfl.cast\"(%427) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %429 = \"tf.Relu6\"(%428) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %430 = \"tf.DepthwiseConv2dNative\"(%429, %50) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %431 = \"tfl.cast\"(%430) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %432 = \"tfl.mul\"(%431, %51) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %433 = \"tfl.add\"(%432, %52) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %434 = \"tfl.cast\"(%433) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %435 = \"tf.Relu6\"(%434) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %436 = \"tf.Conv2D\"(%435, %56) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n  %437 = \"tfl.cast\"(%436) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %438 = \"tfl.mul\"(%437, %57) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %439 = \"tfl.add\"(%438, %58) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %440 = \"tfl.cast\"(%439) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %441 = \"tf.AddV2\"(%423, %440) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n  %442 = \"tf.Conv2D\"(%441, %62) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %443 = \"tfl.cast\"(%442) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %444 = \"tfl.mul\"(%443, %63) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %445 = \"tfl.add\"(%444, %64) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %446 = \"tfl.cast\"(%445) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %447 = \"tf.Relu6\"(%446) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %448 = \"tf.DepthwiseConv2dNative\"(%447, %59) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %449 = \"tfl.cast\"(%448) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %450 = \"tfl.mul\"(%449, %60) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %451 = \"tfl.add\"(%450, %61) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %452 = \"tfl.cast\"(%451) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %453 = \"tf.Relu6\"(%452) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %454 = \"tf.Conv2D\"(%453, %65) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x160xf16>) -> tensor<?x2x2x160xf16>\n  %455 = \"tfl.cast\"(%454) : (tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf32>\n  %456 = \"tfl.mul\"(%455, %66) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %457 = \"tfl.add\"(%456, %67) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x160xf32>, tensor<160xf32>) -> tensor<?x2x2x160xf32>\n  %458 = \"tfl.cast\"(%457) : (tensor<?x2x2x160xf32>) -> tensor<?x2x2x160xf16>\n  %459 = \"tf.AddV2\"(%441, %458) {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<?x2x2x160xf16>) -> tensor<?x2x2x160xf16>\n  %460 = \"tf.Conv2D\"(%459, %71) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x160xf16>, tensor<1x1x160x960xf16>) -> tensor<?x2x2x960xf16>\n  %461 = \"tfl.cast\"(%460) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %462 = \"tfl.mul\"(%461, %72) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %463 = \"tfl.add\"(%462, %73) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %464 = \"tfl.cast\"(%463) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %465 = \"tf.Relu6\"(%464) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %466 = \"tf.DepthwiseConv2dNative\"(%465, %68) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1]}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<3x3x960x1xf16>) -> tensor<?x2x2x960xf16>\n  %467 = \"tfl.cast\"(%466) : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf32>\n  %468 = \"tfl.mul\"(%467, %69) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %469 = \"tfl.add\"(%468, %70) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x960xf32>, tensor<960xf32>) -> tensor<?x2x2x960xf32>\n  %470 = \"tfl.cast\"(%469) : (tensor<?x2x2x960xf32>) -> tensor<?x2x2x960xf16>\n  %471 = \"tf.Relu6\"(%470) {device = \"\"} : (tensor<?x2x2x960xf16>) -> tensor<?x2x2x960xf16>\n  %472 = \"tf.Conv2D\"(%471, %74) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"SAME\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x960xf16>, tensor<1x1x960x320xf16>) -> tensor<?x2x2x320xf16>\n  %473 = \"tfl.cast\"(%472) : (tensor<?x2x2x320xf16>) -> tensor<?x2x2x320xf32>\n  %474 = \"tfl.mul\"(%473, %75) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x320xf32>, tensor<320xf32>) -> tensor<?x2x2x320xf32>\n  %475 = \"tfl.add\"(%474, %76) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x320xf32>, tensor<320xf32>) -> tensor<?x2x2x320xf32>\n  %476 = \"tfl.cast\"(%475) : (tensor<?x2x2x320xf32>) -> tensor<?x2x2x320xf16>\n  %477 = \"tf.Conv2D\"(%476, %161) <{data_format = \"NHWC\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}> {device = \"\"} : (tensor<?x2x2x320xf16>, tensor<1x1x320x1280xf16>) -> tensor<?x2x2x1280xf16>\n  %478 = \"tfl.cast\"(%477) : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf32>\n  %479 = \"tfl.mul\"(%478, %162) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x1280xf32>, tensor<1280xf32>) -> tensor<?x2x2x1280xf32>\n  %480 = \"tfl.add\"(%479, %163) <{fused_activation_function = \"NONE\"}> : (tensor<?x2x2x1280xf32>, tensor<1280xf32>) -> tensor<?x2x2x1280xf32>\n  %481 = \"tfl.cast\"(%480) : (tensor<?x2x2x1280xf32>) -> tensor<?x2x2x1280xf16>\n  %482 = \"tf.Relu6\"(%481) {device = \"\"} : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf16>\n  %483 = \"tfl.cast\"(%482) : (tensor<?x2x2x1280xf16>) -> tensor<?x2x2x1280xf32>\n  %484 = \"tfl.mean\"(%483, %5) <{keep_dims = false}> : (tensor<?x2x2x1280xf32>, tensor<2xi32>) -> tensor<?x1280xf32>\n  %485 = \"tfl.cast\"(%484) : (tensor<?x1280xf32>) -> tensor<?x1280xf16>\n  %486 = \"tf.MatMul\"(%485, %2) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x1280xf16>, tensor<256x1280xf16>) -> tensor<?x256xf16>\n  %487 = \"tf.BiasAdd\"(%486, %13) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x256xf16>, tensor<256xf16>) -> tensor<?x256xf16>\n  %488 = \"tf.Relu\"(%487) {device = \"\"} : (tensor<?x256xf16>) -> tensor<?x256xf16>\n  %489 = \"tfl.cast\"(%488) : (tensor<?x256xf16>) -> tensor<?x256xf32>\n  %490 = \"tfl.mul\"(%489, %10) <{fused_activation_function = \"NONE\"}> : (tensor<?x256xf32>, tensor<256xf32>) -> tensor<?x256xf32>\n  %491 = \"tfl.add\"(%490, %11) <{fused_activation_function = \"NONE\"}> : (tensor<?x256xf32>, tensor<256xf32>) -> tensor<?x256xf32>\n  %492 = \"tfl.cast\"(%491) : (tensor<?x256xf32>) -> tensor<?x256xf16>\n  %493 = \"tf.MatMul\"(%492, %1) <{grad_a = false, grad_b = false, transpose_a = false, transpose_b = true}> : (tensor<?x256xf16>, tensor<128x256xf16>) -> tensor<?x128xf16>\n  %494 = \"tf.BiasAdd\"(%493, %12) <{data_format = \"NHWC\"}> {device = \"\"} : (tensor<?x128xf16>, tensor<128xf16>) -> tensor<?x128xf16>\n  %495 = \"tf.Relu\"(%494) {device = \"\"} : (tensor<?x128xf16>) -> tensor<?x128xf16>\n  %496 = \"tfl.cast\"(%495) : (tensor<?x128xf16>) -> tensor<?x128xf32>\n  %497 = \"tfl.mul\"(%496, %8) <{fused_activation_function = \"NONE\"}> : (tensor<?x128xf32>, tensor<128xf32>) -> tensor<?x128xf32>\n  %498 = \"tfl.add\"(%497, %9) <{fused_activation_function = \"NONE\"}> : (tensor<?x128xf32>, tensor<128xf32>) -> tensor<?x128xf32>\n  %499 = \"tfl.cast\"(%498) : (tensor<?x128xf32>) -> tensor<?x128xf16>\n  %500 = \"tfl.cast\"(%499) : (tensor<?x128xf16>) -> tensor<?x128xf32>\n  %501 = \"tfl.fully_connected\"(%500, %0, %3) <{fused_activation_function = \"NONE\", keep_num_dims = false, weights_format = \"DEFAULT\"}> : (tensor<?x128xf32>, tensor<7x128xf32>, tensor<7xf32>) -> tensor<?x7xf32>\n  %502 = \"tfl.softmax\"(%501) <{beta = 1.000000e+00 : f32}> : (tensor<?x7xf32>) -> tensor<?x7xf32>\n  \"func.return\"(%502) : (tensor<?x7xf32>) -> ()\n}) {tf.entry_function = {control_outputs = \"\", inputs = \"serving_default_keras_tensor_155:0\", outputs = \"StatefulPartitionedCall_1:0\"}, tf_saved_model.exported_names = [\"serving_default\"]} : () -> ()\n"
     ]
    }
   ],
   "source": [
    "# =======================================================================\n",
    "# Export for Snapdragon deployment\n",
    "# =======================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“¦ Exporting models for Snapdragon deployment\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1) TFLite INT8 (best for NPU)\n",
    "def convert_to_tflite_int8(model, train_generator, output_path='emotion_model_int8.tflite', reps=100):\n",
    "    train_generator.reset()\n",
    "    def representative_dataset():\n",
    "        yielded = 0\n",
    "        while yielded < reps:\n",
    "            # Use a fresh iterator each time to avoid exhaustion issues\n",
    "            batch, _ = next(iter(train_generator))\n",
    "            for i in range(min(len(batch), reps - yielded)):\n",
    "                yield [batch[i:i+1].astype(np.float32)]\n",
    "                yielded += 1\n",
    "\n",
    "    conv = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    conv.representative_dataset = representative_dataset\n",
    "    conv.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    conv.inference_input_type = tf.uint8\n",
    "    conv.inference_output_type = tf.uint8\n",
    "    tfl = conv.convert()\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tfl)\n",
    "    print(f\"âœ… INT8 TFLite saved: {output_path} ({len(tfl)/1024:.1f} KB)\")\n",
    "\n",
    "convert_to_tflite_int8(model, train_set, 'emotion_model_int8.tflite', reps=128)\n",
    "\n",
    "# 2) TFLite FP16 (good balance)\n",
    "conv = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "conv.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "conv.target_spec.supported_types = [tf.float16]\n",
    "tfl_fp16 = conv.convert()\n",
    "with open('emotion_model_fp16.tflite', 'wb') as f:\n",
    "    f.write(tfl_fp16)\n",
    "print(f\"âœ… FP16 TFLite saved: emotion_model_fp16.tflite ({len(tfl_fp16)/1024:.1f} KB)\")\n",
    "\n",
    "# 3) ONNX (optional)\n",
    "try:\n",
    "    import tf2onnx\n",
    "    spec = (tf.TensorSpec((None, picture_size, picture_size, 1), tf.float32, name=\"input\"),)\n",
    "    _proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, opset=13, output_path=\"emotion_model.onnx\")\n",
    "    print(\"âœ… ONNX saved: emotion_model.onnx\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ ONNX export skipped: {e}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75101d4",
   "metadata": {},
   "source": [
    "============================== end =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66189dcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# =======================================================================\n",
    "# Model: MobileNetV2 feature extractor for grayscale\n",
    "# =======================================================================\n",
    "# MobileNetV2 expects 3-channel input; we replicate grayscale to RGB\n",
    "base_model = MobileNetV2(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape=(picture_size, picture_size, 3)\n",
    ")\n",
    "base_model.trainable = False  # freeze for initial training\n",
    "\n",
    "inputs = Input(shape=(picture_size, picture_size, 1))\n",
    "x = tf.keras.layers.Concatenate(axis=-1)([inputs, inputs, inputs])  # to 3 channels\n",
    "x = base_model(x, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# Ensure numerically stable float32 at the output with mixed precision\n",
    "outputs = Dense(no_of_classes, activation='softmax', dtype='float32')(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88317c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb3cbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test data pipeline\n",
    "print(\"\\nğŸ” Testing data pipeline...\")\n",
    "sample_batch, sample_labels = next(iter(train_set))\n",
    "print(f\"Batch shape: {sample_batch.shape}\")\n",
    "print(f\"Batch dtype: {sample_batch.dtype}\")\n",
    "print(f\"Batch range: [{sample_batch.min():.3f}, {sample_batch.max():.3f}]\")\n",
    "print(f\"Labels shape: {sample_labels.shape}\")\n",
    "\n",
    "# Test model prediction with error handling\n",
    "try:\n",
    "    test_pred = model.predict(sample_batch[:1], verbose=0)\n",
    "    print(f\"Prediction shape: {test_pred.shape}\")\n",
    "    print(f\"Prediction values: {test_pred[0]}\")\n",
    "    print(\"âœ… Data pipeline working!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Prediction failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82636d00",
   "metadata": {},
   "source": [
    "\n",
    "# =======================================================================\n",
    "# Post-Training: Export for Snapdragon Deployment\n",
    "# =======================================================================\n",
    "\n",
    "# After training completes, add this:\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ Exporting optimized models for Snapdragon X Elite...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 1. TFLite INT8 (Best for NPU)\n",
    "convert_to_tflite_quantized(model, 'emotion_model_int8.tflite')\n",
    "\n",
    "# 2. TFLite FP16 (Good balance)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_fp16 = converter.convert()\n",
    "with open('emotion_model_fp16.tflite', 'wb') as f:\n",
    "    f.write(tflite_fp16)\n",
    "print(f\"âœ… FP16 model saved: emotion_model_fp16.tflite ({len(tflite_fp16)/1024:.2f} KB)\")\n",
    "\n",
    "# 3. ONNX (for cross-platform)\n",
    "try:\n",
    "    import tf2onnx\n",
    "    spec = (tf.TensorSpec((None, 48, 48, 1), tf.float32, name=\"input\"),)\n",
    "    model_proto, _ = tf2onnx.convert.from_keras(\n",
    "        model, \n",
    "        input_signature=spec, \n",
    "        opset=13, \n",
    "        output_path=\"emotion_model.onnx\"\n",
    "    )\n",
    "    print(\"âœ… ONNX model saved: emotion_model.onnx\")\n",
    "except:\n",
    "    print(\"âš ï¸ ONNX export skipped (install: pip install tf2onnx)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š Model Export Summary:\")\n",
    "print(\"=\"*70)\n",
    "print(\"1. emotion_model_int8.tflite  â†’ Best for Snapdragon NPU (fastest)\")\n",
    "print(\"2. emotion_model_fp16.tflite  â†’ Good balance (fast + accurate)\")\n",
    "print(\"3. emotion_model.onnx         â†’ Cross-platform deployment\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf311_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
